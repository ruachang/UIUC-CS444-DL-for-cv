{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "COLAB = False\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "if COLAB:\n",
    "    work_dir = \"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\"\n",
    "else:\n",
    "    work_dir = \"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\"\n",
    "\n",
    "os.chdir(work_dir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changl25/miniconda3/envs/StableVITON/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_time(agent):\n",
    "    done = False\n",
    "    score = 0\n",
    "    frame = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255., device)\n",
    "        state = next_state\n",
    "        next_state, reward, done, truncated, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "        # history = torch.from_numpy(history).to(device)\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 269   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 0.0   memory length: 391   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 2   score: 2.0   memory length: 590   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 2.0   memory length: 788   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 1.0   memory length: 956   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 5   score: 0.0   memory length: 1079   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 6   score: 0.0   memory length: 1202   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
      "episode: 7   score: 0.0   memory length: 1324   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 8   score: 0.0   memory length: 1447   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.8888888888888888\n",
      "episode: 9   score: 3.0   memory length: 1694   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 10   score: 4.0   memory length: 1993   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 11   score: 0.0   memory length: 2116   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 12   score: 0.0   memory length: 2239   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1538461538461537\n",
      "episode: 13   score: 2.0   memory length: 2437   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2142857142857142\n",
      "episode: 14   score: 1.0   memory length: 2605   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 15   score: 6.0   memory length: 2981   epsilon: 1.0    steps: 376    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 16   score: 1.0   memory length: 3132   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4705882352941178\n",
      "episode: 17   score: 1.0   memory length: 3283   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 18   score: 2.0   memory length: 3504   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.4736842105263157\n",
      "episode: 19   score: 2.0   memory length: 3721   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 20   score: 4.0   memory length: 4037   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 21   score: 2.0   memory length: 4255   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 22   score: 1.0   memory length: 4427   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 23   score: 1.0   memory length: 4578   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 24   score: 1.0   memory length: 4746   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 25   score: 0.0   memory length: 4868   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 26   score: 2.0   memory length: 5087   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5185185185185186\n",
      "episode: 27   score: 0.0   memory length: 5209   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4642857142857142\n",
      "episode: 28   score: 6.0   memory length: 5581   epsilon: 1.0    steps: 372    lr: 0.0001     evaluation reward: 1.6206896551724137\n",
      "episode: 29   score: 1.0   memory length: 5732   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 30   score: 3.0   memory length: 5957   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6451612903225807\n",
      "episode: 31   score: 2.0   memory length: 6173   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.65625\n",
      "episode: 32   score: 2.0   memory length: 6370   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 33   score: 0.0   memory length: 6493   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6176470588235294\n",
      "episode: 34   score: 1.0   memory length: 6643   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 35   score: 7.0   memory length: 7046   epsilon: 1.0    steps: 403    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 36   score: 3.0   memory length: 7271   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.7837837837837838\n",
      "episode: 37   score: 2.0   memory length: 7489   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7894736842105263\n",
      "episode: 38   score: 2.0   memory length: 7706   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.794871794871795\n",
      "episode: 39   score: 2.0   memory length: 7923   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 40   score: 2.0   memory length: 8121   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8048780487804879\n",
      "episode: 41   score: 5.0   memory length: 8465   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.880952380952381\n",
      "episode: 42   score: 0.0   memory length: 8588   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8372093023255813\n",
      "episode: 43   score: 2.0   memory length: 8786   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8409090909090908\n",
      "episode: 44   score: 3.0   memory length: 9032   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.8666666666666667\n",
      "episode: 45   score: 1.0   memory length: 9183   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8478260869565217\n",
      "episode: 46   score: 2.0   memory length: 9382   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.851063829787234\n",
      "episode: 47   score: 3.0   memory length: 9607   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 48   score: 0.0   memory length: 9729   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.836734693877551\n",
      "episode: 49   score: 0.0   memory length: 9852   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 50   score: 0.0   memory length: 9975   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7647058823529411\n",
      "episode: 51   score: 1.0   memory length: 10144   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 52   score: 5.0   memory length: 10466   epsilon: 1.0    steps: 322    lr: 0.0001     evaluation reward: 1.8113207547169812\n",
      "episode: 53   score: 4.0   memory length: 10761   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.8518518518518519\n",
      "episode: 54   score: 1.0   memory length: 10929   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.8363636363636364\n",
      "episode: 55   score: 0.0   memory length: 11051   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8035714285714286\n",
      "episode: 56   score: 2.0   memory length: 11269   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8070175438596492\n",
      "episode: 57   score: 2.0   memory length: 11467   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.8103448275862069\n",
      "episode: 58   score: 4.0   memory length: 11754   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.847457627118644\n",
      "episode: 59   score: 1.0   memory length: 11904   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 60   score: 0.0   memory length: 12026   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8032786885245902\n",
      "episode: 61   score: 1.0   memory length: 12195   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7903225806451613\n",
      "episode: 62   score: 2.0   memory length: 12392   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7936507936507937\n",
      "episode: 63   score: 0.0   memory length: 12514   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.765625\n",
      "episode: 64   score: 2.0   memory length: 12713   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7692307692307692\n",
      "episode: 65   score: 3.0   memory length: 12941   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.7878787878787878\n",
      "episode: 66   score: 3.0   memory length: 13209   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.8059701492537314\n",
      "episode: 67   score: 2.0   memory length: 13408   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.8088235294117647\n",
      "episode: 68   score: 2.0   memory length: 13626   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8115942028985508\n",
      "episode: 69   score: 3.0   memory length: 13855   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.8285714285714285\n",
      "episode: 70   score: 3.0   memory length: 14101   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.8450704225352113\n",
      "episode: 71   score: 4.0   memory length: 14374   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 72   score: 1.0   memory length: 14545   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.8630136986301369\n",
      "episode: 73   score: 0.0   memory length: 14668   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.837837837837838\n",
      "episode: 74   score: 1.0   memory length: 14836   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.8266666666666667\n",
      "episode: 75   score: 3.0   memory length: 15083   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.8421052631578947\n",
      "episode: 76   score: 0.0   memory length: 15206   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
      "episode: 77   score: 0.0   memory length: 15328   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.794871794871795\n",
      "episode: 78   score: 2.0   memory length: 15548   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.7974683544303798\n",
      "episode: 79   score: 2.0   memory length: 15766   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 80   score: 2.0   memory length: 15950   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.8024691358024691\n",
      "episode: 81   score: 2.0   memory length: 16147   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8048780487804879\n",
      "episode: 82   score: 0.0   memory length: 16269   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.783132530120482\n",
      "episode: 83   score: 3.0   memory length: 16497   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.7976190476190477\n",
      "episode: 84   score: 0.0   memory length: 16619   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.776470588235294\n",
      "episode: 85   score: 0.0   memory length: 16741   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.755813953488372\n",
      "episode: 86   score: 0.0   memory length: 16863   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.735632183908046\n",
      "episode: 87   score: 0.0   memory length: 16985   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7159090909090908\n",
      "episode: 88   score: 0.0   memory length: 17108   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.696629213483146\n",
      "episode: 89   score: 1.0   memory length: 17259   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6888888888888889\n",
      "episode: 90   score: 2.0   memory length: 17457   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
      "episode: 91   score: 2.0   memory length: 17657   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6956521739130435\n",
      "episode: 92   score: 3.0   memory length: 17904   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.7096774193548387\n",
      "episode: 93   score: 1.0   memory length: 18055   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.702127659574468\n",
      "episode: 94   score: 3.0   memory length: 18321   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.7157894736842105\n",
      "episode: 95   score: 2.0   memory length: 18539   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71875\n",
      "episode: 96   score: 2.0   memory length: 18736   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7216494845360826\n",
      "episode: 97   score: 3.0   memory length: 18963   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.7346938775510203\n",
      "episode: 98   score: 0.0   memory length: 19085   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7171717171717171\n",
      "episode: 99   score: 3.0   memory length: 19332   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 100   score: 3.0   memory length: 19581   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 101   score: 2.0   memory length: 19801   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 102   score: 0.0   memory length: 19924   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 103   score: 1.0   memory length: 20075   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 104   score: 3.0   memory length: 20320   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 105   score: 2.0   memory length: 20538   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 106   score: 2.0   memory length: 20758   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 107   score: 3.0   memory length: 21008   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 108   score: 1.0   memory length: 21177   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 109   score: 1.0   memory length: 21345   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 110   score: 1.0   memory length: 21513   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 111   score: 3.0   memory length: 21780   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 112   score: 1.0   memory length: 21952   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 113   score: 1.0   memory length: 22103   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 114   score: 1.0   memory length: 22274   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 115   score: 3.0   memory length: 22482   epsilon: 1.0    steps: 208    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 116   score: 2.0   memory length: 22680   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 117   score: 2.0   memory length: 22898   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 118   score: 0.0   memory length: 23020   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 119   score: 3.0   memory length: 23248   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 120   score: 1.0   memory length: 23399   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 121   score: 2.0   memory length: 23596   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 122   score: 0.0   memory length: 23718   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 123   score: 2.0   memory length: 23915   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 124   score: 2.0   memory length: 24112   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 125   score: 3.0   memory length: 24356   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 126   score: 0.0   memory length: 24479   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 127   score: 0.0   memory length: 24602   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 128   score: 2.0   memory length: 24802   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 129   score: 2.0   memory length: 25002   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 130   score: 2.0   memory length: 25200   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 131   score: 0.0   memory length: 25323   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 132   score: 0.0   memory length: 25446   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 133   score: 0.0   memory length: 25569   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 134   score: 3.0   memory length: 25795   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 135   score: 1.0   memory length: 25965   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 136   score: 1.0   memory length: 26116   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 137   score: 0.0   memory length: 26239   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 138   score: 4.0   memory length: 26536   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 139   score: 1.0   memory length: 26686   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 140   score: 0.0   memory length: 26808   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 141   score: 0.0   memory length: 26931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 142   score: 0.0   memory length: 27053   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 143   score: 1.0   memory length: 27203   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 144   score: 0.0   memory length: 27325   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 145   score: 1.0   memory length: 27495   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 146   score: 1.0   memory length: 27664   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 147   score: 0.0   memory length: 27787   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 148   score: 4.0   memory length: 28062   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 149   score: 2.0   memory length: 28261   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 150   score: 0.0   memory length: 28384   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 151   score: 1.0   memory length: 28556   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 152   score: 3.0   memory length: 28785   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 153   score: 0.0   memory length: 28908   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 154   score: 1.0   memory length: 29058   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 155   score: 2.0   memory length: 29255   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 156   score: 1.0   memory length: 29424   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 157   score: 1.0   memory length: 29575   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 158   score: 0.0   memory length: 29698   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 159   score: 2.0   memory length: 29879   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 160   score: 0.0   memory length: 30002   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 161   score: 0.0   memory length: 30125   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 162   score: 3.0   memory length: 30355   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 163   score: 1.0   memory length: 30524   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 164   score: 3.0   memory length: 30770   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 165   score: 3.0   memory length: 31034   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 166   score: 3.0   memory length: 31281   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 167   score: 5.0   memory length: 31626   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 168   score: 5.0   memory length: 31950   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 169   score: 1.0   memory length: 32119   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 170   score: 3.0   memory length: 32364   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 171   score: 1.0   memory length: 32515   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 172   score: 1.0   memory length: 32684   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 173   score: 1.0   memory length: 32855   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 174   score: 2.0   memory length: 33054   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 175   score: 1.0   memory length: 33205   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 176   score: 2.0   memory length: 33403   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 177   score: 2.0   memory length: 33623   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 178   score: 2.0   memory length: 33820   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 179   score: 2.0   memory length: 34036   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 180   score: 1.0   memory length: 34187   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 181   score: 1.0   memory length: 34337   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 182   score: 2.0   memory length: 34535   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 183   score: 1.0   memory length: 34704   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 184   score: 0.0   memory length: 34827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 185   score: 4.0   memory length: 35103   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 186   score: 3.0   memory length: 35329   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 187   score: 3.0   memory length: 35559   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 188   score: 4.0   memory length: 35876   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 189   score: 3.0   memory length: 36107   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 190   score: 2.0   memory length: 36304   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 191   score: 0.0   memory length: 36426   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 192   score: 1.0   memory length: 36577   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 193   score: 3.0   memory length: 36823   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 194   score: 2.0   memory length: 37023   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 195   score: 1.0   memory length: 37192   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 196   score: 0.0   memory length: 37315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 197   score: 1.0   memory length: 37466   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 198   score: 0.0   memory length: 37589   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 199   score: 0.0   memory length: 37712   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 200   score: 1.0   memory length: 37884   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 201   score: 3.0   memory length: 38128   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 202   score: 1.0   memory length: 38296   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 203   score: 0.0   memory length: 38418   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 204   score: 2.0   memory length: 38638   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 205   score: 0.0   memory length: 38760   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 206   score: 2.0   memory length: 38958   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 207   score: 0.0   memory length: 39081   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 208   score: 0.0   memory length: 39203   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 209   score: 2.0   memory length: 39400   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 210   score: 2.0   memory length: 39601   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 211   score: 2.0   memory length: 39782   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 212   score: 0.0   memory length: 39904   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 213   score: 3.0   memory length: 40150   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 214   score: 2.0   memory length: 40350   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 215   score: 0.0   memory length: 40472   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 216   score: 0.0   memory length: 40594   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 217   score: 2.0   memory length: 40814   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 218   score: 0.0   memory length: 40937   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 219   score: 0.0   memory length: 41060   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 220   score: 7.0   memory length: 41443   epsilon: 1.0    steps: 383    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 221   score: 4.0   memory length: 41722   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 222   score: 0.0   memory length: 41844   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 223   score: 1.0   memory length: 41995   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 224   score: 4.0   memory length: 42291   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 225   score: 0.0   memory length: 42414   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 226   score: 2.0   memory length: 42594   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 227   score: 0.0   memory length: 42717   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 228   score: 2.0   memory length: 42935   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 229   score: 1.0   memory length: 43104   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 230   score: 0.0   memory length: 43226   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 231   score: 3.0   memory length: 43492   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 232   score: 0.0   memory length: 43615   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 233   score: 0.0   memory length: 43737   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 234   score: 0.0   memory length: 43860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 235   score: 0.0   memory length: 43982   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 236   score: 1.0   memory length: 44133   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 237   score: 2.0   memory length: 44331   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 238   score: 1.0   memory length: 44499   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 239   score: 2.0   memory length: 44698   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 240   score: 2.0   memory length: 44878   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 241   score: 0.0   memory length: 45001   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 242   score: 1.0   memory length: 45170   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 243   score: 0.0   memory length: 45293   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 244   score: 0.0   memory length: 45416   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 245   score: 3.0   memory length: 45641   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 246   score: 2.0   memory length: 45843   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 247   score: 0.0   memory length: 45966   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 248   score: 6.0   memory length: 46338   epsilon: 1.0    steps: 372    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 249   score: 0.0   memory length: 46460   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 250   score: 1.0   memory length: 46629   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 251   score: 2.0   memory length: 46830   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 252   score: 2.0   memory length: 47048   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 253   score: 2.0   memory length: 47263   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 254   score: 3.0   memory length: 47529   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 255   score: 3.0   memory length: 47799   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 256   score: 1.0   memory length: 47949   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 257   score: 0.0   memory length: 48071   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 258   score: 0.0   memory length: 48194   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 259   score: 0.0   memory length: 48317   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 260   score: 1.0   memory length: 48485   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 261   score: 2.0   memory length: 48683   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 262   score: 0.0   memory length: 48805   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 263   score: 3.0   memory length: 49033   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 264   score: 0.0   memory length: 49156   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 265   score: 1.0   memory length: 49307   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 266   score: 3.0   memory length: 49533   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 267   score: 2.0   memory length: 49731   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 268   score: 0.0   memory length: 49854   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 269   score: 0.0   memory length: 49976   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 270   score: 2.0   memory length: 50174   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 271   score: 3.0   memory length: 50441   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 272   score: 1.0   memory length: 50610   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 273   score: 1.0   memory length: 50779   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 274   score: 1.0   memory length: 50930   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 275   score: 1.0   memory length: 51100   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 276   score: 1.0   memory length: 51251   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 277   score: 2.0   memory length: 51432   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 278   score: 2.0   memory length: 51630   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 279   score: 2.0   memory length: 51828   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 280   score: 0.0   memory length: 51951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 281   score: 1.0   memory length: 52121   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 282   score: 1.0   memory length: 52273   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 283   score: 1.0   memory length: 52423   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 284   score: 4.0   memory length: 52718   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 285   score: 0.0   memory length: 52840   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 286   score: 1.0   memory length: 52991   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 287   score: 0.0   memory length: 53114   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 288   score: 0.0   memory length: 53237   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 289   score: 1.0   memory length: 53388   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 290   score: 1.0   memory length: 53559   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 291   score: 1.0   memory length: 53709   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 292   score: 2.0   memory length: 53889   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 293   score: 0.0   memory length: 54011   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 294   score: 0.0   memory length: 54133   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 295   score: 0.0   memory length: 54256   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 296   score: 8.0   memory length: 54705   epsilon: 1.0    steps: 449    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 297   score: 0.0   memory length: 54828   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 298   score: 0.0   memory length: 54951   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 299   score: 4.0   memory length: 55227   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 300   score: 4.0   memory length: 55541   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 301   score: 1.0   memory length: 55711   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 302   score: 0.0   memory length: 55833   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 303   score: 0.0   memory length: 55956   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 304   score: 1.0   memory length: 56124   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 305   score: 0.0   memory length: 56246   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 306   score: 0.0   memory length: 56369   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 307   score: 4.0   memory length: 56670   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 308   score: 4.0   memory length: 56964   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 309   score: 2.0   memory length: 57182   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 310   score: 0.0   memory length: 57305   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 311   score: 1.0   memory length: 57475   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 312   score: 2.0   memory length: 57693   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 313   score: 2.0   memory length: 57891   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 314   score: 0.0   memory length: 58014   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 315   score: 0.0   memory length: 58137   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 316   score: 0.0   memory length: 58259   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 317   score: 2.0   memory length: 58456   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 318   score: 2.0   memory length: 58674   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 319   score: 1.0   memory length: 58824   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 320   score: 2.0   memory length: 59039   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 321   score: 0.0   memory length: 59162   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 322   score: 1.0   memory length: 59333   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 323   score: 1.0   memory length: 59484   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 324   score: 0.0   memory length: 59607   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 325   score: 0.0   memory length: 59730   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 326   score: 3.0   memory length: 59975   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 327   score: 1.0   memory length: 60146   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 328   score: 1.0   memory length: 60296   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 329   score: 5.0   memory length: 60661   epsilon: 1.0    steps: 365    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 330   score: 2.0   memory length: 60859   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 331   score: 0.0   memory length: 60982   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 332   score: 1.0   memory length: 61152   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 333   score: 0.0   memory length: 61275   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 334   score: 2.0   memory length: 61472   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 335   score: 2.0   memory length: 61691   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 336   score: 2.0   memory length: 61907   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 337   score: 3.0   memory length: 62154   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 338   score: 0.0   memory length: 62277   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 339   score: 0.0   memory length: 62399   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 340   score: 0.0   memory length: 62522   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 341   score: 4.0   memory length: 62780   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 342   score: 0.0   memory length: 62903   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 343   score: 0.0   memory length: 63025   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 344   score: 2.0   memory length: 63241   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 345   score: 2.0   memory length: 63458   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 346   score: 0.0   memory length: 63580   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 347   score: 2.0   memory length: 63777   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 348   score: 2.0   memory length: 63992   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 349   score: 0.0   memory length: 64114   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 350   score: 0.0   memory length: 64237   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 351   score: 1.0   memory length: 64387   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 352   score: 0.0   memory length: 64510   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 353   score: 2.0   memory length: 64726   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 354   score: 5.0   memory length: 65073   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 355   score: 4.0   memory length: 65368   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 356   score: 3.0   memory length: 65594   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 357   score: 1.0   memory length: 65763   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 358   score: 0.0   memory length: 65886   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 359   score: 3.0   memory length: 66153   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 360   score: 3.0   memory length: 66397   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 361   score: 0.0   memory length: 66520   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 362   score: 2.0   memory length: 66738   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 363   score: 3.0   memory length: 66964   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 364   score: 0.0   memory length: 67087   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 365   score: 1.0   memory length: 67238   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 366   score: 1.0   memory length: 67407   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 367   score: 0.0   memory length: 67529   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 368   score: 2.0   memory length: 67729   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 369   score: 2.0   memory length: 67927   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 370   score: 0.0   memory length: 68049   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 371   score: 0.0   memory length: 68171   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 372   score: 0.0   memory length: 68294   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 373   score: 1.0   memory length: 68462   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 374   score: 3.0   memory length: 68688   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 375   score: 1.0   memory length: 68839   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 376   score: 1.0   memory length: 68990   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 377   score: 1.0   memory length: 69159   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 378   score: 1.0   memory length: 69330   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 379   score: 0.0   memory length: 69453   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 380   score: 1.0   memory length: 69622   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 381   score: 0.0   memory length: 69744   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 382   score: 0.0   memory length: 69867   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 383   score: 0.0   memory length: 69989   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 384   score: 2.0   memory length: 70168   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 385   score: 3.0   memory length: 70417   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 386   score: 2.0   memory length: 70597   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 387   score: 3.0   memory length: 70824   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 388   score: 3.0   memory length: 71071   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 389   score: 1.0   memory length: 71222   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 390   score: 1.0   memory length: 71373   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 391   score: 0.0   memory length: 71495   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 392   score: 0.0   memory length: 71618   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 393   score: 1.0   memory length: 71768   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 394   score: 2.0   memory length: 71966   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 395   score: 2.0   memory length: 72163   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 396   score: 2.0   memory length: 72361   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 397   score: 2.0   memory length: 72559   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 398   score: 2.0   memory length: 72779   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 399   score: 3.0   memory length: 73029   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 400   score: 0.0   memory length: 73152   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 401   score: 2.0   memory length: 73352   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 402   score: 1.0   memory length: 73521   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 403   score: 3.0   memory length: 73769   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 404   score: 0.0   memory length: 73892   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 405   score: 0.0   memory length: 74015   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 406   score: 1.0   memory length: 74186   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 407   score: 0.0   memory length: 74309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 408   score: 2.0   memory length: 74507   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 409   score: 0.0   memory length: 74630   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 410   score: 3.0   memory length: 74856   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 411   score: 0.0   memory length: 74978   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 412   score: 0.0   memory length: 75100   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 413   score: 0.0   memory length: 75223   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 414   score: 2.0   memory length: 75420   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 415   score: 0.0   memory length: 75542   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 416   score: 1.0   memory length: 75693   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 417   score: 2.0   memory length: 75913   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 418   score: 0.0   memory length: 76035   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 419   score: 0.0   memory length: 76157   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 420   score: 10.0   memory length: 76577   epsilon: 1.0    steps: 420    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 421   score: 0.0   memory length: 76700   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 422   score: 0.0   memory length: 76823   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 423   score: 0.0   memory length: 76946   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 424   score: 5.0   memory length: 77291   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 425   score: 1.0   memory length: 77461   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 426   score: 0.0   memory length: 77584   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 427   score: 2.0   memory length: 77801   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 428   score: 0.0   memory length: 77924   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 429   score: 1.0   memory length: 78094   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 430   score: 0.0   memory length: 78216   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 431   score: 1.0   memory length: 78367   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 432   score: 2.0   memory length: 78547   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 433   score: 1.0   memory length: 78697   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 434   score: 2.0   memory length: 78894   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 435   score: 4.0   memory length: 79169   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 436   score: 2.0   memory length: 79354   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 437   score: 2.0   memory length: 79551   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 438   score: 2.0   memory length: 79732   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 439   score: 2.0   memory length: 79929   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 440   score: 1.0   memory length: 80100   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 441   score: 0.0   memory length: 80223   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 442   score: 0.0   memory length: 80346   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 443   score: 2.0   memory length: 80564   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 444   score: 0.0   memory length: 80687   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 445   score: 0.0   memory length: 80810   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 446   score: 0.0   memory length: 80933   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 447   score: 4.0   memory length: 81223   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 448   score: 3.0   memory length: 81473   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 449   score: 2.0   memory length: 81691   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 450   score: 2.0   memory length: 81909   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 451   score: 1.0   memory length: 82078   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 452   score: 5.0   memory length: 82386   epsilon: 1.0    steps: 308    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 453   score: 3.0   memory length: 82653   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 454   score: 1.0   memory length: 82822   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 455   score: 0.0   memory length: 82945   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 456   score: 1.0   memory length: 83096   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 457   score: 0.0   memory length: 83219   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 458   score: 0.0   memory length: 83341   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 459   score: 2.0   memory length: 83539   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 460   score: 2.0   memory length: 83754   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 461   score: 2.0   memory length: 83938   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 462   score: 1.0   memory length: 84110   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 463   score: 1.0   memory length: 84261   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 464   score: 2.0   memory length: 84458   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 465   score: 2.0   memory length: 84677   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 466   score: 2.0   memory length: 84876   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 467   score: 0.0   memory length: 84998   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 468   score: 0.0   memory length: 85120   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 469   score: 0.0   memory length: 85242   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 470   score: 1.0   memory length: 85393   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 471   score: 2.0   memory length: 85591   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 472   score: 2.0   memory length: 85789   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 473   score: 1.0   memory length: 85961   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 474   score: 2.0   memory length: 86158   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 475   score: 3.0   memory length: 86404   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 476   score: 5.0   memory length: 86745   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 477   score: 0.0   memory length: 86867   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 478   score: 0.0   memory length: 86990   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 479   score: 1.0   memory length: 87159   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 480   score: 5.0   memory length: 87503   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 481   score: 1.0   memory length: 87653   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 482   score: 1.0   memory length: 87822   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 483   score: 4.0   memory length: 88088   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 484   score: 2.0   memory length: 88286   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 485   score: 0.0   memory length: 88409   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 486   score: 1.0   memory length: 88578   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 487   score: 1.0   memory length: 88729   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 488   score: 2.0   memory length: 88930   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 489   score: 0.0   memory length: 89052   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 490   score: 0.0   memory length: 89175   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 491   score: 2.0   memory length: 89373   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 492   score: 0.0   memory length: 89495   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 493   score: 2.0   memory length: 89716   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 494   score: 1.0   memory length: 89866   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 495   score: 0.0   memory length: 89989   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 496   score: 2.0   memory length: 90205   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 497   score: 0.0   memory length: 90328   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 498   score: 3.0   memory length: 90558   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 499   score: 3.0   memory length: 90789   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 500   score: 2.0   memory length: 90988   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 501   score: 1.0   memory length: 91139   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 502   score: 0.0   memory length: 91261   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 503   score: 0.0   memory length: 91384   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 504   score: 4.0   memory length: 91681   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 505   score: 0.0   memory length: 91803   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 506   score: 4.0   memory length: 92118   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 507   score: 0.0   memory length: 92241   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 508   score: 1.0   memory length: 92391   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 509   score: 1.0   memory length: 92562   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 510   score: 0.0   memory length: 92685   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 511   score: 0.0   memory length: 92807   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 512   score: 0.0   memory length: 92929   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 513   score: 3.0   memory length: 93176   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 514   score: 1.0   memory length: 93345   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 515   score: 0.0   memory length: 93468   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 516   score: 1.0   memory length: 93619   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 517   score: 0.0   memory length: 93741   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 518   score: 0.0   memory length: 93863   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 519   score: 4.0   memory length: 94160   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 520   score: 3.0   memory length: 94426   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 521   score: 0.0   memory length: 94549   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 522   score: 3.0   memory length: 94795   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 523   score: 0.0   memory length: 94917   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 524   score: 0.0   memory length: 95040   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 525   score: 1.0   memory length: 95211   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 526   score: 2.0   memory length: 95427   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 527   score: 0.0   memory length: 95549   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 528   score: 1.0   memory length: 95718   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 529   score: 2.0   memory length: 95935   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 530   score: 2.0   memory length: 96135   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 531   score: 1.0   memory length: 96286   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 532   score: 0.0   memory length: 96409   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 533   score: 0.0   memory length: 96531   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 534   score: 1.0   memory length: 96700   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 535   score: 0.0   memory length: 96822   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 536   score: 2.0   memory length: 97040   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 537   score: 2.0   memory length: 97238   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 538   score: 3.0   memory length: 97486   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 539   score: 0.0   memory length: 97609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 540   score: 1.0   memory length: 97777   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 541   score: 1.0   memory length: 97946   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 542   score: 2.0   memory length: 98144   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 543   score: 3.0   memory length: 98370   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 544   score: 0.0   memory length: 98492   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 545   score: 2.0   memory length: 98690   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 546   score: 1.0   memory length: 98858   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 547   score: 0.0   memory length: 98981   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 548   score: 0.0   memory length: 99104   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 549   score: 1.0   memory length: 99256   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 550   score: 1.0   memory length: 99425   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 551   score: 4.0   memory length: 99721   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 552   score: 2.0   memory length: 99919   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 553   score: 0.0   memory length: 100041   epsilon: 0.9999168400000018    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 554   score: 2.0   memory length: 100238   epsilon: 0.9995267800000103    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 555   score: 2.0   memory length: 100436   epsilon: 0.9991347400000188    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 556   score: 0.0   memory length: 100558   epsilon: 0.998893180000024    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 557   score: 3.0   memory length: 100786   epsilon: 0.9984417400000338    steps: 228    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 558   score: 0.0   memory length: 100909   epsilon: 0.9981982000000391    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 559   score: 3.0   memory length: 101157   epsilon: 0.9977071600000498    steps: 248    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 560   score: 0.0   memory length: 101280   epsilon: 0.9974636200000551    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 561   score: 1.0   memory length: 101430   epsilon: 0.9971666200000615    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 562   score: 0.0   memory length: 101552   epsilon: 0.9969250600000668    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 563   score: 3.0   memory length: 101796   epsilon: 0.9964419400000772    steps: 244    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 564   score: 1.0   memory length: 101947   epsilon: 0.9961429600000837    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 565   score: 3.0   memory length: 102194   epsilon: 0.9956539000000943    steps: 247    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 566   score: 1.0   memory length: 102345   epsilon: 0.9953549200001008    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 567   score: 4.0   memory length: 102609   epsilon: 0.9948322000001122    steps: 264    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 568   score: 0.0   memory length: 102732   epsilon: 0.9945886600001175    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 569   score: 4.0   memory length: 103010   epsilon: 0.9940382200001294    steps: 278    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 570   score: 0.0   memory length: 103133   epsilon: 0.9937946800001347    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 571   score: 2.0   memory length: 103331   epsilon: 0.9934026400001432    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 572   score: 2.0   memory length: 103529   epsilon: 0.9930106000001517    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 573   score: 0.0   memory length: 103652   epsilon: 0.992767060000157    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 574   score: 1.0   memory length: 103823   epsilon: 0.9924284800001644    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 575   score: 0.0   memory length: 103946   epsilon: 0.9921849400001697    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 576   score: 2.0   memory length: 104164   epsilon: 0.991753300000179    steps: 218    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 577   score: 1.0   memory length: 104333   epsilon: 0.9914186800001863    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 578   score: 0.0   memory length: 104456   epsilon: 0.9911751400001916    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 579   score: 2.0   memory length: 104654   epsilon: 0.9907831000002001    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 580   score: 0.0   memory length: 104777   epsilon: 0.9905395600002054    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 581   score: 0.0   memory length: 104900   epsilon: 0.9902960200002107    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 582   score: 1.0   memory length: 105069   epsilon: 0.9899614000002179    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 583   score: 2.0   memory length: 105267   epsilon: 0.9895693600002264    steps: 198    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 584   score: 0.0   memory length: 105390   epsilon: 0.9893258200002317    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 585   score: 4.0   memory length: 105706   epsilon: 0.9887001400002453    steps: 316    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 586   score: 2.0   memory length: 105923   epsilon: 0.9882704800002546    steps: 217    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 587   score: 4.0   memory length: 106198   epsilon: 0.9877259800002665    steps: 275    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 588   score: 0.0   memory length: 106321   epsilon: 0.9874824400002717    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 589   score: 2.0   memory length: 106537   epsilon: 0.987054760000281    steps: 216    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 590   score: 1.0   memory length: 106688   epsilon: 0.9867557800002875    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 591   score: 0.0   memory length: 106811   epsilon: 0.9865122400002928    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 592   score: 3.0   memory length: 107040   epsilon: 0.9860588200003026    steps: 229    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 593   score: 4.0   memory length: 107299   epsilon: 0.9855460000003138    steps: 259    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 594   score: 1.0   memory length: 107468   epsilon: 0.985211380000321    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 595   score: 2.0   memory length: 107666   epsilon: 0.9848193400003296    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 596   score: 0.0   memory length: 107788   epsilon: 0.9845777800003348    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 597   score: 1.0   memory length: 107958   epsilon: 0.9842411800003421    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 598   score: 2.0   memory length: 108173   epsilon: 0.9838154800003514    steps: 215    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 599   score: 3.0   memory length: 108384   epsilon: 0.9833977000003604    steps: 211    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 600   score: 0.0   memory length: 108507   epsilon: 0.9831541600003657    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 601   score: 2.0   memory length: 108705   epsilon: 0.9827621200003742    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 602   score: 2.0   memory length: 108903   epsilon: 0.9823700800003827    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 603   score: 1.0   memory length: 109075   epsilon: 0.9820295200003901    steps: 172    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 604   score: 1.0   memory length: 109244   epsilon: 0.9816949000003974    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 605   score: 2.0   memory length: 109460   epsilon: 0.9812672200004067    steps: 216    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 606   score: 2.0   memory length: 109675   epsilon: 0.9808415200004159    steps: 215    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 607   score: 2.0   memory length: 109873   epsilon: 0.9804494800004244    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 608   score: 0.0   memory length: 109996   epsilon: 0.9802059400004297    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 609   score: 1.0   memory length: 110164   epsilon: 0.9798733000004369    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 610   score: 1.0   memory length: 110333   epsilon: 0.9795386800004442    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 611   score: 0.0   memory length: 110455   epsilon: 0.9792971200004494    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 612   score: 1.0   memory length: 110623   epsilon: 0.9789644800004567    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 613   score: 0.0   memory length: 110745   epsilon: 0.9787229200004619    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 614   score: 3.0   memory length: 110989   epsilon: 0.9782398000004724    steps: 244    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 615   score: 2.0   memory length: 111210   epsilon: 0.9778022200004819    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 616   score: 1.0   memory length: 111380   epsilon: 0.9774656200004892    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 617   score: 3.0   memory length: 111647   epsilon: 0.9769369600005007    steps: 267    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 618   score: 1.0   memory length: 111798   epsilon: 0.9766379800005072    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 619   score: 2.0   memory length: 111996   epsilon: 0.9762459400005157    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 620   score: 2.0   memory length: 112193   epsilon: 0.9758558800005241    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 621   score: 4.0   memory length: 112509   epsilon: 0.9752302000005377    steps: 316    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 622   score: 0.0   memory length: 112631   epsilon: 0.974988640000543    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 623   score: 3.0   memory length: 112898   epsilon: 0.9744599800005544    steps: 267    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 624   score: 2.0   memory length: 113096   epsilon: 0.974067940000563    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 625   score: 3.0   memory length: 113361   epsilon: 0.9735432400005744    steps: 265    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 626   score: 0.0   memory length: 113484   epsilon: 0.9732997000005796    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 627   score: 2.0   memory length: 113702   epsilon: 0.972868060000589    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 628   score: 0.0   memory length: 113825   epsilon: 0.9726245200005943    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 629   score: 1.0   memory length: 113997   epsilon: 0.9722839600006017    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 630   score: 1.0   memory length: 114167   epsilon: 0.971947360000609    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 631   score: 1.0   memory length: 114335   epsilon: 0.9716147200006162    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 632   score: 2.0   memory length: 114533   epsilon: 0.9712226800006247    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 633   score: 1.0   memory length: 114702   epsilon: 0.970888060000632    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 634   score: 1.0   memory length: 114871   epsilon: 0.9705534400006393    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 635   score: 4.0   memory length: 115145   epsilon: 0.970010920000651    steps: 274    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 636   score: 4.0   memory length: 115441   epsilon: 0.9694248400006638    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 637   score: 0.0   memory length: 115564   epsilon: 0.969181300000669    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 638   score: 5.0   memory length: 115856   epsilon: 0.9686031400006816    steps: 292    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 639   score: 1.0   memory length: 116025   epsilon: 0.9682685200006889    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 640   score: 0.0   memory length: 116147   epsilon: 0.9680269600006941    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 641   score: 0.0   memory length: 116269   epsilon: 0.9677854000006993    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 642   score: 1.0   memory length: 116419   epsilon: 0.9674884000007058    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 643   score: 0.0   memory length: 116542   epsilon: 0.9672448600007111    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 644   score: 2.0   memory length: 116739   epsilon: 0.9668548000007195    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 645   score: 2.0   memory length: 116939   epsilon: 0.9664588000007281    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 646   score: 4.0   memory length: 117194   epsilon: 0.9659539000007391    steps: 255    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 647   score: 3.0   memory length: 117419   epsilon: 0.9655084000007488    steps: 225    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 648   score: 0.0   memory length: 117541   epsilon: 0.965266840000754    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 649   score: 0.0   memory length: 117663   epsilon: 0.9650252800007593    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 650   score: 2.0   memory length: 117881   epsilon: 0.9645936400007686    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 651   score: 1.0   memory length: 118033   epsilon: 0.9642926800007752    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 652   score: 1.0   memory length: 118184   epsilon: 0.9639937000007817    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 653   score: 3.0   memory length: 118412   epsilon: 0.9635422600007915    steps: 228    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 654   score: 0.0   memory length: 118535   epsilon: 0.9632987200007967    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 655   score: 1.0   memory length: 118704   epsilon: 0.962964100000804    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 656   score: 1.0   memory length: 118875   epsilon: 0.9626255200008114    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 657   score: 1.0   memory length: 119043   epsilon: 0.9622928800008186    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 658   score: 4.0   memory length: 119317   epsilon: 0.9617503600008304    steps: 274    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 659   score: 1.0   memory length: 119468   epsilon: 0.9614513800008369    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 660   score: 1.0   memory length: 119637   epsilon: 0.9611167600008441    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 661   score: 3.0   memory length: 119905   epsilon: 0.9605861200008556    steps: 268    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 662   score: 2.0   memory length: 120086   epsilon: 0.9602277400008634    steps: 181    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 663   score: 3.0   memory length: 120334   epsilon: 0.9597367000008741    steps: 248    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 664   score: 0.0   memory length: 120457   epsilon: 0.9594931600008794    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 665   score: 2.0   memory length: 120673   epsilon: 0.9590654800008886    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 666   score: 0.0   memory length: 120796   epsilon: 0.9588219400008939    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 667   score: 1.0   memory length: 120946   epsilon: 0.9585249400009004    steps: 150    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 668   score: 1.0   memory length: 121116   epsilon: 0.9581883400009077    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 669   score: 1.0   memory length: 121284   epsilon: 0.9578557000009149    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 670   score: 7.0   memory length: 121704   epsilon: 0.957024100000933    steps: 420    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 671   score: 2.0   memory length: 121922   epsilon: 0.9565924600009423    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 672   score: 3.0   memory length: 122171   epsilon: 0.956099440000953    steps: 249    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 673   score: 1.0   memory length: 122322   epsilon: 0.9558004600009595    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 674   score: 0.0   memory length: 122444   epsilon: 0.9555589000009648    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 675   score: 0.0   memory length: 122567   epsilon: 0.9553153600009701    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 676   score: 1.0   memory length: 122736   epsilon: 0.9549807400009773    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 677   score: 4.0   memory length: 123052   epsilon: 0.9543550600009909    steps: 316    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 678   score: 1.0   memory length: 123220   epsilon: 0.9540224200009981    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 679   score: 3.0   memory length: 123464   epsilon: 0.9535393000010086    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 680   score: 2.0   memory length: 123661   epsilon: 0.9531492400010171    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 681   score: 0.0   memory length: 123784   epsilon: 0.9529057000010224    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 682   score: 1.0   memory length: 123953   epsilon: 0.9525710800010296    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 683   score: 4.0   memory length: 124270   epsilon: 0.9519434200010433    steps: 317    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 684   score: 2.0   memory length: 124468   epsilon: 0.9515513800010518    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 685   score: 0.0   memory length: 124591   epsilon: 0.9513078400010571    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 686   score: 2.0   memory length: 124771   epsilon: 0.9509514400010648    steps: 180    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 687   score: 0.0   memory length: 124894   epsilon: 0.9507079000010701    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 688   score: 3.0   memory length: 125120   epsilon: 0.9502604200010798    steps: 226    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 689   score: 1.0   memory length: 125271   epsilon: 0.9499614400010863    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 690   score: 2.0   memory length: 125469   epsilon: 0.9495694000010948    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 691   score: 3.0   memory length: 125717   epsilon: 0.9490783600011055    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 692   score: 3.0   memory length: 125981   epsilon: 0.9485556400011168    steps: 264    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 693   score: 0.0   memory length: 126104   epsilon: 0.9483121000011221    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 694   score: 3.0   memory length: 126351   epsilon: 0.9478230400011327    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 695   score: 0.0   memory length: 126474   epsilon: 0.947579500001138    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 696   score: 2.0   memory length: 126692   epsilon: 0.9471478600011474    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 697   score: 1.0   memory length: 126864   epsilon: 0.9468073000011548    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 698   score: 0.0   memory length: 126987   epsilon: 0.94656376000116    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 699   score: 1.0   memory length: 127157   epsilon: 0.9462271600011674    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 700   score: 1.0   memory length: 127326   epsilon: 0.9458925400011746    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 701   score: 2.0   memory length: 127545   epsilon: 0.945458920001184    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 702   score: 2.0   memory length: 127764   epsilon: 0.9450253000011934    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 703   score: 0.0   memory length: 127887   epsilon: 0.9447817600011987    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 704   score: 0.0   memory length: 128009   epsilon: 0.944540200001204    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 705   score: 2.0   memory length: 128207   epsilon: 0.9441481600012125    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 706   score: 2.0   memory length: 128423   epsilon: 0.9437204800012218    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 707   score: 1.0   memory length: 128574   epsilon: 0.9434215000012283    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 708   score: 0.0   memory length: 128697   epsilon: 0.9431779600012336    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 709   score: 2.0   memory length: 128882   epsilon: 0.9428116600012415    steps: 185    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 710   score: 2.0   memory length: 129082   epsilon: 0.9424156600012501    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 711   score: 1.0   memory length: 129250   epsilon: 0.9420830200012573    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 712   score: 2.0   memory length: 129447   epsilon: 0.9416929600012658    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 713   score: 1.0   memory length: 129598   epsilon: 0.9413939800012723    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 714   score: 1.0   memory length: 129769   epsilon: 0.9410554000012796    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 715   score: 2.0   memory length: 129984   epsilon: 0.9406297000012889    steps: 215    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 716   score: 4.0   memory length: 130278   epsilon: 0.9400475800013015    steps: 294    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 717   score: 1.0   memory length: 130447   epsilon: 0.9397129600013088    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 718   score: 1.0   memory length: 130617   epsilon: 0.9393763600013161    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 719   score: 0.0   memory length: 130740   epsilon: 0.9391328200013214    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 720   score: 2.0   memory length: 130941   epsilon: 0.93873484000133    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 721   score: 0.0   memory length: 131064   epsilon: 0.9384913000013353    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 722   score: 0.0   memory length: 131187   epsilon: 0.9382477600013406    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 723   score: 4.0   memory length: 131486   epsilon: 0.9376557400013534    steps: 299    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 724   score: 0.0   memory length: 131609   epsilon: 0.9374122000013587    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 725   score: 0.0   memory length: 131731   epsilon: 0.937170640001364    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 726   score: 3.0   memory length: 131960   epsilon: 0.9367172200013738    steps: 229    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 727   score: 2.0   memory length: 132178   epsilon: 0.9362855800013832    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 728   score: 2.0   memory length: 132375   epsilon: 0.9358955200013916    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 729   score: 2.0   memory length: 132591   epsilon: 0.9354678400014009    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 730   score: 0.0   memory length: 132714   epsilon: 0.9352243000014062    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 731   score: 3.0   memory length: 132980   epsilon: 0.9346976200014177    steps: 266    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 732   score: 0.0   memory length: 133102   epsilon: 0.9344560600014229    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 733   score: 0.0   memory length: 133225   epsilon: 0.9342125200014282    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 734   score: 4.0   memory length: 133523   epsilon: 0.933622480001441    steps: 298    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 735   score: 1.0   memory length: 133692   epsilon: 0.9332878600014483    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 736   score: 0.0   memory length: 133815   epsilon: 0.9330443200014535    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 737   score: 0.0   memory length: 133937   epsilon: 0.9328027600014588    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 738   score: 0.0   memory length: 134060   epsilon: 0.9325592200014641    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 739   score: 1.0   memory length: 134229   epsilon: 0.9322246000014713    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 740   score: 4.0   memory length: 134504   epsilon: 0.9316801000014832    steps: 275    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 741   score: 2.0   memory length: 134721   epsilon: 0.9312504400014925    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 742   score: 2.0   memory length: 134918   epsilon: 0.930860380001501    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 743   score: 1.0   memory length: 135087   epsilon: 0.9305257600015082    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 744   score: 0.0   memory length: 135209   epsilon: 0.9302842000015135    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 745   score: 2.0   memory length: 135427   epsilon: 0.9298525600015228    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 746   score: 2.0   memory length: 135626   epsilon: 0.9294585400015314    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 747   score: 5.0   memory length: 135953   epsilon: 0.9288110800015454    steps: 327    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 748   score: 1.0   memory length: 136125   epsilon: 0.9284705200015528    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 749   score: 2.0   memory length: 136341   epsilon: 0.9280428400015621    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 750   score: 1.0   memory length: 136491   epsilon: 0.9277458400015686    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 751   score: 0.0   memory length: 136614   epsilon: 0.9275023000015739    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 752   score: 1.0   memory length: 136765   epsilon: 0.9272033200015803    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 753   score: 1.0   memory length: 136934   epsilon: 0.9268687000015876    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 754   score: 1.0   memory length: 137103   epsilon: 0.9265340800015949    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 755   score: 2.0   memory length: 137301   epsilon: 0.9261420400016034    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 756   score: 0.0   memory length: 137423   epsilon: 0.9259004800016086    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 757   score: 3.0   memory length: 137651   epsilon: 0.9254490400016184    steps: 228    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 758   score: 2.0   memory length: 137869   epsilon: 0.9250174000016278    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 759   score: 1.0   memory length: 138038   epsilon: 0.9246827800016351    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 760   score: 3.0   memory length: 138302   epsilon: 0.9241600600016464    steps: 264    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 761   score: 2.0   memory length: 138499   epsilon: 0.9237700000016549    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 762   score: 1.0   memory length: 138668   epsilon: 0.9234353800016621    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 763   score: 3.0   memory length: 138897   epsilon: 0.922981960001672    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 764   score: 5.0   memory length: 139242   epsilon: 0.9222988600016868    steps: 345    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 765   score: 2.0   memory length: 139440   epsilon: 0.9219068200016953    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 766   score: 0.0   memory length: 139562   epsilon: 0.9216652600017006    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 767   score: 5.0   memory length: 139887   epsilon: 0.9210217600017145    steps: 325    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 768   score: 3.0   memory length: 140137   epsilon: 0.9205267600017253    steps: 250    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 769   score: 1.0   memory length: 140309   epsilon: 0.9201862000017327    steps: 172    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 770   score: 0.0   memory length: 140431   epsilon: 0.9199446400017379    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 771   score: 0.0   memory length: 140553   epsilon: 0.9197030800017432    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 772   score: 0.0   memory length: 140675   epsilon: 0.9194615200017484    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 773   score: 1.0   memory length: 140826   epsilon: 0.9191625400017549    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 774   score: 0.0   memory length: 140948   epsilon: 0.9189209800017601    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 775   score: 1.0   memory length: 141098   epsilon: 0.9186239800017666    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 776   score: 1.0   memory length: 141267   epsilon: 0.9182893600017739    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 777   score: 0.0   memory length: 141389   epsilon: 0.9180478000017791    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 778   score: 5.0   memory length: 141677   epsilon: 0.9174775600017915    steps: 288    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 779   score: 4.0   memory length: 141933   epsilon: 0.9169706800018025    steps: 256    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 780   score: 1.0   memory length: 142104   epsilon: 0.9166321000018098    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 781   score: 0.0   memory length: 142227   epsilon: 0.9163885600018151    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 782   score: 2.0   memory length: 142443   epsilon: 0.9159608800018244    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 783   score: 1.0   memory length: 142595   epsilon: 0.9156599200018309    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 784   score: 2.0   memory length: 142815   epsilon: 0.9152243200018404    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 785   score: 2.0   memory length: 143013   epsilon: 0.9148322800018489    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 786   score: 2.0   memory length: 143194   epsilon: 0.9144739000018567    steps: 181    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 787   score: 0.0   memory length: 143316   epsilon: 0.9142323400018619    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 788   score: 3.0   memory length: 143543   epsilon: 0.9137828800018717    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 789   score: 2.0   memory length: 143761   epsilon: 0.9133512400018811    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 790   score: 4.0   memory length: 144015   epsilon: 0.912848320001892    steps: 254    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 791   score: 2.0   memory length: 144212   epsilon: 0.9124582600019004    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 792   score: 2.0   memory length: 144430   epsilon: 0.9120266200019098    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 793   score: 2.0   memory length: 144629   epsilon: 0.9116326000019184    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 794   score: 0.0   memory length: 144752   epsilon: 0.9113890600019237    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 795   score: 3.0   memory length: 144978   epsilon: 0.9109415800019334    steps: 226    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 796   score: 2.0   memory length: 145194   epsilon: 0.9105139000019427    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 797   score: 0.0   memory length: 145316   epsilon: 0.9102723400019479    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 798   score: 2.0   memory length: 145513   epsilon: 0.9098822800019564    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 799   score: 0.0   memory length: 145636   epsilon: 0.9096387400019617    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 800   score: 0.0   memory length: 145759   epsilon: 0.9093952000019669    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 801   score: 2.0   memory length: 145956   epsilon: 0.9090051400019754    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 802   score: 5.0   memory length: 146263   epsilon: 0.9083972800019886    steps: 307    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 803   score: 2.0   memory length: 146479   epsilon: 0.9079696000019979    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 804   score: 0.0   memory length: 146602   epsilon: 0.9077260600020032    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 805   score: 3.0   memory length: 146865   epsilon: 0.9072053200020145    steps: 263    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 806   score: 0.0   memory length: 146987   epsilon: 0.9069637600020197    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 807   score: 4.0   memory length: 147247   epsilon: 0.9064489600020309    steps: 260    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 808   score: 2.0   memory length: 147445   epsilon: 0.9060569200020394    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 809   score: 0.0   memory length: 147568   epsilon: 0.9058133800020447    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 810   score: 1.0   memory length: 147718   epsilon: 0.9055163800020511    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 811   score: 1.0   memory length: 147868   epsilon: 0.9052193800020576    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 812   score: 0.0   memory length: 147991   epsilon: 0.9049758400020629    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 813   score: 2.0   memory length: 148172   epsilon: 0.9046174600020707    steps: 181    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 814   score: 4.0   memory length: 148452   epsilon: 0.9040630600020827    steps: 280    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 815   score: 0.0   memory length: 148574   epsilon: 0.9038215000020879    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 816   score: 1.0   memory length: 148745   epsilon: 0.9034829200020953    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 817   score: 2.0   memory length: 148943   epsilon: 0.9030908800021038    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 818   score: 2.0   memory length: 149159   epsilon: 0.9026632000021131    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 819   score: 0.0   memory length: 149281   epsilon: 0.9024216400021183    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 820   score: 1.0   memory length: 149432   epsilon: 0.9021226600021248    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 821   score: 1.0   memory length: 149601   epsilon: 0.9017880400021321    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 822   score: 1.0   memory length: 149771   epsilon: 0.9014514400021394    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 823   score: 1.0   memory length: 149923   epsilon: 0.9011504800021459    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 824   score: 1.0   memory length: 150093   epsilon: 0.9008138800021532    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 825   score: 2.0   memory length: 150294   epsilon: 0.9004159000021619    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 826   score: 4.0   memory length: 150572   epsilon: 0.8998654600021738    steps: 278    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 827   score: 1.0   memory length: 150743   epsilon: 0.8995268800021812    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 828   score: 3.0   memory length: 150989   epsilon: 0.8990398000021917    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 829   score: 6.0   memory length: 151340   epsilon: 0.8983448200022068    steps: 351    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 830   score: 4.0   memory length: 151635   epsilon: 0.8977607200022195    steps: 295    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 831   score: 1.0   memory length: 151804   epsilon: 0.8974261000022268    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 832   score: 0.0   memory length: 151927   epsilon: 0.8971825600022321    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 833   score: 1.0   memory length: 152078   epsilon: 0.8968835800022386    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 834   score: 2.0   memory length: 152293   epsilon: 0.8964578800022478    steps: 215    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 835   score: 1.0   memory length: 152462   epsilon: 0.8961232600022551    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 836   score: 0.0   memory length: 152585   epsilon: 0.8958797200022603    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 837   score: 0.0   memory length: 152707   epsilon: 0.8956381600022656    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 838   score: 2.0   memory length: 152905   epsilon: 0.8952461200022741    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 839   score: 6.0   memory length: 153318   epsilon: 0.8944283800022919    steps: 413    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 840   score: 2.0   memory length: 153516   epsilon: 0.8940363400023004    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 841   score: 0.0   memory length: 153639   epsilon: 0.8937928000023057    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 842   score: 1.0   memory length: 153790   epsilon: 0.8934938200023121    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 843   score: 2.0   memory length: 154010   epsilon: 0.8930582200023216    steps: 220    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 844   score: 3.0   memory length: 154254   epsilon: 0.8925751000023321    steps: 244    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 845   score: 3.0   memory length: 154502   epsilon: 0.8920840600023427    steps: 248    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 846   score: 2.0   memory length: 154699   epsilon: 0.8916940000023512    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 847   score: 1.0   memory length: 154850   epsilon: 0.8913950200023577    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 848   score: 0.0   memory length: 154973   epsilon: 0.891151480002363    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 849   score: 2.0   memory length: 155171   epsilon: 0.8907594400023715    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 850   score: 0.0   memory length: 155294   epsilon: 0.8905159000023768    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 851   score: 1.0   memory length: 155465   epsilon: 0.8901773200023841    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 852   score: 1.0   memory length: 155616   epsilon: 0.8898783400023906    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 853   score: 1.0   memory length: 155787   epsilon: 0.889539760002398    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 854   score: 1.0   memory length: 155938   epsilon: 0.8892407800024045    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 855   score: 1.0   memory length: 156107   epsilon: 0.8889061600024117    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 856   score: 1.0   memory length: 156276   epsilon: 0.888571540002419    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 857   score: 2.0   memory length: 156455   epsilon: 0.8882171200024267    steps: 179    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 858   score: 2.0   memory length: 156652   epsilon: 0.8878270600024352    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 859   score: 2.0   memory length: 156870   epsilon: 0.8873954200024445    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 860   score: 2.0   memory length: 157068   epsilon: 0.887003380002453    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 861   score: 2.0   memory length: 157266   epsilon: 0.8866113400024616    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 862   score: 0.0   memory length: 157389   epsilon: 0.8863678000024668    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 863   score: 2.0   memory length: 157571   epsilon: 0.8860074400024747    steps: 182    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 864   score: 5.0   memory length: 157895   epsilon: 0.8853659200024886    steps: 324    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 865   score: 3.0   memory length: 158143   epsilon: 0.8848748800024993    steps: 248    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 866   score: 0.0   memory length: 158266   epsilon: 0.8846313400025045    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 867   score: 0.0   memory length: 158389   epsilon: 0.8843878000025098    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 868   score: 0.0   memory length: 158511   epsilon: 0.8841462400025151    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 869   score: 1.0   memory length: 158663   epsilon: 0.8838452800025216    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 870   score: 1.0   memory length: 158813   epsilon: 0.883548280002528    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 871   score: 0.0   memory length: 158936   epsilon: 0.8833047400025333    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 872   score: 0.0   memory length: 159059   epsilon: 0.8830612000025386    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 873   score: 1.0   memory length: 159230   epsilon: 0.882722620002546    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 874   score: 1.0   memory length: 159398   epsilon: 0.8823899800025532    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 875   score: 0.0   memory length: 159521   epsilon: 0.8821464400025585    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 876   score: 0.0   memory length: 159644   epsilon: 0.8819029000025638    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 877   score: 3.0   memory length: 159908   epsilon: 0.8813801800025751    steps: 264    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 878   score: 1.0   memory length: 160076   epsilon: 0.8810475400025823    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 879   score: 2.0   memory length: 160291   epsilon: 0.8806218400025916    steps: 215    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 880   score: 2.0   memory length: 160489   epsilon: 0.8802298000026001    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 881   score: 0.0   memory length: 160611   epsilon: 0.8799882400026053    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 882   score: 2.0   memory length: 160809   epsilon: 0.8795962000026138    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 883   score: 2.0   memory length: 161027   epsilon: 0.8791645600026232    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 884   score: 1.0   memory length: 161196   epsilon: 0.8788299400026305    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 885   score: 0.0   memory length: 161318   epsilon: 0.8785883800026357    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 886   score: 2.0   memory length: 161536   epsilon: 0.8781567400026451    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 887   score: 2.0   memory length: 161736   epsilon: 0.8777607400026537    steps: 200    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 888   score: 1.0   memory length: 161887   epsilon: 0.8774617600026602    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 889   score: 1.0   memory length: 162056   epsilon: 0.8771271400026674    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 890   score: 0.0   memory length: 162179   epsilon: 0.8768836000026727    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 891   score: 0.0   memory length: 162302   epsilon: 0.876640060002678    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 892   score: 1.0   memory length: 162471   epsilon: 0.8763054400026853    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 893   score: 1.0   memory length: 162621   epsilon: 0.8760084400026917    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 894   score: 2.0   memory length: 162819   epsilon: 0.8756164000027002    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 895   score: 1.0   memory length: 162988   epsilon: 0.8752817800027075    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 896   score: 1.0   memory length: 163157   epsilon: 0.8749471600027148    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 897   score: 2.0   memory length: 163355   epsilon: 0.8745551200027233    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 898   score: 2.0   memory length: 163558   epsilon: 0.874153180002732    steps: 203    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 899   score: 1.0   memory length: 163709   epsilon: 0.8738542000027385    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 900   score: 0.0   memory length: 163832   epsilon: 0.8736106600027438    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 901   score: 1.0   memory length: 163984   epsilon: 0.8733097000027503    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 902   score: 1.0   memory length: 164134   epsilon: 0.8730127000027568    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 903   score: 3.0   memory length: 164405   epsilon: 0.8724761200027684    steps: 271    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 904   score: 2.0   memory length: 164604   epsilon: 0.872082100002777    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 905   score: 0.0   memory length: 164726   epsilon: 0.8718405400027822    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 906   score: 0.0   memory length: 164849   epsilon: 0.8715970000027875    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 907   score: 2.0   memory length: 165047   epsilon: 0.871204960002796    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 908   score: 0.0   memory length: 165169   epsilon: 0.8709634000028013    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 909   score: 1.0   memory length: 165320   epsilon: 0.8706644200028077    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 910   score: 0.0   memory length: 165443   epsilon: 0.870420880002813    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 911   score: 0.0   memory length: 165566   epsilon: 0.8701773400028183    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 912   score: 1.0   memory length: 165735   epsilon: 0.8698427200028256    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 913   score: 1.0   memory length: 165905   epsilon: 0.8695061200028329    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 914   score: 3.0   memory length: 166153   epsilon: 0.8690150800028436    steps: 248    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 915   score: 0.0   memory length: 166276   epsilon: 0.8687715400028488    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 916   score: 7.0   memory length: 166700   epsilon: 0.8679320200028671    steps: 424    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 917   score: 0.0   memory length: 166822   epsilon: 0.8676904600028723    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 918   score: 1.0   memory length: 166972   epsilon: 0.8673934600028788    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 919   score: 2.0   memory length: 167170   epsilon: 0.8670014200028873    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 920   score: 1.0   memory length: 167320   epsilon: 0.8667044200028937    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 921   score: 2.0   memory length: 167539   epsilon: 0.8662708000029031    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 922   score: 3.0   memory length: 167752   epsilon: 0.8658490600029123    steps: 213    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 923   score: 0.0   memory length: 167874   epsilon: 0.8656075000029175    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 924   score: 1.0   memory length: 168025   epsilon: 0.865308520002924    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 925   score: 0.0   memory length: 168147   epsilon: 0.8650669600029293    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 926   score: 0.0   memory length: 168270   epsilon: 0.8648234200029346    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 927   score: 2.0   memory length: 168450   epsilon: 0.8644670200029423    steps: 180    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 928   score: 0.0   memory length: 168573   epsilon: 0.8642234800029476    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 929   score: 3.0   memory length: 168817   epsilon: 0.8637403600029581    steps: 244    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 930   score: 2.0   memory length: 169034   epsilon: 0.8633107000029674    steps: 217    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 931   score: 1.0   memory length: 169203   epsilon: 0.8629760800029747    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 932   score: 3.0   memory length: 169429   epsilon: 0.8625286000029844    steps: 226    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 933   score: 4.0   memory length: 169746   epsilon: 0.861900940002998    steps: 317    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 934   score: 0.0   memory length: 169868   epsilon: 0.8616593800030032    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 935   score: 2.0   memory length: 170065   epsilon: 0.8612693200030117    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 936   score: 2.0   memory length: 170262   epsilon: 0.8608792600030202    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 937   score: 1.0   memory length: 170430   epsilon: 0.8605466200030274    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 938   score: 2.0   memory length: 170628   epsilon: 0.8601545800030359    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 939   score: 2.0   memory length: 170826   epsilon: 0.8597625400030444    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 940   score: 3.0   memory length: 171070   epsilon: 0.8592794200030549    steps: 244    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 941   score: 0.0   memory length: 171193   epsilon: 0.8590358800030602    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 942   score: 0.0   memory length: 171316   epsilon: 0.8587923400030655    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 943   score: 2.0   memory length: 171514   epsilon: 0.858400300003074    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 944   score: 0.0   memory length: 171637   epsilon: 0.8581567600030793    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 945   score: 3.0   memory length: 171863   epsilon: 0.857709280003089    steps: 226    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 946   score: 1.0   memory length: 172014   epsilon: 0.8574103000030955    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 947   score: 2.0   memory length: 172211   epsilon: 0.857020240003104    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 948   score: 0.0   memory length: 172334   epsilon: 0.8567767000031092    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 949   score: 0.0   memory length: 172457   epsilon: 0.8565331600031145    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 950   score: 2.0   memory length: 172673   epsilon: 0.8561054800031238    steps: 216    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 951   score: 1.0   memory length: 172843   epsilon: 0.8557688800031311    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 952   score: 5.0   memory length: 173167   epsilon: 0.855127360003145    steps: 324    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 953   score: 2.0   memory length: 173365   epsilon: 0.8547353200031536    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 954   score: 2.0   memory length: 173563   epsilon: 0.8543432800031621    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 955   score: 2.0   memory length: 173761   epsilon: 0.8539512400031706    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 956   score: 1.0   memory length: 173930   epsilon: 0.8536166200031778    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 957   score: 2.0   memory length: 174148   epsilon: 0.8531849800031872    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 958   score: 3.0   memory length: 174398   epsilon: 0.852689980003198    steps: 250    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 959   score: 1.0   memory length: 174549   epsilon: 0.8523910000032044    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 960   score: 3.0   memory length: 174792   epsilon: 0.8519098600032149    steps: 243    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 961   score: 1.0   memory length: 174942   epsilon: 0.8516128600032213    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 962   score: 2.0   memory length: 175141   epsilon: 0.8512188400032299    steps: 199    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 963   score: 0.0   memory length: 175264   epsilon: 0.8509753000032352    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 964   score: 4.0   memory length: 175558   epsilon: 0.8503931800032478    steps: 294    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 965   score: 1.0   memory length: 175709   epsilon: 0.8500942000032543    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 966   score: 1.0   memory length: 175878   epsilon: 0.8497595800032616    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 967   score: 2.0   memory length: 176096   epsilon: 0.8493279400032709    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 968   score: 2.0   memory length: 176314   epsilon: 0.8488963000032803    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 969   score: 4.0   memory length: 176632   epsilon: 0.848266660003294    steps: 318    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 970   score: 2.0   memory length: 176812   epsilon: 0.8479102600033017    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 971   score: 5.0   memory length: 177158   epsilon: 0.8472251800033166    steps: 346    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 972   score: 1.0   memory length: 177328   epsilon: 0.8468885800033239    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 973   score: 1.0   memory length: 177500   epsilon: 0.8465480200033313    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 974   score: 0.0   memory length: 177622   epsilon: 0.8463064600033365    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 975   score: 3.0   memory length: 177888   epsilon: 0.845779780003348    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 976   score: 0.0   memory length: 178011   epsilon: 0.8455362400033533    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 977   score: 3.0   memory length: 178257   epsilon: 0.8450491600033638    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 978   score: 0.0   memory length: 178380   epsilon: 0.8448056200033691    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 979   score: 1.0   memory length: 178531   epsilon: 0.8445066400033756    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 980   score: 0.0   memory length: 178654   epsilon: 0.8442631000033809    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 981   score: 0.0   memory length: 178776   epsilon: 0.8440215400033861    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 982   score: 2.0   memory length: 178974   epsilon: 0.8436295000033947    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 983   score: 2.0   memory length: 179154   epsilon: 0.8432731000034024    steps: 180    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 984   score: 2.0   memory length: 179351   epsilon: 0.8428830400034109    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 985   score: 3.0   memory length: 179595   epsilon: 0.8423999200034213    steps: 244    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 986   score: 1.0   memory length: 179746   epsilon: 0.8421009400034278    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 987   score: 1.0   memory length: 179917   epsilon: 0.8417623600034352    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 988   score: 0.0   memory length: 180040   epsilon: 0.8415188200034405    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 989   score: 4.0   memory length: 180318   epsilon: 0.8409683800034524    steps: 278    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 990   score: 4.0   memory length: 180592   epsilon: 0.8404258600034642    steps: 274    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 991   score: 2.0   memory length: 180774   epsilon: 0.840065500003472    steps: 182    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 992   score: 5.0   memory length: 181118   epsilon: 0.8393843800034868    steps: 344    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 993   score: 0.0   memory length: 181240   epsilon: 0.839142820003492    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 994   score: 3.0   memory length: 181487   epsilon: 0.8386537600035027    steps: 247    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 995   score: 1.0   memory length: 181655   epsilon: 0.8383211200035099    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 996   score: 1.0   memory length: 181823   epsilon: 0.8379884800035171    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 997   score: 3.0   memory length: 182088   epsilon: 0.8374637800035285    steps: 265    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 998   score: 1.0   memory length: 182239   epsilon: 0.837164800003535    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 999   score: 1.0   memory length: 182408   epsilon: 0.8368301800035423    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1000   score: 0.0   memory length: 182531   epsilon: 0.8365866400035475    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1001   score: 0.0   memory length: 182654   epsilon: 0.8363431000035528    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1002   score: 0.0   memory length: 182776   epsilon: 0.8361015400035581    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1003   score: 3.0   memory length: 183022   epsilon: 0.8356144600035686    steps: 246    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1004   score: 4.0   memory length: 183282   epsilon: 0.8350996600035798    steps: 260    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1005   score: 2.0   memory length: 183502   epsilon: 0.8346640600035893    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1006   score: 1.0   memory length: 183673   epsilon: 0.8343254800035966    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1007   score: 2.0   memory length: 183889   epsilon: 0.8338978000036059    steps: 216    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1008   score: 2.0   memory length: 184071   epsilon: 0.8335374400036137    steps: 182    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1009   score: 2.0   memory length: 184289   epsilon: 0.8331058000036231    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1010   score: 1.0   memory length: 184440   epsilon: 0.8328068200036296    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1011   score: 3.0   memory length: 184707   epsilon: 0.8322781600036411    steps: 267    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1012   score: 1.0   memory length: 184876   epsilon: 0.8319435400036483    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1013   score: 2.0   memory length: 185093   epsilon: 0.8315138800036577    steps: 217    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1014   score: 2.0   memory length: 185290   epsilon: 0.8311238200036661    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1015   score: 2.0   memory length: 185487   epsilon: 0.8307337600036746    steps: 197    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1016   score: 3.0   memory length: 185731   epsilon: 0.8302506400036851    steps: 244    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1017   score: 3.0   memory length: 185999   epsilon: 0.8297200000036966    steps: 268    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1018   score: 1.0   memory length: 186150   epsilon: 0.8294210200037031    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1019   score: 1.0   memory length: 186319   epsilon: 0.8290864000037104    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1020   score: 3.0   memory length: 186567   epsilon: 0.828595360003721    steps: 248    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1021   score: 1.0   memory length: 186717   epsilon: 0.8282983600037275    steps: 150    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1022   score: 0.0   memory length: 186840   epsilon: 0.8280548200037328    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1023   score: 3.0   memory length: 187087   epsilon: 0.8275657600037434    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1024   score: 0.0   memory length: 187209   epsilon: 0.8273242000037486    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1025   score: 0.0   memory length: 187331   epsilon: 0.8270826400037539    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1026   score: 3.0   memory length: 187574   epsilon: 0.8266015000037643    steps: 243    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1027   score: 1.0   memory length: 187725   epsilon: 0.8263025200037708    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1028   score: 1.0   memory length: 187875   epsilon: 0.8260055200037772    steps: 150    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1029   score: 0.0   memory length: 187997   epsilon: 0.8257639600037825    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1030   score: 2.0   memory length: 188194   epsilon: 0.825373900003791    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1031   score: 3.0   memory length: 188405   epsilon: 0.8249561200038    steps: 211    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1032   score: 2.0   memory length: 188602   epsilon: 0.8245660600038085    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1033   score: 4.0   memory length: 188903   epsilon: 0.8239700800038214    steps: 301    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1034   score: 3.0   memory length: 189150   epsilon: 0.823481020003832    steps: 247    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1035   score: 2.0   memory length: 189368   epsilon: 0.8230493800038414    steps: 218    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1036   score: 1.0   memory length: 189539   epsilon: 0.8227108000038488    steps: 171    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1037   score: 2.0   memory length: 189737   epsilon: 0.8223187600038573    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1038   score: 1.0   memory length: 189907   epsilon: 0.8219821600038646    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1039   score: 3.0   memory length: 190136   epsilon: 0.8215287400038744    steps: 229    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1040   score: 1.0   memory length: 190305   epsilon: 0.8211941200038817    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1041   score: 6.0   memory length: 190674   epsilon: 0.8204635000038976    steps: 369    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1042   score: 0.0   memory length: 190796   epsilon: 0.8202219400039028    steps: 122    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1043   score: 2.0   memory length: 190994   epsilon: 0.8198299000039113    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1044   score: 3.0   memory length: 191220   epsilon: 0.819382420003921    steps: 226    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1045   score: 2.0   memory length: 191437   epsilon: 0.8189527600039304    steps: 217    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1046   score: 1.0   memory length: 191606   epsilon: 0.8186181400039376    steps: 169    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1047   score: 1.0   memory length: 191775   epsilon: 0.8182835200039449    steps: 169    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1048   score: 5.0   memory length: 192102   epsilon: 0.8176360600039589    steps: 327    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1049   score: 0.0   memory length: 192225   epsilon: 0.8173925200039642    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1050   score: 2.0   memory length: 192423   epsilon: 0.8170004800039727    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1051   score: 1.0   memory length: 192592   epsilon: 0.81666586000398    steps: 169    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1052   score: 2.0   memory length: 192791   epsilon: 0.8162718400039886    steps: 199    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1053   score: 0.0   memory length: 192914   epsilon: 0.8160283000039938    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1054   score: 0.0   memory length: 193036   epsilon: 0.8157867400039991    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1055   score: 0.0   memory length: 193158   epsilon: 0.8155451800040043    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1056   score: 2.0   memory length: 193375   epsilon: 0.8151155200040137    steps: 217    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1057   score: 6.0   memory length: 193733   epsilon: 0.814406680004029    steps: 358    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1058   score: 1.0   memory length: 193883   epsilon: 0.8141096800040355    steps: 150    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1059   score: 3.0   memory length: 194128   epsilon: 0.813624580004046    steps: 245    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1060   score: 4.0   memory length: 194403   epsilon: 0.8130800800040578    steps: 275    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1061   score: 2.0   memory length: 194621   epsilon: 0.8126484400040672    steps: 218    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1062   score: 0.0   memory length: 194743   epsilon: 0.8124068800040725    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1063   score: 3.0   memory length: 194988   epsilon: 0.811921780004083    steps: 245    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1064   score: 4.0   memory length: 195244   epsilon: 0.811414900004094    steps: 256    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1065   score: 0.0   memory length: 195367   epsilon: 0.8111713600040993    steps: 123    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1066   score: 0.0   memory length: 195490   epsilon: 0.8109278200041046    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1067   score: 0.0   memory length: 195613   epsilon: 0.8106842800041099    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1068   score: 1.0   memory length: 195764   epsilon: 0.8103853000041163    steps: 151    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1069   score: 1.0   memory length: 195932   epsilon: 0.8100526600041236    steps: 168    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1070   score: 2.0   memory length: 196150   epsilon: 0.8096210200041329    steps: 218    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1071   score: 0.0   memory length: 196272   epsilon: 0.8093794600041382    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1072   score: 1.0   memory length: 196423   epsilon: 0.8090804800041447    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1073   score: 7.0   memory length: 196832   epsilon: 0.8082706600041623    steps: 409    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1074   score: 2.0   memory length: 197030   epsilon: 0.8078786200041708    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1075   score: 1.0   memory length: 197199   epsilon: 0.807544000004178    steps: 169    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1076   score: 2.0   memory length: 197396   epsilon: 0.8071539400041865    steps: 197    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1077   score: 4.0   memory length: 197655   epsilon: 0.8066411200041976    steps: 259    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1078   score: 2.0   memory length: 197836   epsilon: 0.8062827400042054    steps: 181    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1079   score: 4.0   memory length: 198110   epsilon: 0.8057402200042172    steps: 274    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1080   score: 3.0   memory length: 198359   epsilon: 0.8052472000042279    steps: 249    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1081   score: 3.0   memory length: 198606   epsilon: 0.8047581400042385    steps: 247    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1082   score: 1.0   memory length: 198757   epsilon: 0.804459160004245    steps: 151    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1083   score: 4.0   memory length: 199056   epsilon: 0.8038671400042579    steps: 299    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1084   score: 0.0   memory length: 199178   epsilon: 0.8036255800042631    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1085   score: 2.0   memory length: 199376   epsilon: 0.8032335400042716    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1086   score: 1.0   memory length: 199547   epsilon: 0.802894960004279    steps: 171    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1087   score: 3.0   memory length: 199790   epsilon: 0.8024138200042894    steps: 243    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1088   score: 3.0   memory length: 200034   epsilon: 0.8019307000042999    steps: 244    lr: 4e-05     evaluation reward: 1.93\n",
      "episode: 1089   score: 1.0   memory length: 200206   epsilon: 0.8015901400043073    steps: 172    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1090   score: 2.0   memory length: 200404   epsilon: 0.8011981000043158    steps: 198    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1091   score: 1.0   memory length: 200554   epsilon: 0.8009011000043222    steps: 150    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1092   score: 2.0   memory length: 200752   epsilon: 0.8005090600043308    steps: 198    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1093   score: 2.0   memory length: 200953   epsilon: 0.8001110800043394    steps: 201    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1094   score: 1.0   memory length: 201103   epsilon: 0.7998140800043458    steps: 150    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1095   score: 3.0   memory length: 201329   epsilon: 0.7993666000043556    steps: 226    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1096   score: 2.0   memory length: 201527   epsilon: 0.7989745600043641    steps: 198    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1097   score: 0.0   memory length: 201650   epsilon: 0.7987310200043694    steps: 123    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1098   score: 4.0   memory length: 201961   epsilon: 0.7981152400043827    steps: 311    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1099   score: 2.0   memory length: 202143   epsilon: 0.7977548800043905    steps: 182    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1100   score: 0.0   memory length: 202265   epsilon: 0.7975133200043958    steps: 122    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1101   score: 2.0   memory length: 202486   epsilon: 0.7970757400044053    steps: 221    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1102   score: 2.0   memory length: 202684   epsilon: 0.7966837000044138    steps: 198    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1103   score: 0.0   memory length: 202807   epsilon: 0.7964401600044191    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1104   score: 4.0   memory length: 203083   epsilon: 0.795893680004431    steps: 276    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1105   score: 1.0   memory length: 203253   epsilon: 0.7955570800044383    steps: 170    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1106   score: 3.0   memory length: 203479   epsilon: 0.795109600004448    steps: 226    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1107   score: 2.0   memory length: 203677   epsilon: 0.7947175600044565    steps: 198    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1108   score: 4.0   memory length: 203953   epsilon: 0.7941710800044683    steps: 276    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1109   score: 0.0   memory length: 204075   epsilon: 0.7939295200044736    steps: 122    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1110   score: 0.0   memory length: 204198   epsilon: 0.7936859800044789    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1111   score: 3.0   memory length: 204427   epsilon: 0.7932325600044887    steps: 229    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1112   score: 3.0   memory length: 204653   epsilon: 0.7927850800044984    steps: 226    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1113   score: 2.0   memory length: 204869   epsilon: 0.7923574000045077    steps: 216    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1114   score: 0.0   memory length: 204991   epsilon: 0.792115840004513    steps: 122    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1115   score: 2.0   memory length: 205188   epsilon: 0.7917257800045214    steps: 197    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1116   score: 4.0   memory length: 205464   epsilon: 0.7911793000045333    steps: 276    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1117   score: 1.0   memory length: 205635   epsilon: 0.7908407200045406    steps: 171    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1118   score: 3.0   memory length: 205884   epsilon: 0.7903477000045513    steps: 249    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1119   score: 3.0   memory length: 206151   epsilon: 0.7898190400045628    steps: 267    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1120   score: 0.0   memory length: 206274   epsilon: 0.7895755000045681    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1121   score: 3.0   memory length: 206519   epsilon: 0.7890904000045786    steps: 245    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1122   score: 0.0   memory length: 206642   epsilon: 0.7888468600045839    steps: 123    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1123   score: 1.0   memory length: 206813   epsilon: 0.7885082800045913    steps: 171    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1124   score: 1.0   memory length: 206964   epsilon: 0.7882093000045978    steps: 151    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1125   score: 2.0   memory length: 207167   epsilon: 0.7878073600046065    steps: 203    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1126   score: 3.0   memory length: 207411   epsilon: 0.787324240004617    steps: 244    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1127   score: 3.0   memory length: 207637   epsilon: 0.7868767600046267    steps: 226    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1128   score: 3.0   memory length: 207850   epsilon: 0.7864550200046359    steps: 213    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1129   score: 1.0   memory length: 208000   epsilon: 0.7861580200046423    steps: 150    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1130   score: 1.0   memory length: 208150   epsilon: 0.7858610200046487    steps: 150    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1131   score: 2.0   memory length: 208368   epsilon: 0.7854293800046581    steps: 218    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1132   score: 2.0   memory length: 208566   epsilon: 0.7850373400046666    steps: 198    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1133   score: 1.0   memory length: 208716   epsilon: 0.7847403400046731    steps: 150    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1134   score: 2.0   memory length: 208916   epsilon: 0.7843443400046817    steps: 200    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1135   score: 0.0   memory length: 209039   epsilon: 0.784100800004687    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1136   score: 0.0   memory length: 209162   epsilon: 0.7838572600046922    steps: 123    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1137   score: 3.0   memory length: 209374   epsilon: 0.7834375000047014    steps: 212    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1138   score: 1.0   memory length: 209525   epsilon: 0.7831385200047078    steps: 151    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1139   score: 2.0   memory length: 209726   epsilon: 0.7827405400047165    steps: 201    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1140   score: 3.0   memory length: 209970   epsilon: 0.782257420004727    steps: 244    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1141   score: 2.0   memory length: 210167   epsilon: 0.7818673600047354    steps: 197    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1142   score: 0.0   memory length: 210289   epsilon: 0.7816258000047407    steps: 122    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1143   score: 4.0   memory length: 210546   epsilon: 0.7811169400047517    steps: 257    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1144   score: 3.0   memory length: 210792   epsilon: 0.7806298600047623    steps: 246    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1145   score: 0.0   memory length: 210914   epsilon: 0.7803883000047676    steps: 122    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1146   score: 1.0   memory length: 211065   epsilon: 0.780089320004774    steps: 151    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1147   score: 1.0   memory length: 211234   epsilon: 0.7797547000047813    steps: 169    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1148   score: 4.0   memory length: 211519   epsilon: 0.7791904000047936    steps: 285    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1149   score: 2.0   memory length: 211735   epsilon: 0.7787627200048028    steps: 216    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1150   score: 2.0   memory length: 211933   epsilon: 0.7783706800048114    steps: 198    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1151   score: 0.0   memory length: 212056   epsilon: 0.7781271400048166    steps: 123    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1152   score: 2.0   memory length: 212254   epsilon: 0.7777351000048252    steps: 198    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1153   score: 0.0   memory length: 212377   epsilon: 0.7774915600048304    steps: 123    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1154   score: 1.0   memory length: 212528   epsilon: 0.7771925800048369    steps: 151    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1155   score: 0.0   memory length: 212651   epsilon: 0.7769490400048422    steps: 123    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1156   score: 1.0   memory length: 212802   epsilon: 0.7766500600048487    steps: 151    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1157   score: 7.0   memory length: 213127   epsilon: 0.7760065600048627    steps: 325    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1158   score: 4.0   memory length: 213371   epsilon: 0.7755234400048732    steps: 244    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1159   score: 2.0   memory length: 213569   epsilon: 0.7751314000048817    steps: 198    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1160   score: 3.0   memory length: 213836   epsilon: 0.7746027400048932    steps: 267    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1161   score: 4.0   memory length: 214125   epsilon: 0.7740305200049056    steps: 289    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1162   score: 0.0   memory length: 214248   epsilon: 0.7737869800049109    steps: 123    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1163   score: 4.0   memory length: 214525   epsilon: 0.7732385200049228    steps: 277    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1164   score: 1.0   memory length: 214676   epsilon: 0.7729395400049293    steps: 151    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1165   score: 0.0   memory length: 214799   epsilon: 0.7726960000049345    steps: 123    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1166   score: 2.0   memory length: 214996   epsilon: 0.772305940004943    steps: 197    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1167   score: 5.0   memory length: 215320   epsilon: 0.7716644200049569    steps: 324    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1168   score: 2.0   memory length: 215539   epsilon: 0.7712308000049664    steps: 219    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1169   score: 3.0   memory length: 215786   epsilon: 0.770741740004977    steps: 247    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1170   score: 3.0   memory length: 215999   epsilon: 0.7703200000049861    steps: 213    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1171   score: 4.0   memory length: 216294   epsilon: 0.7697359000049988    steps: 295    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1172   score: 0.0   memory length: 216416   epsilon: 0.769494340005004    steps: 122    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1173   score: 4.0   memory length: 216693   epsilon: 0.768945880005016    steps: 277    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1174   score: 1.0   memory length: 216862   epsilon: 0.7686112600050232    steps: 169    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1175   score: 5.0   memory length: 217191   epsilon: 0.7679598400050374    steps: 329    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1176   score: 2.0   memory length: 217409   epsilon: 0.7675282000050467    steps: 218    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1177   score: 3.0   memory length: 217639   epsilon: 0.7670728000050566    steps: 230    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1178   score: 3.0   memory length: 217850   epsilon: 0.7666550200050657    steps: 211    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1179   score: 1.0   memory length: 218021   epsilon: 0.766316440005073    steps: 171    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1180   score: 0.0   memory length: 218144   epsilon: 0.7660729000050783    steps: 123    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1181   score: 2.0   memory length: 218342   epsilon: 0.7656808600050868    steps: 198    lr: 4e-05     evaluation reward: 1.95\n",
      "episode: 1182   score: 3.0   memory length: 218590   epsilon: 0.7651898200050975    steps: 248    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1183   score: 4.0   memory length: 218901   epsilon: 0.7645740400051109    steps: 311    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1184   score: 3.0   memory length: 219111   epsilon: 0.7641582400051199    steps: 210    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1185   score: 3.0   memory length: 219339   epsilon: 0.7637068000051297    steps: 228    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1186   score: 3.0   memory length: 219586   epsilon: 0.7632177400051403    steps: 247    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1187   score: 3.0   memory length: 219816   epsilon: 0.7627623400051502    steps: 230    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1188   score: 3.0   memory length: 220045   epsilon: 0.76230892000516    steps: 229    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1189   score: 2.0   memory length: 220243   epsilon: 0.7619168800051686    steps: 198    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1190   score: 4.0   memory length: 220517   epsilon: 0.7613743600051803    steps: 274    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1191   score: 7.0   memory length: 220863   epsilon: 0.7606892800051952    steps: 346    lr: 4e-05     evaluation reward: 2.12\n",
      "episode: 1192   score: 3.0   memory length: 221094   epsilon: 0.7602319000052051    steps: 231    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1193   score: 4.0   memory length: 221368   epsilon: 0.7596893800052169    steps: 274    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1194   score: 1.0   memory length: 221519   epsilon: 0.7593904000052234    steps: 151    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1195   score: 2.0   memory length: 221717   epsilon: 0.7589983600052319    steps: 198    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1196   score: 0.0   memory length: 221840   epsilon: 0.7587548200052372    steps: 123    lr: 4e-05     evaluation reward: 2.12\n",
      "episode: 1197   score: 6.0   memory length: 222205   epsilon: 0.7580321200052529    steps: 365    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1198   score: 1.0   memory length: 222356   epsilon: 0.7577331400052594    steps: 151    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1199   score: 2.0   memory length: 222572   epsilon: 0.7573054600052687    steps: 216    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1200   score: 1.0   memory length: 222723   epsilon: 0.7570064800052752    steps: 151    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1201   score: 6.0   memory length: 223121   epsilon: 0.7562184400052923    steps: 398    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1202   score: 3.0   memory length: 223365   epsilon: 0.7557353200053027    steps: 244    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1203   score: 2.0   memory length: 223546   epsilon: 0.7553769400053105    steps: 181    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1204   score: 1.0   memory length: 223717   epsilon: 0.7550383600053179    steps: 171    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1205   score: 2.0   memory length: 223898   epsilon: 0.7546799800053257    steps: 181    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1206   score: 2.0   memory length: 224079   epsilon: 0.7543216000053334    steps: 181    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1207   score: 2.0   memory length: 224277   epsilon: 0.753929560005342    steps: 198    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1208   score: 4.0   memory length: 224557   epsilon: 0.753375160005354    steps: 280    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1209   score: 3.0   memory length: 224770   epsilon: 0.7529534200053631    steps: 213    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1210   score: 2.0   memory length: 224992   epsilon: 0.7525138600053727    steps: 222    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1211   score: 0.0   memory length: 225115   epsilon: 0.752270320005378    steps: 123    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1212   score: 1.0   memory length: 225284   epsilon: 0.7519357000053852    steps: 169    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1213   score: 0.0   memory length: 225407   epsilon: 0.7516921600053905    steps: 123    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1214   score: 5.0   memory length: 225735   epsilon: 0.7510427200054046    steps: 328    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1215   score: 5.0   memory length: 226061   epsilon: 0.7503972400054186    steps: 326    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1216   score: 3.0   memory length: 226289   epsilon: 0.7499458000054284    steps: 228    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1217   score: 1.0   memory length: 226440   epsilon: 0.7496468200054349    steps: 151    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1218   score: 2.0   memory length: 226637   epsilon: 0.7492567600054434    steps: 197    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1219   score: 4.0   memory length: 226894   epsilon: 0.7487479000054544    steps: 257    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1220   score: 2.0   memory length: 227076   epsilon: 0.7483875400054623    steps: 182    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1221   score: 0.0   memory length: 227199   epsilon: 0.7481440000054675    steps: 123    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1222   score: 1.0   memory length: 227349   epsilon: 0.747847000005474    steps: 150    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1223   score: 3.0   memory length: 227575   epsilon: 0.7473995200054837    steps: 226    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1224   score: 2.0   memory length: 227792   epsilon: 0.746969860005493    steps: 217    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1225   score: 2.0   memory length: 227992   epsilon: 0.7465738600055016    steps: 200    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1226   score: 3.0   memory length: 228203   epsilon: 0.7461560800055107    steps: 211    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1227   score: 3.0   memory length: 228432   epsilon: 0.7457026600055205    steps: 229    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1228   score: 3.0   memory length: 228644   epsilon: 0.7452829000055297    steps: 212    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1229   score: 3.0   memory length: 228872   epsilon: 0.7448314600055395    steps: 228    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1230   score: 4.0   memory length: 229165   epsilon: 0.744251320005552    steps: 293    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1231   score: 3.0   memory length: 229414   epsilon: 0.7437583000055628    steps: 249    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1232   score: 1.0   memory length: 229583   epsilon: 0.74342368000557    steps: 169    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1233   score: 1.0   memory length: 229754   epsilon: 0.7430851000055774    steps: 171    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1234   score: 3.0   memory length: 229965   epsilon: 0.7426673200055864    steps: 211    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1235   score: 0.0   memory length: 230088   epsilon: 0.7424237800055917    steps: 123    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1236   score: 0.0   memory length: 230211   epsilon: 0.742180240005597    steps: 123    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1237   score: 3.0   memory length: 230460   epsilon: 0.7416872200056077    steps: 249    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1238   score: 2.0   memory length: 230658   epsilon: 0.7412951800056162    steps: 198    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1239   score: 1.0   memory length: 230809   epsilon: 0.7409962000056227    steps: 151    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1240   score: 1.0   memory length: 230960   epsilon: 0.7406972200056292    steps: 151    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1241   score: 2.0   memory length: 231157   epsilon: 0.7403071600056377    steps: 197    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1242   score: 1.0   memory length: 231308   epsilon: 0.7400081800056442    steps: 151    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1243   score: 3.0   memory length: 231536   epsilon: 0.739556740005654    steps: 228    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1244   score: 1.0   memory length: 231687   epsilon: 0.7392577600056605    steps: 151    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1245   score: 5.0   memory length: 231996   epsilon: 0.7386459400056737    steps: 309    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1246   score: 1.0   memory length: 232167   epsilon: 0.7383073600056811    steps: 171    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1247   score: 2.0   memory length: 232365   epsilon: 0.7379153200056896    steps: 198    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1248   score: 3.0   memory length: 232611   epsilon: 0.7374282400057002    steps: 246    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1249   score: 0.0   memory length: 232733   epsilon: 0.7371866800057054    steps: 122    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1250   score: 4.0   memory length: 233012   epsilon: 0.7366342600057174    steps: 279    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1251   score: 1.0   memory length: 233163   epsilon: 0.7363352800057239    steps: 151    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1252   score: 1.0   memory length: 233314   epsilon: 0.7360363000057304    steps: 151    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1253   score: 1.0   memory length: 233464   epsilon: 0.7357393000057368    steps: 150    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1254   score: 0.0   memory length: 233587   epsilon: 0.7354957600057421    steps: 123    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1255   score: 1.0   memory length: 233737   epsilon: 0.7351987600057486    steps: 150    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1256   score: 0.0   memory length: 233860   epsilon: 0.7349552200057539    steps: 123    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1257   score: 5.0   memory length: 234186   epsilon: 0.7343097400057679    steps: 326    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1258   score: 0.0   memory length: 234309   epsilon: 0.7340662000057732    steps: 123    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1259   score: 2.0   memory length: 234509   epsilon: 0.7336702000057818    steps: 200    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1260   score: 1.0   memory length: 234660   epsilon: 0.7333712200057882    steps: 151    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1261   score: 2.0   memory length: 234878   epsilon: 0.7329395800057976    steps: 218    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1262   score: 3.0   memory length: 235087   epsilon: 0.7325257600058066    steps: 209    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1263   score: 2.0   memory length: 235268   epsilon: 0.7321673800058144    steps: 181    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1264   score: 5.0   memory length: 235613   epsilon: 0.7314842800058292    steps: 345    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1265   score: 0.0   memory length: 235736   epsilon: 0.7312407400058345    steps: 123    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1266   score: 1.0   memory length: 235887   epsilon: 0.730941760005841    steps: 151    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1267   score: 1.0   memory length: 236038   epsilon: 0.7306427800058475    steps: 151    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1268   score: 7.0   memory length: 236438   epsilon: 0.7298507800058647    steps: 400    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1269   score: 1.0   memory length: 236589   epsilon: 0.7295518000058712    steps: 151    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1270   score: 4.0   memory length: 236839   epsilon: 0.7290568000058819    steps: 250    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1271   score: 1.0   memory length: 236990   epsilon: 0.7287578200058884    steps: 151    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1272   score: 4.0   memory length: 237267   epsilon: 0.7282093600059003    steps: 277    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1273   score: 2.0   memory length: 237467   epsilon: 0.7278133600059089    steps: 200    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1274   score: 1.0   memory length: 237617   epsilon: 0.7275163600059154    steps: 150    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1275   score: 1.0   memory length: 237768   epsilon: 0.7272173800059218    steps: 151    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1276   score: 5.0   memory length: 238045   epsilon: 0.7266689200059337    steps: 277    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1277   score: 0.0   memory length: 238168   epsilon: 0.726425380005939    steps: 123    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1278   score: 1.0   memory length: 238339   epsilon: 0.7260868000059464    steps: 171    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1279   score: 2.0   memory length: 238521   epsilon: 0.7257264400059542    steps: 182    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1280   score: 1.0   memory length: 238671   epsilon: 0.7254294400059607    steps: 150    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1281   score: 0.0   memory length: 238794   epsilon: 0.7251859000059659    steps: 123    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1282   score: 3.0   memory length: 239005   epsilon: 0.724768120005975    steps: 211    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1283   score: 1.0   memory length: 239156   epsilon: 0.7244691400059815    steps: 151    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1284   score: 3.0   memory length: 239368   epsilon: 0.7240493800059906    steps: 212    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1285   score: 4.0   memory length: 239665   epsilon: 0.7234613200060034    steps: 297    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1286   score: 4.0   memory length: 239957   epsilon: 0.7228831600060159    steps: 292    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1287   score: 4.0   memory length: 240254   epsilon: 0.7222951000060287    steps: 297    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1288   score: 3.0   memory length: 240501   epsilon: 0.7218060400060393    steps: 247    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1289   score: 3.0   memory length: 240745   epsilon: 0.7213229200060498    steps: 244    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1290   score: 3.0   memory length: 240971   epsilon: 0.7208754400060595    steps: 226    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1291   score: 1.0   memory length: 241140   epsilon: 0.7205408200060668    steps: 169    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1292   score: 4.0   memory length: 241433   epsilon: 0.7199606800060794    steps: 293    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1293   score: 2.0   memory length: 241615   epsilon: 0.7196003200060872    steps: 182    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1294   score: 1.0   memory length: 241766   epsilon: 0.7193013400060937    steps: 151    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1295   score: 0.0   memory length: 241889   epsilon: 0.719057800006099    steps: 123    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1296   score: 2.0   memory length: 242087   epsilon: 0.7186657600061075    steps: 198    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1297   score: 2.0   memory length: 242286   epsilon: 0.718271740006116    steps: 199    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1298   score: 7.0   memory length: 242677   epsilon: 0.7174975600061329    steps: 391    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1299   score: 2.0   memory length: 242874   epsilon: 0.7171075000061413    steps: 197    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1300   score: 3.0   memory length: 243100   epsilon: 0.716660020006151    steps: 226    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1301   score: 2.0   memory length: 243281   epsilon: 0.7163016400061588    steps: 181    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1302   score: 4.0   memory length: 243538   epsilon: 0.7157927800061699    steps: 257    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1303   score: 2.0   memory length: 243739   epsilon: 0.7153948000061785    steps: 201    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1304   score: 2.0   memory length: 243919   epsilon: 0.7150384000061862    steps: 180    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1305   score: 1.0   memory length: 244089   epsilon: 0.7147018000061935    steps: 170    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1306   score: 0.0   memory length: 244211   epsilon: 0.7144602400061988    steps: 122    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1307   score: 4.0   memory length: 244486   epsilon: 0.7139157400062106    steps: 275    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1308   score: 1.0   memory length: 244637   epsilon: 0.7136167600062171    steps: 151    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1309   score: 4.0   memory length: 244933   epsilon: 0.7130306800062298    steps: 296    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1310   score: 4.0   memory length: 245209   epsilon: 0.7124842000062417    steps: 276    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1311   score: 2.0   memory length: 245391   epsilon: 0.7121238400062495    steps: 182    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1312   score: 3.0   memory length: 245602   epsilon: 0.7117060600062586    steps: 211    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1313   score: 2.0   memory length: 245802   epsilon: 0.7113100600062672    steps: 200    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1314   score: 3.0   memory length: 246053   epsilon: 0.710813080006278    steps: 251    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1315   score: 3.0   memory length: 246299   epsilon: 0.7103260000062885    steps: 246    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1316   score: 3.0   memory length: 246512   epsilon: 0.7099042600062977    steps: 213    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1317   score: 1.0   memory length: 246663   epsilon: 0.7096052800063042    steps: 151    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1318   score: 5.0   memory length: 246992   epsilon: 0.7089538600063183    steps: 329    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1319   score: 1.0   memory length: 247143   epsilon: 0.7086548800063248    steps: 151    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1320   score: 2.0   memory length: 247323   epsilon: 0.7082984800063326    steps: 180    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1321   score: 5.0   memory length: 247651   epsilon: 0.7076490400063467    steps: 328    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1322   score: 4.0   memory length: 247925   epsilon: 0.7071065200063584    steps: 274    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1323   score: 1.0   memory length: 248096   epsilon: 0.7067679400063658    steps: 171    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1324   score: 3.0   memory length: 248324   epsilon: 0.7063165000063756    steps: 228    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1325   score: 2.0   memory length: 248521   epsilon: 0.705926440006384    steps: 197    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1326   score: 8.0   memory length: 248818   epsilon: 0.7053383800063968    steps: 297    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1327   score: 1.0   memory length: 248969   epsilon: 0.7050394000064033    steps: 151    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1328   score: 1.0   memory length: 249119   epsilon: 0.7047424000064098    steps: 150    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1329   score: 3.0   memory length: 249345   epsilon: 0.7042949200064195    steps: 226    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1330   score: 4.0   memory length: 249641   epsilon: 0.7037088400064322    steps: 296    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1331   score: 6.0   memory length: 249999   epsilon: 0.7030000000064476    steps: 358    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1332   score: 4.0   memory length: 250274   epsilon: 0.7024555000064594    steps: 275    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1333   score: 4.0   memory length: 250567   epsilon: 0.701875360006472    steps: 293    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1334   score: 4.0   memory length: 250842   epsilon: 0.7013308600064838    steps: 275    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1335   score: 3.0   memory length: 251070   epsilon: 0.7008794200064936    steps: 228    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1336   score: 1.0   memory length: 251221   epsilon: 0.7005804400065001    steps: 151    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1337   score: 1.0   memory length: 251392   epsilon: 0.7002418600065075    steps: 171    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1338   score: 2.0   memory length: 251592   epsilon: 0.699845860006516    steps: 200    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1339   score: 5.0   memory length: 251939   epsilon: 0.699158800006531    steps: 347    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1340   score: 6.0   memory length: 252299   epsilon: 0.6984460000065464    steps: 360    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1341   score: 5.0   memory length: 252644   epsilon: 0.6977629000065613    steps: 345    lr: 4e-05     evaluation reward: 2.5\n",
      "episode: 1342   score: 2.0   memory length: 252824   epsilon: 0.697406500006569    steps: 180    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1343   score: 1.0   memory length: 252975   epsilon: 0.6971075200065755    steps: 151    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1344   score: 4.0   memory length: 253257   epsilon: 0.6965491600065876    steps: 282    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1345   score: 5.0   memory length: 253547   epsilon: 0.6959749600066001    steps: 290    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1346   score: 0.0   memory length: 253670   epsilon: 0.6957314200066054    steps: 123    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1347   score: 3.0   memory length: 253883   epsilon: 0.6953096800066145    steps: 213    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1348   score: 2.0   memory length: 254101   epsilon: 0.6948780400066239    steps: 218    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1349   score: 0.0   memory length: 254224   epsilon: 0.6946345000066292    steps: 123    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1350   score: 1.0   memory length: 254393   epsilon: 0.6942998800066364    steps: 169    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1351   score: 3.0   memory length: 254606   epsilon: 0.6938781400066456    steps: 213    lr: 4e-05     evaluation reward: 2.5\n",
      "episode: 1352   score: 3.0   memory length: 254833   epsilon: 0.6934286800066554    steps: 227    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1353   score: 1.0   memory length: 254984   epsilon: 0.6931297000066619    steps: 151    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1354   score: 3.0   memory length: 255210   epsilon: 0.6926822200066716    steps: 226    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1355   score: 3.0   memory length: 255454   epsilon: 0.692199100006682    steps: 244    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1356   score: 5.0   memory length: 255781   epsilon: 0.6915516400066961    steps: 327    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1357   score: 2.0   memory length: 256000   epsilon: 0.6911180200067055    steps: 219    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1358   score: 1.0   memory length: 256151   epsilon: 0.690819040006712    steps: 151    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1359   score: 4.0   memory length: 256412   epsilon: 0.6903022600067232    steps: 261    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1360   score: 3.0   memory length: 256660   epsilon: 0.6898112200067339    steps: 248    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1361   score: 3.0   memory length: 256907   epsilon: 0.6893221600067445    steps: 247    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1362   score: 2.0   memory length: 257125   epsilon: 0.6888905200067539    steps: 218    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1363   score: 3.0   memory length: 257351   epsilon: 0.6884430400067636    steps: 226    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1364   score: 3.0   memory length: 257583   epsilon: 0.6879836800067736    steps: 232    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1365   score: 2.0   memory length: 257781   epsilon: 0.6875916400067821    steps: 198    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1366   score: 0.0   memory length: 257904   epsilon: 0.6873481000067874    steps: 123    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1367   score: 3.0   memory length: 258149   epsilon: 0.6868630000067979    steps: 245    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1368   score: 2.0   memory length: 258348   epsilon: 0.6864689800068065    steps: 199    lr: 4e-05     evaluation reward: 2.61\n",
      "episode: 1369   score: 7.0   memory length: 258755   epsilon: 0.685663120006824    steps: 407    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1370   score: 5.0   memory length: 259044   epsilon: 0.6850909000068364    steps: 289    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1371   score: 3.0   memory length: 259273   epsilon: 0.6846374800068462    steps: 229    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1372   score: 4.0   memory length: 259549   epsilon: 0.6840910000068581    steps: 276    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1373   score: 3.0   memory length: 259792   epsilon: 0.6836098600068685    steps: 243    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1374   score: 2.0   memory length: 260010   epsilon: 0.6831782200068779    steps: 218    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1375   score: 1.0   memory length: 260179   epsilon: 0.6828436000068852    steps: 169    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1376   score: 3.0   memory length: 260392   epsilon: 0.6824218600068943    steps: 213    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1377   score: 3.0   memory length: 260620   epsilon: 0.6819704200069041    steps: 228    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1378   score: 2.0   memory length: 260818   epsilon: 0.6815783800069126    steps: 198    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1379   score: 1.0   memory length: 260969   epsilon: 0.6812794000069191    steps: 151    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1380   score: 2.0   memory length: 261188   epsilon: 0.6808457800069285    steps: 219    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1381   score: 3.0   memory length: 261414   epsilon: 0.6803983000069382    steps: 226    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1382   score: 3.0   memory length: 261640   epsilon: 0.679950820006948    steps: 226    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1383   score: 5.0   memory length: 262003   epsilon: 0.6792320800069636    steps: 363    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1384   score: 4.0   memory length: 262269   epsilon: 0.678705400006975    steps: 266    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1385   score: 2.0   memory length: 262469   epsilon: 0.6783094000069836    steps: 200    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1386   score: 2.0   memory length: 262667   epsilon: 0.6779173600069921    steps: 198    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1387   score: 3.0   memory length: 262880   epsilon: 0.6774956200070013    steps: 213    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1388   score: 6.0   memory length: 263254   epsilon: 0.6767551000070173    steps: 374    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1389   score: 2.0   memory length: 263452   epsilon: 0.6763630600070258    steps: 198    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1390   score: 3.0   memory length: 263698   epsilon: 0.6758759800070364    steps: 246    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1391   score: 9.0   memory length: 264009   epsilon: 0.6752602000070498    steps: 311    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1392   score: 4.0   memory length: 264287   epsilon: 0.6747097600070617    steps: 278    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1393   score: 3.0   memory length: 264534   epsilon: 0.6742207000070723    steps: 247    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1394   score: 5.0   memory length: 264823   epsilon: 0.6736484800070848    steps: 289    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1395   score: 2.0   memory length: 265020   epsilon: 0.6732584200070932    steps: 197    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1396   score: 3.0   memory length: 265267   epsilon: 0.6727693600071039    steps: 247    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1397   score: 3.0   memory length: 265495   epsilon: 0.6723179200071137    steps: 228    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1398   score: 0.0   memory length: 265618   epsilon: 0.6720743800071189    steps: 123    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1399   score: 3.0   memory length: 265826   epsilon: 0.6716625400071279    steps: 208    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1400   score: 2.0   memory length: 266026   epsilon: 0.6712665400071365    steps: 200    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1401   score: 5.0   memory length: 266321   epsilon: 0.6706824400071492    steps: 295    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1402   score: 2.0   memory length: 266518   epsilon: 0.6702923800071576    steps: 197    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1403   score: 2.0   memory length: 266718   epsilon: 0.6698963800071662    steps: 200    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1404   score: 2.0   memory length: 266918   epsilon: 0.6695003800071748    steps: 200    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1405   score: 4.0   memory length: 267197   epsilon: 0.6689479600071868    steps: 279    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1406   score: 8.0   memory length: 267642   epsilon: 0.6680668600072059    steps: 445    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1407   score: 1.0   memory length: 267793   epsilon: 0.6677678800072124    steps: 151    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1408   score: 3.0   memory length: 268021   epsilon: 0.6673164400072222    steps: 228    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1409   score: 3.0   memory length: 268247   epsilon: 0.666868960007232    steps: 226    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1410   score: 1.0   memory length: 268398   epsilon: 0.6665699800072384    steps: 151    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1411   score: 3.0   memory length: 268624   epsilon: 0.6661225000072482    steps: 226    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1412   score: 1.0   memory length: 268775   epsilon: 0.6658235200072546    steps: 151    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1413   score: 2.0   memory length: 268973   epsilon: 0.6654314800072632    steps: 198    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1414   score: 5.0   memory length: 269268   epsilon: 0.6648473800072758    steps: 295    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1415   score: 6.0   memory length: 269659   epsilon: 0.6640732000072926    steps: 391    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1416   score: 3.0   memory length: 269889   epsilon: 0.6636178000073025    steps: 230    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1417   score: 3.0   memory length: 270119   epsilon: 0.6631624000073124    steps: 230    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1418   score: 3.0   memory length: 270347   epsilon: 0.6627109600073222    steps: 228    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1419   score: 5.0   memory length: 270672   epsilon: 0.6620674600073362    steps: 325    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1420   score: 3.0   memory length: 270885   epsilon: 0.6616457200073453    steps: 213    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1421   score: 1.0   memory length: 271055   epsilon: 0.6613091200073526    steps: 170    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1422   score: 1.0   memory length: 271206   epsilon: 0.6610101400073591    steps: 151    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1423   score: 1.0   memory length: 271357   epsilon: 0.6607111600073656    steps: 151    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1424   score: 3.0   memory length: 271570   epsilon: 0.6602894200073748    steps: 213    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1425   score: 4.0   memory length: 271846   epsilon: 0.6597429400073866    steps: 276    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1426   score: 5.0   memory length: 272152   epsilon: 0.6591370600073998    steps: 306    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1427   score: 3.0   memory length: 272382   epsilon: 0.6586816600074097    steps: 230    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1428   score: 4.0   memory length: 272683   epsilon: 0.6580856800074226    steps: 301    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1429   score: 6.0   memory length: 273035   epsilon: 0.6573887200074378    steps: 352    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1430   score: 1.0   memory length: 273186   epsilon: 0.6570897400074442    steps: 151    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1431   score: 2.0   memory length: 273386   epsilon: 0.6566937400074528    steps: 200    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1432   score: 0.0   memory length: 273509   epsilon: 0.6564502000074581    steps: 123    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1433   score: 2.0   memory length: 273707   epsilon: 0.6560581600074666    steps: 198    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1434   score: 2.0   memory length: 273906   epsilon: 0.6556641400074752    steps: 199    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1435   score: 4.0   memory length: 274182   epsilon: 0.6551176600074871    steps: 276    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1436   score: 2.0   memory length: 274366   epsilon: 0.654753340007495    steps: 184    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1437   score: 4.0   memory length: 274642   epsilon: 0.6542068600075068    steps: 276    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1438   score: 5.0   memory length: 274966   epsilon: 0.6535653400075208    steps: 324    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1439   score: 3.0   memory length: 275192   epsilon: 0.6531178600075305    steps: 226    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1440   score: 4.0   memory length: 275490   epsilon: 0.6525278200075433    steps: 298    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1441   score: 4.0   memory length: 275786   epsilon: 0.651941740007556    steps: 296    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1442   score: 2.0   memory length: 275966   epsilon: 0.6515853400075637    steps: 180    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1443   score: 3.0   memory length: 276192   epsilon: 0.6511378600075735    steps: 226    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1444   score: 4.0   memory length: 276469   epsilon: 0.6505894000075854    steps: 277    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1445   score: 4.0   memory length: 276723   epsilon: 0.6500864800075963    steps: 254    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1446   score: 1.0   memory length: 276873   epsilon: 0.6497894800076027    steps: 150    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1447   score: 3.0   memory length: 277100   epsilon: 0.6493400200076125    steps: 227    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1448   score: 1.0   memory length: 277251   epsilon: 0.649041040007619    steps: 151    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1449   score: 1.0   memory length: 277422   epsilon: 0.6487024600076263    steps: 171    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1450   score: 1.0   memory length: 277593   epsilon: 0.6483638800076337    steps: 171    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1451   score: 2.0   memory length: 277777   epsilon: 0.6479995600076416    steps: 184    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1452   score: 1.0   memory length: 277927   epsilon: 0.647702560007648    steps: 150    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1453   score: 3.0   memory length: 278153   epsilon: 0.6472550800076577    steps: 226    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1454   score: 4.0   memory length: 278414   epsilon: 0.646738300007669    steps: 261    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1455   score: 3.0   memory length: 278648   epsilon: 0.646274980007679    steps: 234    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1456   score: 3.0   memory length: 278894   epsilon: 0.6457879000076896    steps: 246    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1457   score: 3.0   memory length: 279137   epsilon: 0.6453067600077    steps: 243    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1458   score: 4.0   memory length: 279434   epsilon: 0.6447187000077128    steps: 297    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1459   score: 2.0   memory length: 279655   epsilon: 0.6442811200077223    steps: 221    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1460   score: 11.0   memory length: 280066   epsilon: 0.64346734000774    steps: 411    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1461   score: 3.0   memory length: 280294   epsilon: 0.6430159000077498    steps: 228    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1462   score: 3.0   memory length: 280506   epsilon: 0.6425961400077589    steps: 212    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1463   score: 4.0   memory length: 280748   epsilon: 0.6421169800077693    steps: 242    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1464   score: 3.0   memory length: 280979   epsilon: 0.6416596000077792    steps: 231    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1465   score: 3.0   memory length: 281244   epsilon: 0.6411349000077906    steps: 265    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1466   score: 4.0   memory length: 281545   epsilon: 0.6405389200078035    steps: 301    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1467   score: 3.0   memory length: 281758   epsilon: 0.6401171800078127    steps: 213    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1468   score: 3.0   memory length: 282004   epsilon: 0.6396301000078233    steps: 246    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1469   score: 3.0   memory length: 282215   epsilon: 0.6392123200078323    steps: 211    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1470   score: 4.0   memory length: 282509   epsilon: 0.638630200007845    steps: 294    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1471   score: 5.0   memory length: 282819   epsilon: 0.6380164000078583    steps: 310    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1472   score: 4.0   memory length: 283112   epsilon: 0.6374362600078709    steps: 293    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1473   score: 1.0   memory length: 283262   epsilon: 0.6371392600078774    steps: 150    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1474   score: 0.0   memory length: 283385   epsilon: 0.6368957200078826    steps: 123    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1475   score: 3.0   memory length: 283596   epsilon: 0.6364779400078917    steps: 211    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1476   score: 6.0   memory length: 283995   epsilon: 0.6356879200079089    steps: 399    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1477   score: 2.0   memory length: 284193   epsilon: 0.6352958800079174    steps: 198    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1478   score: 1.0   memory length: 284365   epsilon: 0.6349553200079248    steps: 172    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1479   score: 5.0   memory length: 284676   epsilon: 0.6343395400079381    steps: 311    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1480   score: 2.0   memory length: 284855   epsilon: 0.6339851200079458    steps: 179    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1481   score: 6.0   memory length: 285209   epsilon: 0.633284200007961    steps: 354    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1482   score: 3.0   memory length: 285457   epsilon: 0.6327931600079717    steps: 248    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1483   score: 2.0   memory length: 285655   epsilon: 0.6324011200079802    steps: 198    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1484   score: 4.0   memory length: 285897   epsilon: 0.6319219600079906    steps: 242    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1485   score: 5.0   memory length: 286178   epsilon: 0.6313655800080027    steps: 281    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1486   score: 3.0   memory length: 286403   epsilon: 0.6309200800080124    steps: 225    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1487   score: 3.0   memory length: 286650   epsilon: 0.630431020008023    steps: 247    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1488   score: 5.0   memory length: 286955   epsilon: 0.6298271200080361    steps: 305    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1489   score: 3.0   memory length: 287205   epsilon: 0.6293321200080468    steps: 250    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1490   score: 6.0   memory length: 287532   epsilon: 0.6286846600080609    steps: 327    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1491   score: 4.0   memory length: 287787   epsilon: 0.6281797600080719    steps: 255    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1492   score: 2.0   memory length: 287969   epsilon: 0.6278194000080797    steps: 182    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1493   score: 3.0   memory length: 288234   epsilon: 0.6272947000080911    steps: 265    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1494   score: 5.0   memory length: 288582   epsilon: 0.626605660008106    steps: 348    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1495   score: 3.0   memory length: 288807   epsilon: 0.6261601600081157    steps: 225    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1496   score: 1.0   memory length: 288978   epsilon: 0.625821580008123    steps: 171    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1497   score: 7.0   memory length: 289378   epsilon: 0.6250295800081402    steps: 400    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1498   score: 3.0   memory length: 289624   epsilon: 0.6245425000081508    steps: 246    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1499   score: 4.0   memory length: 289902   epsilon: 0.6239920600081628    steps: 278    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1500   score: 3.0   memory length: 290113   epsilon: 0.6235742800081718    steps: 211    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1501   score: 4.0   memory length: 290371   epsilon: 0.6230634400081829    steps: 258    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1502   score: 3.0   memory length: 290584   epsilon: 0.6226417000081921    steps: 213    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1503   score: 12.0   memory length: 290884   epsilon: 0.622047700008205    steps: 300    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1504   score: 3.0   memory length: 291111   epsilon: 0.6215982400082147    steps: 227    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1505   score: 3.0   memory length: 291358   epsilon: 0.6211091800082253    steps: 247    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1506   score: 7.0   memory length: 291760   epsilon: 0.6203132200082426    steps: 402    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1507   score: 3.0   memory length: 291986   epsilon: 0.6198657400082523    steps: 226    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1508   score: 2.0   memory length: 292165   epsilon: 0.61951132000826    steps: 179    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1509   score: 4.0   memory length: 292426   epsilon: 0.6189945400082713    steps: 261    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1510   score: 2.0   memory length: 292608   epsilon: 0.6186341800082791    steps: 182    lr: 4e-05     evaluation reward: 3.31\n",
      "episode: 1511   score: 5.0   memory length: 292909   epsilon: 0.618038200008292    steps: 301    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1512   score: 5.0   memory length: 293230   epsilon: 0.6174026200083058    steps: 321    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1513   score: 3.0   memory length: 293458   epsilon: 0.6169511800083156    steps: 228    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1514   score: 6.0   memory length: 293795   epsilon: 0.6162839200083301    steps: 337    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1515   score: 4.0   memory length: 294090   epsilon: 0.6156998200083428    steps: 295    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1516   score: 5.0   memory length: 294418   epsilon: 0.6150503800083569    steps: 328    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1517   score: 3.0   memory length: 294667   epsilon: 0.6145573600083676    steps: 249    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1518   score: 2.0   memory length: 294867   epsilon: 0.6141613600083762    steps: 200    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1519   score: 2.0   memory length: 295049   epsilon: 0.613801000008384    steps: 182    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1520   score: 10.0   memory length: 295412   epsilon: 0.6130822600083996    steps: 363    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1521   score: 2.0   memory length: 295610   epsilon: 0.6126902200084081    steps: 198    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1522   score: 1.0   memory length: 295761   epsilon: 0.6123912400084146    steps: 151    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1523   score: 4.0   memory length: 296005   epsilon: 0.6119081200084251    steps: 244    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1524   score: 4.0   memory length: 296262   epsilon: 0.6113992600084361    steps: 257    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1525   score: 2.0   memory length: 296444   epsilon: 0.611038900008444    steps: 182    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1526   score: 5.0   memory length: 296755   epsilon: 0.6104231200084573    steps: 311    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1527   score: 4.0   memory length: 297051   epsilon: 0.60983704000847    steps: 296    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1528   score: 2.0   memory length: 297249   epsilon: 0.6094450000084786    steps: 198    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1529   score: 4.0   memory length: 297526   epsilon: 0.6088965400084905    steps: 277    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1530   score: 5.0   memory length: 297809   epsilon: 0.6083362000085026    steps: 283    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1531   score: 3.0   memory length: 298056   epsilon: 0.6078471400085133    steps: 247    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1532   score: 5.0   memory length: 298373   epsilon: 0.6072194800085269    steps: 317    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1533   score: 4.0   memory length: 298611   epsilon: 0.6067482400085371    steps: 238    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1534   score: 2.0   memory length: 298830   epsilon: 0.6063146200085465    steps: 219    lr: 4e-05     evaluation reward: 3.54\n",
      "episode: 1535   score: 6.0   memory length: 299169   epsilon: 0.6056434000085611    steps: 339    lr: 4e-05     evaluation reward: 3.56\n",
      "episode: 1536   score: 7.0   memory length: 299523   epsilon: 0.6049424800085763    steps: 354    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1537   score: 4.0   memory length: 299816   epsilon: 0.6043623400085889    steps: 293    lr: 4e-05     evaluation reward: 3.61\n",
      "episode: 1538   score: 3.0   memory length: 300042   epsilon: 0.6039148600085986    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1539   score: 1.0   memory length: 300193   epsilon: 0.6036158800086051    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1540   score: 4.0   memory length: 300449   epsilon: 0.6031090000086161    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1541   score: 3.0   memory length: 300662   epsilon: 0.6026872600086253    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1542   score: 4.0   memory length: 300935   epsilon: 0.602146720008637    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1543   score: 5.0   memory length: 301244   epsilon: 0.6015349000086503    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1544   score: 3.0   memory length: 301474   epsilon: 0.6010795000086602    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1545   score: 2.0   memory length: 301672   epsilon: 0.6006874600086687    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1546   score: 5.0   memory length: 301989   epsilon: 0.6000598000086823    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1547   score: 1.0   memory length: 302140   epsilon: 0.5997608200086888    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1548   score: 3.0   memory length: 302350   epsilon: 0.5993450200086978    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1549   score: 5.0   memory length: 302630   epsilon: 0.5987906200087099    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1550   score: 7.0   memory length: 302989   epsilon: 0.5980798000087253    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1551   score: 6.0   memory length: 303350   epsilon: 0.5973650200087408    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1552   score: 5.0   memory length: 303658   epsilon: 0.596755180008754    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1553   score: 3.0   memory length: 303869   epsilon: 0.5963374000087631    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1554   score: 6.0   memory length: 304212   epsilon: 0.5956582600087779    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1555   score: 3.0   memory length: 304479   epsilon: 0.5951296000087893    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1556   score: 4.0   memory length: 304753   epsilon: 0.5945870800088011    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1557   score: 4.0   memory length: 305013   epsilon: 0.5940722800088123    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1558   score: 5.0   memory length: 305317   epsilon: 0.5934703600088254    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1559   score: 3.0   memory length: 305530   epsilon: 0.5930486200088345    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1560   score: 8.0   memory length: 305991   epsilon: 0.5921358400088543    steps: 461    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1561   score: 5.0   memory length: 306335   epsilon: 0.5914547200088691    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1562   score: 1.0   memory length: 306485   epsilon: 0.5911577200088756    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1563   score: 2.0   memory length: 306664   epsilon: 0.5908033000088833    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1564   score: 5.0   memory length: 306970   epsilon: 0.5901974200088964    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1565   score: 2.0   memory length: 307170   epsilon: 0.589801420008905    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1566   score: 1.0   memory length: 307321   epsilon: 0.5895024400089115    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1567   score: 2.0   memory length: 307503   epsilon: 0.5891420800089193    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1568   score: 2.0   memory length: 307703   epsilon: 0.5887460800089279    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.76\n",
      "episode: 1569   score: 3.0   memory length: 307932   epsilon: 0.5882926600089378    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.76\n",
      "episode: 1570   score: 5.0   memory length: 308218   epsilon: 0.5877263800089501    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1571   score: 5.0   memory length: 308545   epsilon: 0.5870789200089641    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1572   score: 5.0   memory length: 308854   epsilon: 0.5864671000089774    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1573   score: 5.0   memory length: 309162   epsilon: 0.5858572600089906    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1574   score: 5.0   memory length: 309463   epsilon: 0.5852612800090036    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1575   score: 3.0   memory length: 309698   epsilon: 0.5847959800090137    steps: 235    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1576   score: 3.0   memory length: 309926   epsilon: 0.5843445400090235    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1577   score: 3.0   memory length: 310155   epsilon: 0.5838911200090333    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1578   score: 5.0   memory length: 310437   epsilon: 0.5833327600090454    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1579   score: 4.0   memory length: 310712   epsilon: 0.5827882600090573    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1580   score: 3.0   memory length: 310940   epsilon: 0.5823368200090671    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1581   score: 5.0   memory length: 311248   epsilon: 0.5817269800090803    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1582   score: 4.0   memory length: 311525   epsilon: 0.5811785200090922    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1583   score: 4.0   memory length: 311785   epsilon: 0.5806637200091034    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1584   score: 4.0   memory length: 312044   epsilon: 0.5801509000091145    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1585   score: 3.0   memory length: 312269   epsilon: 0.5797054000091242    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1586   score: 4.0   memory length: 312528   epsilon: 0.5791925800091353    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1587   score: 5.0   memory length: 312857   epsilon: 0.5785411600091495    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1588   score: 4.0   memory length: 313132   epsilon: 0.5779966600091613    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1589   score: 0.0   memory length: 313255   epsilon: 0.5777531200091666    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1590   score: 1.0   memory length: 313405   epsilon: 0.577456120009173    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1591   score: 6.0   memory length: 313746   epsilon: 0.5767809400091877    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1592   score: 3.0   memory length: 313972   epsilon: 0.5763334600091974    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1593   score: 3.0   memory length: 314201   epsilon: 0.5758800400092072    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1594   score: 7.0   memory length: 314571   epsilon: 0.5751474400092231    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1595   score: 3.0   memory length: 314798   epsilon: 0.5746979800092329    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1596   score: 4.0   memory length: 315057   epsilon: 0.574185160009244    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1597   score: 3.0   memory length: 315287   epsilon: 0.5737297600092539    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1598   score: 4.0   memory length: 315565   epsilon: 0.5731793200092659    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1599   score: 4.0   memory length: 315826   epsilon: 0.5726625400092771    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1600   score: 3.0   memory length: 316036   epsilon: 0.5722467400092861    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1601   score: 4.0   memory length: 316298   epsilon: 0.5717279800092974    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1602   score: 2.0   memory length: 316498   epsilon: 0.571331980009306    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1603   score: 6.0   memory length: 316858   epsilon: 0.5706191800093214    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1604   score: 4.0   memory length: 317115   epsilon: 0.5701103200093325    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1605   score: 8.0   memory length: 317408   epsilon: 0.5695301800093451    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1606   score: 4.0   memory length: 317706   epsilon: 0.5689401400093579    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1607   score: 1.0   memory length: 317857   epsilon: 0.5686411600093644    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1608   score: 4.0   memory length: 318111   epsilon: 0.5681382400093753    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1609   score: 2.0   memory length: 318293   epsilon: 0.5677778800093831    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1610   score: 2.0   memory length: 318473   epsilon: 0.5674214800093909    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1611   score: 5.0   memory length: 318762   epsilon: 0.5668492600094033    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1612   score: 8.0   memory length: 319217   epsilon: 0.5659483600094228    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1613   score: 6.0   memory length: 319551   epsilon: 0.5652870400094372    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1614   score: 4.0   memory length: 319809   epsilon: 0.5647762000094483    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1615   score: 1.0   memory length: 319960   epsilon: 0.5644772200094548    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1616   score: 5.0   memory length: 320252   epsilon: 0.5638990600094673    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1617   score: 5.0   memory length: 320558   epsilon: 0.5632931800094805    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1618   score: 6.0   memory length: 320918   epsilon: 0.562580380009496    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1619   score: 3.0   memory length: 321127   epsilon: 0.5621665600095049    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1620   score: 1.0   memory length: 321298   epsilon: 0.5618279800095123    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1621   score: 5.0   memory length: 321585   epsilon: 0.5612597200095246    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1622   score: 4.0   memory length: 321838   epsilon: 0.5607587800095355    steps: 253    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1623   score: 1.0   memory length: 321989   epsilon: 0.560459800009542    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1624   score: 1.0   memory length: 322139   epsilon: 0.5601628000095484    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1625   score: 5.0   memory length: 322429   epsilon: 0.5595886000095609    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1626   score: 2.0   memory length: 322626   epsilon: 0.5591985400095694    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1627   score: 2.0   memory length: 322824   epsilon: 0.5588065000095779    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1628   score: 5.0   memory length: 323096   epsilon: 0.5582679400095896    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1629   score: 7.0   memory length: 323542   epsilon: 0.5573848600096087    steps: 446    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1630   score: 3.0   memory length: 323792   epsilon: 0.5568898600096195    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1631   score: 4.0   memory length: 324050   epsilon: 0.5563790200096306    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1632   score: 6.0   memory length: 324386   epsilon: 0.555713740009645    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1633   score: 5.0   memory length: 324714   epsilon: 0.5550643000096591    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1634   score: 3.0   memory length: 324961   epsilon: 0.5545752400096697    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1635   score: 4.0   memory length: 325236   epsilon: 0.5540307400096816    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1636   score: 5.0   memory length: 325543   epsilon: 0.5534228800096948    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1637   score: 1.0   memory length: 325694   epsilon: 0.5531239000097012    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1638   score: 7.0   memory length: 326062   epsilon: 0.5523952600097171    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1639   score: 3.0   memory length: 326272   epsilon: 0.5519794600097261    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1640   score: 6.0   memory length: 326645   epsilon: 0.5512409200097421    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1641   score: 4.0   memory length: 326920   epsilon: 0.5506964200097539    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1642   score: 4.0   memory length: 327196   epsilon: 0.5501499400097658    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1643   score: 3.0   memory length: 327442   epsilon: 0.5496628600097764    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1644   score: 2.0   memory length: 327661   epsilon: 0.5492292400097858    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1645   score: 6.0   memory length: 328017   epsilon: 0.5485243600098011    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1646   score: 3.0   memory length: 328248   epsilon: 0.548066980009811    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1647   score: 2.0   memory length: 328429   epsilon: 0.5477086000098188    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1648   score: 3.0   memory length: 328659   epsilon: 0.5472532000098287    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1649   score: 4.0   memory length: 328910   epsilon: 0.5467562200098395    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1650   score: 4.0   memory length: 329226   epsilon: 0.5461305400098531    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1651   score: 4.0   memory length: 329506   epsilon: 0.5455761400098651    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1652   score: 3.0   memory length: 329717   epsilon: 0.5451583600098742    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1653   score: 4.0   memory length: 330012   epsilon: 0.5445742600098868    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1654   score: 4.0   memory length: 330269   epsilon: 0.5440654000098979    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1655   score: 7.0   memory length: 330667   epsilon: 0.543277360009915    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1656   score: 3.0   memory length: 330879   epsilon: 0.5428576000099241    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1657   score: 5.0   memory length: 331151   epsilon: 0.5423190400099358    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1658   score: 5.0   memory length: 331457   epsilon: 0.541713160009949    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1659   score: 3.0   memory length: 331686   epsilon: 0.5412597400099588    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1660   score: 2.0   memory length: 331868   epsilon: 0.5408993800099666    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1661   score: 7.0   memory length: 332292   epsilon: 0.5400598600099848    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1662   score: 2.0   memory length: 332490   epsilon: 0.5396678200099934    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1663   score: 5.0   memory length: 332773   epsilon: 0.5391074800100055    steps: 283    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1664   score: 2.0   memory length: 332993   epsilon: 0.538671880010015    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1665   score: 5.0   memory length: 333316   epsilon: 0.5380323400100289    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1666   score: 4.0   memory length: 333572   epsilon: 0.5375254600100399    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1667   score: 4.0   memory length: 333827   epsilon: 0.5370205600100508    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1668   score: 4.0   memory length: 334069   epsilon: 0.5365414000100612    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1669   score: 3.0   memory length: 334279   epsilon: 0.5361256000100703    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1670   score: 6.0   memory length: 334616   epsilon: 0.5354583400100847    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1671   score: 1.0   memory length: 334767   epsilon: 0.5351593600100912    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1672   score: 1.0   memory length: 334917   epsilon: 0.5348623600100977    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1673   score: 3.0   memory length: 335130   epsilon: 0.5344406200101068    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1674   score: 7.0   memory length: 335518   epsilon: 0.5336723800101235    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1675   score: 3.0   memory length: 335729   epsilon: 0.5332546000101326    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1676   score: 3.0   memory length: 335959   epsilon: 0.5327992000101425    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1677   score: 3.0   memory length: 336172   epsilon: 0.5323774600101516    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1678   score: 4.0   memory length: 336469   epsilon: 0.5317894000101644    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1679   score: 10.0   memory length: 337018   epsilon: 0.530702380010188    steps: 549    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1680   score: 6.0   memory length: 337377   epsilon: 0.5299915600102034    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1681   score: 3.0   memory length: 337603   epsilon: 0.5295440800102131    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1682   score: 6.0   memory length: 337917   epsilon: 0.5289223600102266    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1683   score: 4.0   memory length: 338193   epsilon: 0.5283758800102385    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1684   score: 3.0   memory length: 338440   epsilon: 0.5278868200102491    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1685   score: 4.0   memory length: 338734   epsilon: 0.5273047000102618    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1686   score: 4.0   memory length: 339010   epsilon: 0.5267582200102736    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1687   score: 1.0   memory length: 339161   epsilon: 0.5264592400102801    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1688   score: 4.0   memory length: 339421   epsilon: 0.5259444400102913    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1689   score: 11.0   memory length: 339959   epsilon: 0.5248792000103144    steps: 538    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1690   score: 4.0   memory length: 340233   epsilon: 0.5243366800103262    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1691   score: 3.0   memory length: 340462   epsilon: 0.523883260010336    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1692   score: 2.0   memory length: 340663   epsilon: 0.5234852800103447    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1693   score: 4.0   memory length: 340979   epsilon: 0.5228596000103582    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1694   score: 2.0   memory length: 341161   epsilon: 0.5224992400103661    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1695   score: 9.0   memory length: 341641   epsilon: 0.5215488400103867    steps: 480    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1696   score: 5.0   memory length: 341966   epsilon: 0.5209053400104007    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1697   score: 3.0   memory length: 342197   epsilon: 0.5204479600104106    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1698   score: 4.0   memory length: 342454   epsilon: 0.5199391000104217    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1699   score: 4.0   memory length: 342751   epsilon: 0.5193510400104344    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1700   score: 4.0   memory length: 343030   epsilon: 0.5187986200104464    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1701   score: 5.0   memory length: 343339   epsilon: 0.5181868000104597    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1702   score: 4.0   memory length: 343638   epsilon: 0.5175947800104725    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1703   score: 7.0   memory length: 343993   epsilon: 0.5168918800104878    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1704   score: 5.0   memory length: 344301   epsilon: 0.516282040010501    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1705   score: 5.0   memory length: 344609   epsilon: 0.5156722000105143    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1706   score: 3.0   memory length: 344835   epsilon: 0.515224720010524    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1707   score: 2.0   memory length: 345033   epsilon: 0.5148326800105325    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1708   score: 8.0   memory length: 345488   epsilon: 0.5139317800105521    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1709   score: 1.0   memory length: 345639   epsilon: 0.5136328000105586    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1710   score: 5.0   memory length: 345931   epsilon: 0.5130546400105711    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1711   score: 5.0   memory length: 346238   epsilon: 0.5124467800105843    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1712   score: 3.0   memory length: 346463   epsilon: 0.512001280010594    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1713   score: 5.0   memory length: 346789   epsilon: 0.511355800010608    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1714   score: 3.0   memory length: 347017   epsilon: 0.5109043600106178    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1715   score: 5.0   memory length: 347344   epsilon: 0.5102569000106318    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1716   score: 3.0   memory length: 347560   epsilon: 0.5098292200106411    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1717   score: 5.0   memory length: 347887   epsilon: 0.5091817600106552    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1718   score: 3.0   memory length: 348096   epsilon: 0.5087679400106642    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1719   score: 1.0   memory length: 348247   epsilon: 0.5084689600106707    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1720   score: 4.0   memory length: 348504   epsilon: 0.5079601000106817    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1721   score: 7.0   memory length: 348872   epsilon: 0.5072314600106975    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1722   score: 3.0   memory length: 349085   epsilon: 0.5068097200107067    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1723   score: 3.0   memory length: 349294   epsilon: 0.5063959000107157    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1724   score: 4.0   memory length: 349534   epsilon: 0.505920700010726    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1725   score: 6.0   memory length: 349890   epsilon: 0.5052158200107413    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1726   score: 6.0   memory length: 350249   epsilon: 0.5045050000107567    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1727   score: 4.0   memory length: 350528   epsilon: 0.5039525800107687    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1728   score: 3.0   memory length: 350741   epsilon: 0.5035308400107779    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1729   score: 9.0   memory length: 351163   epsilon: 0.502695280010796    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1730   score: 8.0   memory length: 351601   epsilon: 0.5018280400108148    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1731   score: 5.0   memory length: 351922   epsilon: 0.5011924600108286    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1732   score: 5.0   memory length: 352209   epsilon: 0.500624200010841    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1733   score: 5.0   memory length: 352543   epsilon: 0.49996288001085426    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1734   score: 3.0   memory length: 352771   epsilon: 0.4995114400108514    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1735   score: 6.0   memory length: 353117   epsilon: 0.49882636001084707    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1736   score: 5.0   memory length: 353441   epsilon: 0.498184840010843    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1737   score: 7.0   memory length: 353825   epsilon: 0.4974245200108382    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1738   score: 5.0   memory length: 354176   epsilon: 0.4967295400108338    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1739   score: 5.0   memory length: 354521   epsilon: 0.4960464400108295    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1740   score: 4.0   memory length: 354820   epsilon: 0.49545442001082574    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1741   score: 6.0   memory length: 355166   epsilon: 0.4947693400108214    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1742   score: 7.0   memory length: 355553   epsilon: 0.49400308001081655    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1743   score: 2.0   memory length: 355755   epsilon: 0.493603120010814    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1744   score: 3.0   memory length: 356022   epsilon: 0.4930744600108107    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1745   score: 12.0   memory length: 356548   epsilon: 0.4920329800108041    steps: 526    lr: 1.6000000000000003e-05     evaluation reward: 4.38\n",
      "episode: 1746   score: 5.0   memory length: 356858   epsilon: 0.4914191800108002    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 4.4\n",
      "episode: 1747   score: 5.0   memory length: 357166   epsilon: 0.49080934001079635    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1748   score: 4.0   memory length: 357459   epsilon: 0.4902292000107927    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1749   score: 3.0   memory length: 357687   epsilon: 0.4897777600107898    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.43\n",
      "episode: 1750   score: 5.0   memory length: 357977   epsilon: 0.4892035600107862    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 4.44\n",
      "episode: 1751   score: 5.0   memory length: 358322   epsilon: 0.48852046001078187    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1752   score: 3.0   memory length: 358549   epsilon: 0.488071000010779    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1753   score: 5.0   memory length: 358877   epsilon: 0.4874215600107749    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1754   score: 4.0   memory length: 359135   epsilon: 0.4869107200107717    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.46\n",
      "episode: 1755   score: 6.0   memory length: 359448   epsilon: 0.48629098001076776    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1756   score: 7.0   memory length: 359854   epsilon: 0.4854871000107627    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1757   score: 3.0   memory length: 360065   epsilon: 0.48506932001076003    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1758   score: 6.0   memory length: 360440   epsilon: 0.48432682001075533    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1759   score: 2.0   memory length: 360638   epsilon: 0.48393478001075285    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1760   score: 5.0   memory length: 360961   epsilon: 0.4832952400107488    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1761   score: 2.0   memory length: 361177   epsilon: 0.4828675600107461    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 4.45\n",
      "episode: 1762   score: 5.0   memory length: 361485   epsilon: 0.48225772001074224    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1763   score: 4.0   memory length: 361744   epsilon: 0.481744900010739    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.47\n",
      "episode: 1764   score: 5.0   memory length: 362049   epsilon: 0.4811410000107352    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1765   score: 7.0   memory length: 362437   epsilon: 0.4803727600107303    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1766   score: 9.0   memory length: 362898   epsilon: 0.47945998001072454    steps: 461    lr: 1.6000000000000003e-05     evaluation reward: 4.57\n",
      "episode: 1767   score: 7.0   memory length: 363319   epsilon: 0.47862640001071927    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1768   score: 2.0   memory length: 363517   epsilon: 0.4782343600107168    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1769   score: 5.0   memory length: 363858   epsilon: 0.4775591800107125    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1770   score: 4.0   memory length: 364100   epsilon: 0.4770800200107095    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1771   score: 8.0   memory length: 364520   epsilon: 0.4762484200107042    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1772   score: 3.0   memory length: 364749   epsilon: 0.47579500001070135    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1773   score: 2.0   memory length: 364947   epsilon: 0.47540296001069887    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1774   score: 4.0   memory length: 365206   epsilon: 0.4748901400106956    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1775   score: 5.0   memory length: 365531   epsilon: 0.47424664001069156    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1776   score: 4.0   memory length: 365793   epsilon: 0.4737278800106883    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1777   score: 4.0   memory length: 366069   epsilon: 0.4731814000106848    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1778   score: 4.0   memory length: 366367   epsilon: 0.4725913600106811    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1779   score: 3.0   memory length: 366596   epsilon: 0.4721379400106782    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.6\n",
      "episode: 1780   score: 9.0   memory length: 367001   epsilon: 0.47133604001067314    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1781   score: 5.0   memory length: 367309   epsilon: 0.4707262000106693    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1782   score: 3.0   memory length: 367522   epsilon: 0.4703044600106666    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1783   score: 4.0   memory length: 367762   epsilon: 0.4698292600106636    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1784   score: 5.0   memory length: 368107   epsilon: 0.4691461600106593    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1785   score: 5.0   memory length: 368396   epsilon: 0.46857394001065567    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1786   score: 2.0   memory length: 368596   epsilon: 0.46817794001065316    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1787   score: 6.0   memory length: 368953   epsilon: 0.4674710800106487    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1788   score: 6.0   memory length: 369309   epsilon: 0.4667662000106442    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1789   score: 3.0   memory length: 369536   epsilon: 0.4663167400106414    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1790   score: 3.0   memory length: 369749   epsilon: 0.4658950000106387    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1791   score: 4.0   memory length: 370025   epsilon: 0.46534852001063526    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1792   score: 4.0   memory length: 370279   epsilon: 0.4648456000106321    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1793   score: 3.0   memory length: 370489   epsilon: 0.46442980001062945    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1794   score: 7.0   memory length: 370900   epsilon: 0.4636160200106243    steps: 411    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1795   score: 4.0   memory length: 371177   epsilon: 0.4630675600106208    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1796   score: 5.0   memory length: 371483   epsilon: 0.462461680010617    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1797   score: 2.0   memory length: 371681   epsilon: 0.4620696400106145    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1798   score: 5.0   memory length: 371987   epsilon: 0.4614637600106107    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1799   score: 6.0   memory length: 372340   epsilon: 0.46076482001060626    steps: 353    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1800   score: 5.0   memory length: 372646   epsilon: 0.4601589400106024    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1801   score: 4.0   memory length: 372943   epsilon: 0.4595708800105987    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1802   score: 6.0   memory length: 373299   epsilon: 0.45886600001059424    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1803   score: 4.0   memory length: 373560   epsilon: 0.458349220010591    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1804   score: 9.0   memory length: 374018   epsilon: 0.45744238001058524    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1805   score: 2.0   memory length: 374198   epsilon: 0.457085980010583    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1806   score: 1.0   memory length: 374349   epsilon: 0.4567870000105811    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1807   score: 6.0   memory length: 374724   epsilon: 0.4560445000105764    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1808   score: 4.0   memory length: 374984   epsilon: 0.45552970001057314    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1809   score: 5.0   memory length: 375288   epsilon: 0.4549277800105693    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1810   score: 3.0   memory length: 375535   epsilon: 0.45443872001056623    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1811   score: 6.0   memory length: 375880   epsilon: 0.4537556200105619    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1812   score: 7.0   memory length: 376284   epsilon: 0.45295570001055685    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1813   score: 5.0   memory length: 376596   epsilon: 0.45233794001055294    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1814   score: 3.0   memory length: 376861   epsilon: 0.4518132400105496    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1815   score: 3.0   memory length: 377091   epsilon: 0.45135784001054674    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1816   score: 4.0   memory length: 377370   epsilon: 0.45080542001054325    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1817   score: 1.0   memory length: 377521   epsilon: 0.45050644001054135    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1818   score: 5.0   memory length: 377864   epsilon: 0.44982730001053706    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1819   score: 5.0   memory length: 378175   epsilon: 0.44921152001053316    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1820   score: 7.0   memory length: 378511   epsilon: 0.44854624001052895    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1821   score: 3.0   memory length: 378738   epsilon: 0.4480967800105261    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1822   score: 1.0   memory length: 378889   epsilon: 0.4477978000105242    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1823   score: 4.0   memory length: 379185   epsilon: 0.4472117200105205    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1824   score: 7.0   memory length: 379554   epsilon: 0.4464811000105159    steps: 369    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1825   score: 5.0   memory length: 379884   epsilon: 0.44582770001051175    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1826   score: 4.0   memory length: 380179   epsilon: 0.44524360001050806    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1827   score: 12.0   memory length: 380672   epsilon: 0.4442674600105019    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 4.77\n",
      "episode: 1828   score: 5.0   memory length: 380976   epsilon: 0.44366554001049807    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.79\n",
      "episode: 1829   score: 4.0   memory length: 381235   epsilon: 0.4431527200104948    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1830   score: 9.0   memory length: 381703   epsilon: 0.44222608001048896    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1831   score: 4.0   memory length: 381979   epsilon: 0.4416796000104855    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1832   score: 3.0   memory length: 382188   epsilon: 0.4412657800104829    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1833   score: 2.0   memory length: 382386   epsilon: 0.4408737400104804    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1834   score: 3.0   memory length: 382595   epsilon: 0.4404599200104778    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1835   score: 5.0   memory length: 382881   epsilon: 0.4398936400104742    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1836   score: 4.0   memory length: 383157   epsilon: 0.43934716001047075    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1837   score: 6.0   memory length: 383550   epsilon: 0.4385690200104658    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1838   score: 3.0   memory length: 383763   epsilon: 0.43814728001046316    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1839   score: 4.0   memory length: 384022   epsilon: 0.4376344600104599    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1840   score: 4.0   memory length: 384317   epsilon: 0.4370503600104562    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1841   score: 5.0   memory length: 384626   epsilon: 0.43643854001045235    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1842   score: 4.0   memory length: 384885   epsilon: 0.4359257200104491    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.59\n",
      "episode: 1843   score: 6.0   memory length: 385263   epsilon: 0.43517728001044437    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1844   score: 4.0   memory length: 385521   epsilon: 0.43466644001044114    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1845   score: 5.0   memory length: 385825   epsilon: 0.4340645200104373    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.57\n",
      "episode: 1846   score: 3.0   memory length: 386072   epsilon: 0.43357546001043423    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1847   score: 4.0   memory length: 386348   epsilon: 0.4330289800104308    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
      "episode: 1848   score: 4.0   memory length: 386605   epsilon: 0.43252012001042756    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
      "episode: 1849   score: 3.0   memory length: 386831   epsilon: 0.4320726400104247    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.54\n",
      "episode: 1850   score: 3.0   memory length: 387044   epsilon: 0.43165090001042206    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1851   score: 3.0   memory length: 387292   epsilon: 0.43115986001041895    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1852   score: 2.0   memory length: 387473   epsilon: 0.4308014800104167    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1853   score: 4.0   memory length: 387729   epsilon: 0.4302946000104135    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 4.48\n",
      "episode: 1854   score: 6.0   memory length: 388107   epsilon: 0.42954616001040874    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 4.5\n",
      "episode: 1855   score: 8.0   memory length: 388565   epsilon: 0.428639320010403    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1856   score: 4.0   memory length: 388820   epsilon: 0.4281344200103998    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 4.49\n",
      "episode: 1857   score: 6.0   memory length: 389176   epsilon: 0.42742954001039535    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.52\n",
      "episode: 1858   score: 5.0   memory length: 389485   epsilon: 0.4268177200103915    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.51\n",
      "episode: 1859   score: 6.0   memory length: 389826   epsilon: 0.4261425400103872    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 4.55\n",
      "episode: 1860   score: 8.0   memory length: 390259   epsilon: 0.4252852000103818    steps: 433    lr: 1.6000000000000003e-05     evaluation reward: 4.58\n",
      "episode: 1861   score: 7.0   memory length: 390668   epsilon: 0.42447538001037666    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1862   score: 3.0   memory length: 390878   epsilon: 0.424059580010374    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1863   score: 6.0   memory length: 391211   epsilon: 0.42340024001036985    steps: 333    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1864   score: 5.0   memory length: 391521   epsilon: 0.42278644001036597    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1865   score: 11.0   memory length: 392032   epsilon: 0.42177466001035957    steps: 511    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1866   score: 4.0   memory length: 392308   epsilon: 0.4212281800103561    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 4.62\n",
      "episode: 1867   score: 6.0   memory length: 392651   epsilon: 0.4205490400103518    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 4.61\n",
      "episode: 1868   score: 8.0   memory length: 392929   epsilon: 0.41999860001034833    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1869   score: 7.0   memory length: 393309   epsilon: 0.4192462000103436    steps: 380    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1870   score: 3.0   memory length: 393520   epsilon: 0.41882842001034093    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.68\n",
      "episode: 1871   score: 5.0   memory length: 393826   epsilon: 0.4182225400103371    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1872   score: 2.0   memory length: 394026   epsilon: 0.4178265400103346    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.64\n",
      "episode: 1873   score: 3.0   memory length: 394237   epsilon: 0.41740876001033195    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1874   score: 9.0   memory length: 394723   epsilon: 0.41644648001032586    steps: 486    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1875   score: 4.0   memory length: 394983   epsilon: 0.4159316800103226    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.69\n",
      "episode: 1876   score: 7.0   memory length: 395385   epsilon: 0.41513572001031757    steps: 402    lr: 1.6000000000000003e-05     evaluation reward: 4.72\n",
      "episode: 1877   score: 6.0   memory length: 395775   epsilon: 0.4143635200103127    steps: 390    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1878   score: 4.0   memory length: 396034   epsilon: 0.41385070001030944    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1879   score: 2.0   memory length: 396234   epsilon: 0.41345470001030693    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 4.73\n",
      "episode: 1880   score: 3.0   memory length: 396447   epsilon: 0.41303296001030426    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1881   score: 1.0   memory length: 396598   epsilon: 0.41273398001030237    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.63\n",
      "episode: 1882   score: 6.0   memory length: 396971   epsilon: 0.4119954400102977    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 4.66\n",
      "episode: 1883   score: 3.0   memory length: 397201   epsilon: 0.4115400400102948    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.65\n",
      "episode: 1884   score: 7.0   memory length: 397613   epsilon: 0.41072428001028966    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 4.67\n",
      "episode: 1885   score: 8.0   memory length: 398089   epsilon: 0.4097818000102837    steps: 476    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1886   score: 3.0   memory length: 398299   epsilon: 0.40936600001028106    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1887   score: 5.0   memory length: 398606   epsilon: 0.4087581400102772    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1888   score: 6.0   memory length: 398966   epsilon: 0.4080453400102727    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 4.7\n",
      "episode: 1889   score: 4.0   memory length: 399241   epsilon: 0.40750084001026926    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.71\n",
      "episode: 1890   score: 7.0   memory length: 399610   epsilon: 0.40677022001026464    steps: 369    lr: 1.6000000000000003e-05     evaluation reward: 4.75\n",
      "episode: 1891   score: 3.0   memory length: 399822   epsilon: 0.406350460010262    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.74\n",
      "episode: 1892   score: 4.0   memory length: 400064   epsilon: 0.40587130001025895    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 1893   score: 6.0   memory length: 400439   epsilon: 0.40512880001025425    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 1894   score: 6.0   memory length: 400777   epsilon: 0.40445956001025    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 1895   score: 5.0   memory length: 401058   epsilon: 0.4039031800102465    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 1896   score: 7.0   memory length: 401440   epsilon: 0.4031468200102417    steps: 382    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 1897   score: 9.0   memory length: 401903   epsilon: 0.4022300800102359    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 4.86\n",
      "episode: 1898   score: 4.0   memory length: 402162   epsilon: 0.40171726001023267    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.85\n",
      "episode: 1899   score: 2.0   memory length: 402342   epsilon: 0.4013608600102304    steps: 180    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 1900   score: 4.0   memory length: 402618   epsilon: 0.40081438001022696    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.8\n",
      "episode: 1901   score: 3.0   memory length: 402828   epsilon: 0.4003985800102243    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 1902   score: 4.0   memory length: 403088   epsilon: 0.39988378001022107    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 1903   score: 5.0   memory length: 403369   epsilon: 0.39932740001021755    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 1904   score: 5.0   memory length: 403657   epsilon: 0.39875716001021394    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 1905   score: 5.0   memory length: 404002   epsilon: 0.3980740600102096    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 1906   score: 3.0   memory length: 404232   epsilon: 0.39761866001020674    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 1907   score: 8.0   memory length: 404651   epsilon: 0.3967890400102015    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 1908   score: 6.0   memory length: 404994   epsilon: 0.3961099000101972    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 1909   score: 3.0   memory length: 405204   epsilon: 0.39569410001019456    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 1910   score: 5.0   memory length: 405492   epsilon: 0.39512386001019095    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 1911   score: 4.0   memory length: 405731   epsilon: 0.39465064001018796    steps: 239    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 1912   score: 4.0   memory length: 405989   epsilon: 0.3941398000101847    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 1913   score: 4.0   memory length: 406231   epsilon: 0.3936606400101817    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 1914   score: 17.0   memory length: 406825   epsilon: 0.39248452001017425    steps: 594    lr: 6.400000000000001e-06     evaluation reward: 4.91\n",
      "episode: 1915   score: 8.0   memory length: 407260   epsilon: 0.3916232200101688    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 1916   score: 5.0   memory length: 407564   epsilon: 0.391021300010165    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 4.97\n",
      "episode: 1917   score: 4.0   memory length: 407845   epsilon: 0.3904649200101615    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 1918   score: 4.0   memory length: 408104   epsilon: 0.38995210001015823    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 1919   score: 7.0   memory length: 408522   epsilon: 0.389124460010153    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 1920   score: 9.0   memory length: 409027   epsilon: 0.38812456001014667    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 5.03\n",
      "episode: 1921   score: 6.0   memory length: 409380   epsilon: 0.38742562001014225    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 5.06\n",
      "episode: 1922   score: 6.0   memory length: 409715   epsilon: 0.38676232001013805    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 5.11\n",
      "episode: 1923   score: 5.0   memory length: 410019   epsilon: 0.38616040001013424    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 5.12\n",
      "episode: 1924   score: 5.0   memory length: 410312   epsilon: 0.38558026001013057    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 5.1\n",
      "episode: 1925   score: 8.0   memory length: 410733   epsilon: 0.3847466800101253    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 5.13\n",
      "episode: 1926   score: 3.0   memory length: 410980   epsilon: 0.3842576200101222    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 5.12\n",
      "episode: 1927   score: 5.0   memory length: 411308   epsilon: 0.3836081800101181    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 1928   score: 5.0   memory length: 411632   epsilon: 0.38296666001011404    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 1929   score: 8.0   memory length: 412062   epsilon: 0.38211526001010865    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 1930   score: 7.0   memory length: 412419   epsilon: 0.3814084000101042    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 1931   score: 4.0   memory length: 412676   epsilon: 0.38089954001010096    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 1932   score: 6.0   memory length: 412996   epsilon: 0.38026594001009695    steps: 320    lr: 6.400000000000001e-06     evaluation reward: 5.1\n",
      "episode: 1933   score: 3.0   memory length: 413209   epsilon: 0.3798442000100943    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.11\n",
      "episode: 1934   score: 6.0   memory length: 413584   epsilon: 0.3791017000100896    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.14\n",
      "episode: 1935   score: 5.0   memory length: 413875   epsilon: 0.37852552001008594    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 5.14\n",
      "episode: 1936   score: 1.0   memory length: 414026   epsilon: 0.37822654001008404    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 5.11\n",
      "episode: 1937   score: 4.0   memory length: 414283   epsilon: 0.3777176800100808    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 1938   score: 4.0   memory length: 414543   epsilon: 0.37720288001007757    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 5.1\n",
      "episode: 1939   score: 8.0   memory length: 414956   epsilon: 0.3763851400100724    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 5.14\n",
      "episode: 1940   score: 6.0   memory length: 415295   epsilon: 0.37571392001006815    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 5.16\n",
      "episode: 1941   score: 4.0   memory length: 415551   epsilon: 0.37520704001006494    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 1942   score: 5.0   memory length: 415822   epsilon: 0.37467046001006155    steps: 271    lr: 6.400000000000001e-06     evaluation reward: 5.16\n",
      "episode: 1943   score: 4.0   memory length: 416081   epsilon: 0.3741576400100583    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.14\n",
      "episode: 1944   score: 13.0   memory length: 416460   epsilon: 0.37340722001005355    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 1945   score: 9.0   memory length: 416934   epsilon: 0.3724687000100476    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 1946   score: 3.0   memory length: 417160   epsilon: 0.3720212200100448    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 1947   score: 5.0   memory length: 417448   epsilon: 0.3714509800100412    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 1948   score: 5.0   memory length: 417757   epsilon: 0.3708391600100373    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.29\n",
      "episode: 1949   score: 4.0   memory length: 417999   epsilon: 0.3703600000100343    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 1950   score: 5.0   memory length: 418304   epsilon: 0.36975610001003045    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 5.32\n",
      "episode: 1951   score: 7.0   memory length: 418708   epsilon: 0.3689561800100254    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 5.36\n",
      "episode: 1952   score: 6.0   memory length: 419029   epsilon: 0.36832060001002137    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 1953   score: 4.0   memory length: 419288   epsilon: 0.3678077800100181    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 1954   score: 7.0   memory length: 419674   epsilon: 0.3670435000100133    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 5.41\n",
      "episode: 1955   score: 7.0   memory length: 420101   epsilon: 0.36619804001000794    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 1956   score: 7.0   memory length: 420485   epsilon: 0.36543772001000313    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 5.43\n",
      "episode: 1957   score: 9.0   memory length: 420973   epsilon: 0.364471480009997    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1958   score: 5.0   memory length: 421281   epsilon: 0.36386164000999316    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1959   score: 9.0   memory length: 421736   epsilon: 0.36296074000998746    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 1960   score: 7.0   memory length: 422140   epsilon: 0.3621608200099824    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 5.48\n",
      "episode: 1961   score: 5.0   memory length: 422449   epsilon: 0.3615490000099785    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 1962   score: 6.0   memory length: 422789   epsilon: 0.36087580000997427    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 1963   score: 8.0   memory length: 423208   epsilon: 0.360046180009969    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1964   score: 9.0   memory length: 423552   epsilon: 0.3593650600099647    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1965   score: 7.0   memory length: 423957   epsilon: 0.35856316000995964    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 1966   score: 8.0   memory length: 424374   epsilon: 0.3577375000099544    steps: 417    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1967   score: 4.0   memory length: 424618   epsilon: 0.35725438000995136    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 5.53\n",
      "episode: 1968   score: 3.0   memory length: 424848   epsilon: 0.3567989800099485    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 5.48\n",
      "episode: 1969   score: 9.0   memory length: 425293   epsilon: 0.3559178800099429    steps: 445    lr: 6.400000000000001e-06     evaluation reward: 5.5\n",
      "episode: 1970   score: 7.0   memory length: 425648   epsilon: 0.35521498000993845    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1971   score: 5.0   memory length: 425977   epsilon: 0.35456356000993433    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1972   score: 4.0   memory length: 426253   epsilon: 0.3540170800099309    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1973   score: 3.0   memory length: 426481   epsilon: 0.353565640009928    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 1974   score: 5.0   memory length: 426774   epsilon: 0.35298550000992435    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 5.52\n",
      "episode: 1975   score: 7.0   memory length: 427127   epsilon: 0.3522865600099199    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 1976   score: 6.0   memory length: 427478   epsilon: 0.3515915800099155    steps: 351    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1977   score: 6.0   memory length: 427834   epsilon: 0.35088670000991107    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 5.54\n",
      "episode: 1978   score: 13.0   memory length: 428379   epsilon: 0.34980760000990424    steps: 545    lr: 6.400000000000001e-06     evaluation reward: 5.63\n",
      "episode: 1979   score: 3.0   memory length: 428610   epsilon: 0.34935022000990135    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 1980   score: 5.0   memory length: 428918   epsilon: 0.3487403800098975    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
      "episode: 1981   score: 7.0   memory length: 429301   epsilon: 0.3479820400098927    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1982   score: 6.0   memory length: 429674   epsilon: 0.347243500009888    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 1983   score: 8.0   memory length: 430149   epsilon: 0.34630300000988207    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 1984   score: 7.0   memory length: 430522   epsilon: 0.3455644600098774    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 1985   score: 4.0   memory length: 430768   epsilon: 0.3450773800098743    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1986   score: 5.0   memory length: 431043   epsilon: 0.34453288000987087    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 5.75\n",
      "episode: 1987   score: 5.0   memory length: 431332   epsilon: 0.34396066000986725    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 5.75\n",
      "episode: 1988   score: 4.0   memory length: 431594   epsilon: 0.34344190000986397    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 1989   score: 5.0   memory length: 431903   epsilon: 0.3428300800098601    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 1990   score: 9.0   memory length: 432341   epsilon: 0.3419628400098546    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 1991   score: 3.0   memory length: 432552   epsilon: 0.34154506000985196    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 1992   score: 9.0   memory length: 433025   epsilon: 0.34060852000984604    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 5.81\n",
      "episode: 1993   score: 4.0   memory length: 433302   epsilon: 0.34006006000984257    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 1994   score: 5.0   memory length: 433594   epsilon: 0.3394819000098389    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1995   score: 8.0   memory length: 433989   epsilon: 0.33869980000983396    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 5.81\n",
      "episode: 1996   score: 9.0   memory length: 434439   epsilon: 0.3378088000098283    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 5.83\n",
      "episode: 1997   score: 4.0   memory length: 434695   epsilon: 0.3373019200098251    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 1998   score: 8.0   memory length: 435092   epsilon: 0.33651586000982014    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 1999   score: 5.0   memory length: 435423   epsilon: 0.335860480009816    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2000   score: 4.0   memory length: 435700   epsilon: 0.33531202000981253    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2001   score: 5.0   memory length: 435994   epsilon: 0.33472990000980885    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 2002   score: 5.0   memory length: 436300   epsilon: 0.334124020009805    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2003   score: 5.0   memory length: 436624   epsilon: 0.33348250000980095    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2004   score: 6.0   memory length: 436938   epsilon: 0.332860780009797    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 2005   score: 9.0   memory length: 437246   epsilon: 0.33225094000979316    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2006   score: 4.0   memory length: 437505   epsilon: 0.3317381200097899    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2007   score: 7.0   memory length: 437867   epsilon: 0.3310213600097854    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2008   score: 14.0   memory length: 438389   epsilon: 0.32998780000977884    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2009   score: 5.0   memory length: 438696   epsilon: 0.329379940009775    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2010   score: 9.0   memory length: 439172   epsilon: 0.32843746000976903    steps: 476    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2011   score: 6.0   memory length: 439546   epsilon: 0.32769694000976435    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2012   score: 6.0   memory length: 439885   epsilon: 0.3270257200097601    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2013   score: 12.0   memory length: 440452   epsilon: 0.325903060009753    steps: 567    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2014   score: 7.0   memory length: 440855   epsilon: 0.32510512000974795    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2015   score: 5.0   memory length: 441165   epsilon: 0.32449132000974407    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2016   score: 3.0   memory length: 441375   epsilon: 0.32407552000974144    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2017   score: 9.0   memory length: 441738   epsilon: 0.3233567800097369    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2018   score: 4.0   memory length: 441980   epsilon: 0.32287762000973386    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2019   score: 10.0   memory length: 442369   epsilon: 0.322107400009729    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2020   score: 3.0   memory length: 442602   epsilon: 0.32164606000972606    steps: 233    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2021   score: 8.0   memory length: 443017   epsilon: 0.32082436000972087    steps: 415    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2022   score: 8.0   memory length: 443404   epsilon: 0.320058100009716    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 6.1\n",
      "episode: 2023   score: 9.0   memory length: 443846   epsilon: 0.3191829400097105    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2024   score: 7.0   memory length: 444167   epsilon: 0.31854736000970646    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2025   score: 6.0   memory length: 444508   epsilon: 0.3178721800097022    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2026   score: 7.0   memory length: 444914   epsilon: 0.3170683000096971    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2027   score: 4.0   memory length: 445193   epsilon: 0.3165158800096936    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2028   score: 7.0   memory length: 445566   epsilon: 0.31577734000968893    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2029   score: 5.0   memory length: 445853   epsilon: 0.31520908000968534    steps: 287    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2030   score: 5.0   memory length: 446125   epsilon: 0.31467052000968193    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2031   score: 5.0   memory length: 446416   epsilon: 0.3140943400096783    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2032   score: 6.0   memory length: 446771   epsilon: 0.31339144000967384    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2033   score: 6.0   memory length: 447098   epsilon: 0.31274398000966974    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2034   score: 4.0   memory length: 447340   epsilon: 0.3122648200096667    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2035   score: 4.0   memory length: 447601   epsilon: 0.31174804000966344    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2036   score: 5.0   memory length: 447894   epsilon: 0.31116790000965977    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2037   score: 9.0   memory length: 448212   epsilon: 0.3105382600096558    steps: 318    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
      "episode: 2038   score: 5.0   memory length: 448502   epsilon: 0.30996406000965215    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2039   score: 7.0   memory length: 448924   epsilon: 0.30912850000964687    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
      "episode: 2040   score: 9.0   memory length: 449231   epsilon: 0.308520640009643    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.27\n",
      "episode: 2041   score: 12.0   memory length: 449642   epsilon: 0.3077068600096379    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2042   score: 6.0   memory length: 449964   epsilon: 0.30706930000963384    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2043   score: 3.0   memory length: 450177   epsilon: 0.30664756000963117    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2044   score: 9.0   memory length: 450649   epsilon: 0.30571300000962526    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 6.31\n",
      "episode: 2045   score: 6.0   memory length: 451030   epsilon: 0.3049586200096205    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 2046   score: 7.0   memory length: 451415   epsilon: 0.30419632000961566    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.32\n",
      "episode: 2047   score: 6.0   memory length: 451773   epsilon: 0.3034874800096112    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2048   score: 5.0   memory length: 452047   epsilon: 0.30294496000960774    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2049   score: 4.0   memory length: 452324   epsilon: 0.3023965000096043    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2050   score: 9.0   memory length: 452832   epsilon: 0.3013906600095979    steps: 508    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2051   score: 8.0   memory length: 453259   epsilon: 0.30054520000959256    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2052   score: 7.0   memory length: 453654   epsilon: 0.2997631000095876    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2053   score: 10.0   memory length: 454144   epsilon: 0.2987929000095815    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 2054   score: 10.0   memory length: 454612   epsilon: 0.2978662600095756    steps: 468    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 2055   score: 3.0   memory length: 454824   epsilon: 0.29744650000957296    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.44\n",
      "episode: 2056   score: 9.0   memory length: 455296   epsilon: 0.29651194000956704    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
      "episode: 2057   score: 6.0   memory length: 455611   epsilon: 0.2958882400095631    steps: 315    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 2058   score: 5.0   memory length: 455922   epsilon: 0.2952724600095592    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 2059   score: 7.0   memory length: 456307   epsilon: 0.2945101600095544    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2060   score: 7.0   memory length: 456669   epsilon: 0.29379340000954984    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2061   score: 6.0   memory length: 457005   epsilon: 0.29312812000954563    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 2062   score: 3.0   memory length: 457218   epsilon: 0.29270638000954297    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2063   score: 7.0   memory length: 457603   epsilon: 0.29194408000953814    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2064   score: 6.0   memory length: 457960   epsilon: 0.29123722000953367    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2065   score: 6.0   memory length: 458323   epsilon: 0.2905184800095291    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 2066   score: 7.0   memory length: 458713   epsilon: 0.28974628000952424    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2067   score: 4.0   memory length: 458988   epsilon: 0.2892017800095208    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2068   score: 11.0   memory length: 459439   epsilon: 0.28830880000951514    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2069   score: 5.0   memory length: 459708   epsilon: 0.2877761800095118    steps: 269    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2070   score: 6.0   memory length: 460041   epsilon: 0.2871168400095076    steps: 333    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2071   score: 6.0   memory length: 460398   epsilon: 0.28640998000950313    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2072   score: 9.0   memory length: 460920   epsilon: 0.2853764200094966    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 2073   score: 4.0   memory length: 461199   epsilon: 0.2848240000094931    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 2074   score: 3.0   memory length: 461410   epsilon: 0.28440622000949045    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2075   score: 3.0   memory length: 461623   epsilon: 0.2839844800094878    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2076   score: 14.0   memory length: 462139   epsilon: 0.2829628000094813    steps: 516    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 2077   score: 8.0   memory length: 462539   epsilon: 0.2821708000094763    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 2078   score: 6.0   memory length: 462860   epsilon: 0.2815352200094723    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2079   score: 4.0   memory length: 463137   epsilon: 0.2809867600094688    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2080   score: 12.0   memory length: 463600   epsilon: 0.280070020009463    steps: 463    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 2081   score: 5.0   memory length: 463875   epsilon: 0.27952552000945957    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.46\n",
      "episode: 2082   score: 8.0   memory length: 464301   epsilon: 0.27868204000945423    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 6.48\n",
      "episode: 2083   score: 9.0   memory length: 464806   epsilon: 0.2776821400094479    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2084   score: 5.0   memory length: 465097   epsilon: 0.27710596000944426    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 6.47\n",
      "episode: 2085   score: 6.0   memory length: 465436   epsilon: 0.27643474000944    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2086   score: 5.0   memory length: 465761   epsilon: 0.27579124000943595    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2087   score: 5.0   memory length: 466068   epsilon: 0.2751833800094321    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2088   score: 10.0   memory length: 466434   epsilon: 0.2744587000094275    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2089   score: 3.0   memory length: 466647   epsilon: 0.27403696000942485    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
      "episode: 2090   score: 11.0   memory length: 467088   epsilon: 0.2731637800094193    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2091   score: 8.0   memory length: 467516   epsilon: 0.27231634000941396    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2092   score: 4.0   memory length: 467793   epsilon: 0.2717678800094105    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2093   score: 7.0   memory length: 468169   epsilon: 0.2710234000094058    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2094   score: 4.0   memory length: 468444   epsilon: 0.27047890000940233    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2095   score: 4.0   memory length: 468720   epsilon: 0.2699324200093989    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
      "episode: 2096   score: 6.0   memory length: 469039   epsilon: 0.2693008000093949    steps: 319    lr: 6.400000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2097   score: 10.0   memory length: 469560   epsilon: 0.26826922000938835    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 6.56\n",
      "episode: 2098   score: 7.0   memory length: 469964   epsilon: 0.2674693000093833    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2099   score: 4.0   memory length: 470221   epsilon: 0.2669604400093801    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 6.54\n",
      "episode: 2100   score: 7.0   memory length: 470643   epsilon: 0.2661248800093748    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2101   score: 10.0   memory length: 471163   epsilon: 0.2650952800093683    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2102   score: 11.0   memory length: 471688   epsilon: 0.2640557800093617    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2103   score: 3.0   memory length: 471899   epsilon: 0.26363800000935905    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2104   score: 9.0   memory length: 472366   epsilon: 0.2627133400093532    steps: 467    lr: 6.400000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2105   score: 8.0   memory length: 472644   epsilon: 0.2621629000093497    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2106   score: 4.0   memory length: 472922   epsilon: 0.26161246000934624    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2107   score: 3.0   memory length: 473133   epsilon: 0.2611946800093436    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2108   score: 9.0   memory length: 473576   epsilon: 0.26031754000933804    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
      "episode: 2109   score: 8.0   memory length: 473976   epsilon: 0.25952554000933303    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2110   score: 8.0   memory length: 474377   epsilon: 0.258731560009328    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 6.61\n",
      "episode: 2111   score: 5.0   memory length: 474668   epsilon: 0.25815538000932436    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2112   score: 10.0   memory length: 475154   epsilon: 0.2571931000093183    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2113   score: 6.0   memory length: 475467   epsilon: 0.25657336000931436    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2114   score: 5.0   memory length: 475773   epsilon: 0.2559674800093105    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 6.56\n",
      "episode: 2115   score: 4.0   memory length: 476050   epsilon: 0.25541902000930705    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2116   score: 5.0   memory length: 476343   epsilon: 0.2548388800093034    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2117   score: 3.0   memory length: 476556   epsilon: 0.2544171400093007    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.51\n",
      "episode: 2118   score: 5.0   memory length: 476827   epsilon: 0.2538805600092973    steps: 271    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 2119   score: 10.0   memory length: 477272   epsilon: 0.25299946000929174    steps: 445    lr: 6.400000000000001e-06     evaluation reward: 6.52\n",
      "episode: 2120   score: 8.0   memory length: 477738   epsilon: 0.2520767800092859    steps: 466    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2121   score: 8.0   memory length: 478016   epsilon: 0.2515263400092824    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2122   score: 6.0   memory length: 478373   epsilon: 0.25081948000927795    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2123   score: 7.0   memory length: 478801   epsilon: 0.2499720400092726    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
      "episode: 2124   score: 7.0   memory length: 479185   epsilon: 0.24921172000926778    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 6.53\n",
      "episode: 2125   score: 10.0   memory length: 479652   epsilon: 0.24828706000926193    steps: 467    lr: 6.400000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2126   score: 8.0   memory length: 480084   epsilon: 0.24743170000925652    steps: 432    lr: 6.400000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2127   score: 8.0   memory length: 480362   epsilon: 0.24688126000925303    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2128   score: 7.0   memory length: 480765   epsilon: 0.24608332000924799    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2129   score: 3.0   memory length: 480978   epsilon: 0.24566158000924532    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2130   score: 4.0   memory length: 481218   epsilon: 0.2451863800092423    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 6.59\n",
      "episode: 2131   score: 6.0   memory length: 481559   epsilon: 0.24451120000923804    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2132   score: 10.0   memory length: 482103   epsilon: 0.24343408000923122    steps: 544    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2133   score: 9.0   memory length: 482530   epsilon: 0.24258862000922587    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2134   score: 5.0   memory length: 482834   epsilon: 0.24198670000922207    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2135   score: 7.0   memory length: 483187   epsilon: 0.24128776000921764    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2136   score: 7.0   memory length: 483546   epsilon: 0.24057694000921315    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2137   score: 10.0   memory length: 484043   epsilon: 0.23959288000920692    steps: 497    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2138   score: 10.0   memory length: 484486   epsilon: 0.23871574000920137    steps: 443    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2139   score: 10.0   memory length: 484968   epsilon: 0.23776138000919533    steps: 482    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2140   score: 4.0   memory length: 485245   epsilon: 0.23721292000919186    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2141   score: 7.0   memory length: 485632   epsilon: 0.23644666000918702    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2142   score: 9.0   memory length: 486059   epsilon: 0.23560120000918167    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2143   score: 3.0   memory length: 486272   epsilon: 0.235179460009179    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2144   score: 5.0   memory length: 486599   epsilon: 0.2345320000091749    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2145   score: 6.0   memory length: 486922   epsilon: 0.23389246000917085    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2146   score: 8.0   memory length: 487313   epsilon: 0.23311828000916596    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2147   score: 6.0   memory length: 487689   epsilon: 0.23237380000916125    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2148   score: 7.0   memory length: 488062   epsilon: 0.23163526000915657    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2149   score: 7.0   memory length: 488428   epsilon: 0.230910580009152    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2150   score: 11.0   memory length: 488823   epsilon: 0.23012848000914704    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2151   score: 7.0   memory length: 489204   epsilon: 0.22937410000914227    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2152   score: 8.0   memory length: 489625   epsilon: 0.228540520009137    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2153   score: 6.0   memory length: 489949   epsilon: 0.22789900000913293    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2154   score: 9.0   memory length: 490385   epsilon: 0.22703572000912747    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2155   score: 11.0   memory length: 490787   epsilon: 0.22623976000912244    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2156   score: 3.0   memory length: 491000   epsilon: 0.22581802000911977    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2157   score: 7.0   memory length: 491376   epsilon: 0.22507354000911506    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2158   score: 5.0   memory length: 491662   epsilon: 0.22450726000911148    steps: 286    lr: 6.400000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2159   score: 5.0   memory length: 491971   epsilon: 0.2238954400091076    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2160   score: 4.0   memory length: 492251   epsilon: 0.2233410400091041    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2161   score: 4.0   memory length: 492528   epsilon: 0.22279258000910063    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2162   score: 5.0   memory length: 492836   epsilon: 0.22218274000909677    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2163   score: 5.0   memory length: 493129   epsilon: 0.2216026000090931    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2164   score: 9.0   memory length: 493603   epsilon: 0.22066408000908716    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2165   score: 4.0   memory length: 493878   epsilon: 0.22011958000908372    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2166   score: 7.0   memory length: 494300   epsilon: 0.21928402000907843    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2167   score: 6.0   memory length: 494613   epsilon: 0.2186642800090745    steps: 313    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2168   score: 3.0   memory length: 494824   epsilon: 0.21824650000907186    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2169   score: 3.0   memory length: 495053   epsilon: 0.217793080009069    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2170   score: 7.0   memory length: 495437   epsilon: 0.21703276000906419    steps: 384    lr: 6.400000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2171   score: 17.0   memory length: 495911   epsilon: 0.21609424000905825    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2172   score: 5.0   memory length: 496235   epsilon: 0.2154527200090542    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2173   score: 6.0   memory length: 496565   epsilon: 0.21479932000905005    steps: 330    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2174   score: 8.0   memory length: 497006   epsilon: 0.21392614000904453    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2175   score: 7.0   memory length: 497372   epsilon: 0.21320146000903994    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2176   score: 7.0   memory length: 497728   epsilon: 0.21249658000903549    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2177   score: 6.0   memory length: 498070   epsilon: 0.2118194200090312    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2178   score: 8.0   memory length: 498494   epsilon: 0.2109799000090259    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2179   score: 4.0   memory length: 498740   epsilon: 0.2104928200090228    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2180   score: 10.0   memory length: 499219   epsilon: 0.2095444000090168    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2181   score: 5.0   memory length: 499512   epsilon: 0.20896426000901314    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2182   score: 13.0   memory length: 500013   epsilon: 0.20797228000900686    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2183   score: 9.0   memory length: 500504   epsilon: 0.2070001000090007    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2184   score: 7.0   memory length: 500933   epsilon: 0.20615068000899534    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 6.8\n",
      "episode: 2185   score: 5.0   memory length: 501243   epsilon: 0.20553688000899145    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2186   score: 8.0   memory length: 501699   epsilon: 0.20463400000898574    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2187   score: 11.0   memory length: 502101   epsilon: 0.2038380400089807    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2188   score: 5.0   memory length: 502426   epsilon: 0.20319454000897663    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2189   score: 9.0   memory length: 502884   epsilon: 0.2022877000089709    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2190   score: 8.0   memory length: 503282   epsilon: 0.2014996600089659    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2191   score: 6.0   memory length: 503624   epsilon: 0.20082250000896162    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2192   score: 8.0   memory length: 504079   epsilon: 0.19992160000895592    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2193   score: 9.0   memory length: 504386   epsilon: 0.19931374000895208    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2194   score: 10.0   memory length: 504859   epsilon: 0.19837720000894615    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 6.96\n",
      "episode: 2195   score: 7.0   memory length: 505240   epsilon: 0.19762282000894138    steps: 381    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
      "episode: 2196   score: 4.0   memory length: 505518   epsilon: 0.1970723800089379    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
      "episode: 2197   score: 7.0   memory length: 505862   epsilon: 0.1963912600089336    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 6.94\n",
      "episode: 2198   score: 13.0   memory length: 506297   epsilon: 0.19552996000892814    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2199   score: 7.0   memory length: 506663   epsilon: 0.19480528000892355    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2200   score: 10.0   memory length: 507204   epsilon: 0.19373410000891678    steps: 541    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2201   score: 9.0   memory length: 507670   epsilon: 0.19281142000891094    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2202   score: 8.0   memory length: 508075   epsilon: 0.19200952000890587    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.02\n",
      "episode: 2203   score: 10.0   memory length: 508473   epsilon: 0.19122148000890088    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2204   score: 4.0   memory length: 508753   epsilon: 0.19066708000889737    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2205   score: 9.0   memory length: 509226   epsilon: 0.18973054000889145    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2206   score: 4.0   memory length: 509489   epsilon: 0.18920980000888815    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2207   score: 3.0   memory length: 509702   epsilon: 0.18878806000888548    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2208   score: 5.0   memory length: 510012   epsilon: 0.1881742600088816    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2209   score: 10.0   memory length: 510390   epsilon: 0.18742582000887686    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2210   score: 5.0   memory length: 510681   epsilon: 0.18684964000887322    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2211   score: 5.0   memory length: 510969   epsilon: 0.1862794000088696    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2212   score: 8.0   memory length: 511378   epsilon: 0.1854695800088645    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
      "episode: 2213   score: 6.0   memory length: 511741   epsilon: 0.18475084000885994    steps: 363    lr: 2.560000000000001e-06     evaluation reward: 6.98\n",
      "episode: 2214   score: 8.0   memory length: 512179   epsilon: 0.18388360000885445    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2215   score: 2.0   memory length: 512379   epsilon: 0.18348760000885195    steps: 200    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
      "episode: 2216   score: 6.0   memory length: 512736   epsilon: 0.18278074000884748    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2217   score: 7.0   memory length: 513057   epsilon: 0.18214516000884345    steps: 321    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2218   score: 8.0   memory length: 513493   epsilon: 0.181281880008838    steps: 436    lr: 2.560000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2219   score: 9.0   memory length: 513977   epsilon: 0.18032356000883193    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2220   score: 6.0   memory length: 514342   epsilon: 0.17960086000882736    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2221   score: 10.0   memory length: 514827   epsilon: 0.17864056000882128    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2222   score: 7.0   memory length: 515174   epsilon: 0.17795350000881693    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2223   score: 12.0   memory length: 515667   epsilon: 0.17697736000881076    steps: 493    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
      "episode: 2224   score: 11.0   memory length: 516241   epsilon: 0.17584084000880357    steps: 574    lr: 2.560000000000001e-06     evaluation reward: 7.16\n",
      "episode: 2225   score: 4.0   memory length: 516483   epsilon: 0.17536168000880054    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2226   score: 3.0   memory length: 516694   epsilon: 0.1749439000087979    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2227   score: 6.0   memory length: 517051   epsilon: 0.17423704000879342    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2228   score: 12.0   memory length: 517593   epsilon: 0.17316388000878663    steps: 542    lr: 2.560000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2229   score: 6.0   memory length: 517918   epsilon: 0.17252038000878256    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2230   score: 3.0   memory length: 518129   epsilon: 0.17210260000877992    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2231   score: 7.0   memory length: 518490   epsilon: 0.1713878200087754    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2232   score: 9.0   memory length: 518963   epsilon: 0.17045128000876947    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2233   score: 5.0   memory length: 519255   epsilon: 0.1698731200087658    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2234   score: 8.0   memory length: 519660   epsilon: 0.16907122000876074    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2235   score: 7.0   memory length: 520055   epsilon: 0.1682891200087558    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2236   score: 5.0   memory length: 520343   epsilon: 0.16771888000875218    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2237   score: 4.0   memory length: 520585   epsilon: 0.16723972000874915    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2238   score: 11.0   memory length: 521116   epsilon: 0.1661883400087425    steps: 531    lr: 2.560000000000001e-06     evaluation reward: 7.02\n",
      "episode: 2239   score: 8.0   memory length: 521520   epsilon: 0.16538842000873744    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2240   score: 10.0   memory length: 522025   epsilon: 0.1643885200087311    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2241   score: 5.0   memory length: 522314   epsilon: 0.1638163000087275    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2242   score: 4.0   memory length: 522571   epsilon: 0.16330744000872427    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 6.99\n",
      "episode: 2243   score: 7.0   memory length: 522939   epsilon: 0.16257880000871966    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 7.03\n",
      "episode: 2244   score: 10.0   memory length: 523424   epsilon: 0.16161850000871358    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2245   score: 7.0   memory length: 523806   epsilon: 0.1608621400087088    steps: 382    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2246   score: 5.0   memory length: 524081   epsilon: 0.16031764000870535    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2247   score: 14.0   memory length: 524783   epsilon: 0.15892768000869656    steps: 702    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2248   score: 5.0   memory length: 525057   epsilon: 0.15838516000869313    steps: 274    lr: 2.560000000000001e-06     evaluation reward: 7.12\n",
      "episode: 2249   score: 6.0   memory length: 525393   epsilon: 0.15771988000868892    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2250   score: 4.0   memory length: 525652   epsilon: 0.15720706000868567    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2251   score: 11.0   memory length: 526204   epsilon: 0.15611410000867876    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2252   score: 8.0   memory length: 526656   epsilon: 0.1552191400086731    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 7.08\n",
      "episode: 2253   score: 9.0   memory length: 527113   epsilon: 0.15431428000866737    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 7.11\n",
      "episode: 2254   score: 8.0   memory length: 527551   epsilon: 0.15344704000866188    steps: 438    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2255   score: 11.0   memory length: 528123   epsilon: 0.15231448000865472    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2256   score: 10.0   memory length: 528664   epsilon: 0.15124330000864794    steps: 541    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2257   score: 7.0   memory length: 529018   epsilon: 0.1505423800086435    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 7.17\n",
      "episode: 2258   score: 8.0   memory length: 529448   epsilon: 0.14969098000863812    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
      "episode: 2259   score: 7.0   memory length: 529834   epsilon: 0.14892670000863328    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
      "episode: 2260   score: 10.0   memory length: 530181   epsilon: 0.14823964000862894    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
      "episode: 2261   score: 9.0   memory length: 530637   epsilon: 0.14733676000862322    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 7.33\n",
      "episode: 2262   score: 5.0   memory length: 530926   epsilon: 0.1467645400086196    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 7.33\n",
      "episode: 2263   score: 5.0   memory length: 531201   epsilon: 0.14622004000861616    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 7.33\n",
      "episode: 2264   score: 4.0   memory length: 531460   epsilon: 0.14570722000861291    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
      "episode: 2265   score: 6.0   memory length: 531814   epsilon: 0.14500630000860848    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 7.3\n",
      "episode: 2266   score: 5.0   memory length: 532142   epsilon: 0.14435686000860437    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
      "episode: 2267   score: 11.0   memory length: 532563   epsilon: 0.1435232800085991    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 7.33\n",
      "episode: 2268   score: 12.0   memory length: 533146   epsilon: 0.1423689400085918    steps: 583    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
      "episode: 2269   score: 18.0   memory length: 533597   epsilon: 0.14147596000858614    steps: 451    lr: 2.560000000000001e-06     evaluation reward: 7.57\n",
      "episode: 2270   score: 6.0   memory length: 533932   epsilon: 0.14081266000858195    steps: 335    lr: 2.560000000000001e-06     evaluation reward: 7.56\n",
      "episode: 2271   score: 8.0   memory length: 534348   epsilon: 0.13998898000857674    steps: 416    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2272   score: 6.0   memory length: 534691   epsilon: 0.13930984000857244    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2273   score: 5.0   memory length: 534983   epsilon: 0.13873168000856878    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2274   score: 8.0   memory length: 535420   epsilon: 0.1378664200085633    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2275   score: 6.0   memory length: 535797   epsilon: 0.13711996000855858    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 2276   score: 11.0   memory length: 536243   epsilon: 0.136236880008553    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 2277   score: 6.0   memory length: 536582   epsilon: 0.13556566000854875    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 2278   score: 11.0   memory length: 537010   epsilon: 0.1347182200085434    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 7.53\n",
      "episode: 2279   score: 6.0   memory length: 537334   epsilon: 0.13407670000853933    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 2280   score: 5.0   memory length: 537640   epsilon: 0.1334708200085355    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 2281   score: 8.0   memory length: 538060   epsilon: 0.13263922000853023    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 7.53\n",
      "episode: 2282   score: 7.0   memory length: 538449   epsilon: 0.13186900000852536    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2283   score: 4.0   memory length: 538707   epsilon: 0.13135816000852213    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
      "episode: 2284   score: 8.0   memory length: 539183   epsilon: 0.13041568000851617    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2285   score: 3.0   memory length: 539413   epsilon: 0.12996028000851328    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 7.41\n",
      "episode: 2286   score: 10.0   memory length: 539909   epsilon: 0.12897820000850707    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2287   score: 3.0   memory length: 540122   epsilon: 0.1285564600085044    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.35\n",
      "episode: 2288   score: 14.0   memory length: 540648   epsilon: 0.1275149800084978    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 2289   score: 11.0   memory length: 541184   epsilon: 0.1264537000084911    steps: 536    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 2290   score: 5.0   memory length: 541497   epsilon: 0.12583396000848718    steps: 313    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2291   score: 11.0   memory length: 541920   epsilon: 0.1249964200084819    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2292   score: 8.0   memory length: 542197   epsilon: 0.12444796000848228    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2293   score: 5.0   memory length: 542502   epsilon: 0.12384406000848269    steps: 305    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 2294   score: 10.0   memory length: 543010   epsilon: 0.12283822000848338    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 2295   score: 4.0   memory length: 543267   epsilon: 0.12232936000848373    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 7.41\n",
      "episode: 2296   score: 11.0   memory length: 543829   epsilon: 0.12121660000848448    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2297   score: 10.0   memory length: 544328   epsilon: 0.12022858000848516    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
      "episode: 2298   score: 5.0   memory length: 544603   epsilon: 0.11968408000848553    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2299   score: 9.0   memory length: 544922   epsilon: 0.11905246000848596    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
      "episode: 2300   score: 3.0   memory length: 545135   epsilon: 0.11863072000848625    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 2301   score: 3.0   memory length: 545365   epsilon: 0.11817532000848656    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 7.32\n",
      "episode: 2302   score: 5.0   memory length: 545673   epsilon: 0.11756548000848697    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 7.29\n",
      "episode: 2303   score: 6.0   memory length: 546009   epsilon: 0.11690020000848743    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
      "episode: 2304   score: 6.0   memory length: 546333   epsilon: 0.11625868000848787    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
      "episode: 2305   score: 7.0   memory length: 546762   epsilon: 0.11540926000848845    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
      "episode: 2306   score: 6.0   memory length: 547099   epsilon: 0.1147420000084889    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 7.27\n",
      "episode: 2307   score: 10.0   memory length: 547620   epsilon: 0.1137104200084896    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 7.34\n",
      "episode: 2308   score: 8.0   memory length: 548007   epsilon: 0.11294416000849013    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 7.37\n",
      "episode: 2309   score: 11.0   memory length: 548584   epsilon: 0.1118017000084909    steps: 577    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 2310   score: 5.0   memory length: 548890   epsilon: 0.11119582000849132    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 2311   score: 11.0   memory length: 549435   epsilon: 0.11011672000849206    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 7.44\n",
      "episode: 2312   score: 10.0   memory length: 549943   epsilon: 0.10911088000849274    steps: 508    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 2313   score: 8.0   memory length: 550341   epsilon: 0.10832284000849328    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2314   score: 8.0   memory length: 550761   epsilon: 0.10749124000849385    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2315   score: 6.0   memory length: 551087   epsilon: 0.10684576000849429    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 7.52\n",
      "episode: 2316   score: 8.0   memory length: 551480   epsilon: 0.10606762000849482    steps: 393    lr: 2.560000000000001e-06     evaluation reward: 7.54\n",
      "episode: 2317   score: 12.0   memory length: 552067   epsilon: 0.10490536000849561    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2318   score: 10.0   memory length: 552470   epsilon: 0.10410742000849615    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2319   score: 9.0   memory length: 552924   epsilon: 0.10320850000849677    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2320   score: 9.0   memory length: 553378   epsilon: 0.10230958000849738    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.64\n",
      "episode: 2321   score: 8.0   memory length: 553854   epsilon: 0.10136710000849802    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 7.62\n",
      "episode: 2322   score: 8.0   memory length: 554280   epsilon: 0.1005236200084986    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
      "episode: 2323   score: 5.0   memory length: 554570   epsilon: 0.09994942000849899    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 7.56\n",
      "episode: 2324   score: 6.0   memory length: 554897   epsilon: 0.09930196000849943    steps: 327    lr: 2.560000000000001e-06     evaluation reward: 7.51\n",
      "episode: 2325   score: 7.0   memory length: 555326   epsilon: 0.09845254000850001    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 7.54\n",
      "episode: 2326   score: 5.0   memory length: 555650   epsilon: 0.09781102000850045    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.56\n",
      "episode: 2327   score: 10.0   memory length: 556185   epsilon: 0.09675172000850117    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2328   score: 10.0   memory length: 556706   epsilon: 0.09572014000850187    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 7.58\n",
      "episode: 2329   score: 8.0   memory length: 556984   epsilon: 0.09516970000850225    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2330   score: 8.0   memory length: 557397   epsilon: 0.09435196000850281    steps: 413    lr: 2.560000000000001e-06     evaluation reward: 7.65\n",
      "episode: 2331   score: 5.0   memory length: 557690   epsilon: 0.0937718200085032    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 7.63\n",
      "episode: 2332   score: 13.0   memory length: 558069   epsilon: 0.09302140000850372    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
      "episode: 2333   score: 10.0   memory length: 558544   epsilon: 0.09208090000850436    steps: 475    lr: 2.560000000000001e-06     evaluation reward: 7.72\n",
      "episode: 2334   score: 6.0   memory length: 558888   epsilon: 0.09139978000850482    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
      "episode: 2335   score: 3.0   memory length: 559101   epsilon: 0.09097804000850511    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.66\n",
      "episode: 2336   score: 6.0   memory length: 559458   epsilon: 0.09027118000850559    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 7.67\n",
      "episode: 2337   score: 7.0   memory length: 559825   epsilon: 0.08954452000850609    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 7.7\n",
      "episode: 2338   score: 12.0   memory length: 560228   epsilon: 0.08874658000850663    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2339   score: 8.0   memory length: 560632   epsilon: 0.08794666000850718    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 7.71\n",
      "episode: 2340   score: 7.0   memory length: 561053   epsilon: 0.08711308000850775    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 7.68\n",
      "episode: 2341   score: 13.0   memory length: 561403   epsilon: 0.08642008000850822    steps: 350    lr: 2.560000000000001e-06     evaluation reward: 7.76\n",
      "episode: 2342   score: 6.0   memory length: 561747   epsilon: 0.08573896000850868    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2343   score: 12.0   memory length: 562296   epsilon: 0.08465194000850942    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2344   score: 8.0   memory length: 562688   epsilon: 0.08387578000850995    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2345   score: 10.0   memory length: 563145   epsilon: 0.08297092000851057    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2346   score: 11.0   memory length: 563590   epsilon: 0.08208982000851117    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 7.9\n",
      "episode: 2347   score: 7.0   memory length: 563991   epsilon: 0.08129584000851171    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2348   score: 7.0   memory length: 564352   epsilon: 0.0805810600085122    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2349   score: 3.0   memory length: 564565   epsilon: 0.08015932000851249    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2350   score: 11.0   memory length: 565115   epsilon: 0.07907032000851323    steps: 550    lr: 2.560000000000001e-06     evaluation reward: 7.89\n",
      "episode: 2351   score: 7.0   memory length: 565482   epsilon: 0.07834366000851373    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2352   score: 3.0   memory length: 565710   epsilon: 0.07789222000851403    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2353   score: 10.0   memory length: 566234   epsilon: 0.07685470000851474    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 7.81\n",
      "episode: 2354   score: 10.0   memory length: 566749   epsilon: 0.07583500000851544    steps: 515    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2355   score: 10.0   memory length: 567290   epsilon: 0.07476382000851617    steps: 541    lr: 2.560000000000001e-06     evaluation reward: 7.82\n",
      "episode: 2356   score: 7.0   memory length: 567678   epsilon: 0.07399558000851669    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2357   score: 8.0   memory length: 567956   epsilon: 0.07344514000851707    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2358   score: 8.0   memory length: 568409   epsilon: 0.07254820000851768    steps: 453    lr: 2.560000000000001e-06     evaluation reward: 7.8\n",
      "episode: 2359   score: 10.0   memory length: 568804   epsilon: 0.07176610000851821    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2360   score: 12.0   memory length: 569226   epsilon: 0.07093054000851878    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 7.85\n",
      "episode: 2361   score: 7.0   memory length: 569635   epsilon: 0.07012072000851934    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2362   score: 5.0   memory length: 569912   epsilon: 0.06957226000851971    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 7.83\n",
      "episode: 2363   score: 6.0   memory length: 570249   epsilon: 0.06890500000852016    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 7.84\n",
      "episode: 2364   score: 11.0   memory length: 570772   epsilon: 0.06786946000852087    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 7.91\n",
      "episode: 2365   score: 13.0   memory length: 571124   epsilon: 0.06717250000852135    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 7.98\n",
      "episode: 2366   score: 11.0   memory length: 571521   epsilon: 0.06638644000852188    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 8.04\n",
      "episode: 2367   score: 6.0   memory length: 571842   epsilon: 0.06575086000852232    steps: 321    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2368   score: 5.0   memory length: 572115   epsilon: 0.06521032000852268    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 7.92\n",
      "episode: 2369   score: 5.0   memory length: 572390   epsilon: 0.06466582000852306    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2370   score: 5.0   memory length: 572721   epsilon: 0.0640104400085235    steps: 331    lr: 2.560000000000001e-06     evaluation reward: 7.78\n",
      "episode: 2371   score: 9.0   memory length: 573046   epsilon: 0.06336694000852394    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 7.79\n",
      "episode: 2372   score: 15.0   memory length: 573641   epsilon: 0.062188840008524746    steps: 595    lr: 2.560000000000001e-06     evaluation reward: 7.88\n",
      "episode: 2373   score: 9.0   memory length: 574111   epsilon: 0.06125824000852538    steps: 470    lr: 2.560000000000001e-06     evaluation reward: 7.92\n",
      "episode: 2374   score: 13.0   memory length: 574760   epsilon: 0.05997322000852626    steps: 649    lr: 2.560000000000001e-06     evaluation reward: 7.97\n",
      "episode: 2375   score: 9.0   memory length: 575086   epsilon: 0.0593277400085267    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 8.0\n",
      "episode: 2376   score: 10.0   memory length: 575585   epsilon: 0.05833972000852737    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2377   score: 8.0   memory length: 575974   epsilon: 0.057569500008527896    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 8.01\n",
      "episode: 2378   score: 9.0   memory length: 576466   epsilon: 0.05659534000852856    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 7.99\n",
      "episode: 2379   score: 12.0   memory length: 577053   epsilon: 0.05543308000852935    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 8.05\n",
      "episode: 2380   score: 8.0   memory length: 577452   epsilon: 0.05464306000852989    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 8.08\n",
      "episode: 2381   score: 13.0   memory length: 577936   epsilon: 0.053684740008530546    steps: 484    lr: 2.560000000000001e-06     evaluation reward: 8.13\n",
      "episode: 2382   score: 3.0   memory length: 578168   epsilon: 0.05322538000853086    steps: 232    lr: 2.560000000000001e-06     evaluation reward: 8.09\n",
      "episode: 2383   score: 11.0   memory length: 578578   epsilon: 0.05241358000853141    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 8.16\n",
      "episode: 2384   score: 10.0   memory length: 579107   epsilon: 0.05136616000853213    steps: 529    lr: 2.560000000000001e-06     evaluation reward: 8.18\n",
      "episode: 2385   score: 9.0   memory length: 579539   epsilon: 0.05051080000853271    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 8.24\n",
      "episode: 2386   score: 12.0   memory length: 580125   epsilon: 0.0493505200085335    steps: 586    lr: 2.560000000000001e-06     evaluation reward: 8.26\n",
      "episode: 2387   score: 11.0   memory length: 580479   epsilon: 0.04864960000853398    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2388   score: 10.0   memory length: 580982   epsilon: 0.04765366000853466    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 8.3\n",
      "episode: 2389   score: 11.0   memory length: 581400   epsilon: 0.046826020008535224    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 8.3\n",
      "episode: 2390   score: 9.0   memory length: 581855   epsilon: 0.04592512000853584    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 8.34\n",
      "episode: 2391   score: 8.0   memory length: 582307   epsilon: 0.04503016000853645    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 8.31\n",
      "episode: 2392   score: 4.0   memory length: 582549   epsilon: 0.044551000008536776    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 8.27\n",
      "episode: 2393   score: 9.0   memory length: 582860   epsilon: 0.043935220008537196    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 8.31\n",
      "episode: 2394   score: 10.0   memory length: 583347   epsilon: 0.042970960008537853    steps: 487    lr: 2.560000000000001e-06     evaluation reward: 8.31\n",
      "episode: 2395   score: 10.0   memory length: 583888   epsilon: 0.041899780008538584    steps: 541    lr: 2.560000000000001e-06     evaluation reward: 8.37\n",
      "episode: 2396   score: 6.0   memory length: 584228   epsilon: 0.04122658000853904    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 8.32\n",
      "episode: 2397   score: 3.0   memory length: 584441   epsilon: 0.04080484000853933    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 8.25\n",
      "episode: 2398   score: 8.0   memory length: 584888   epsilon: 0.039919780008539935    steps: 447    lr: 2.560000000000001e-06     evaluation reward: 8.28\n",
      "episode: 2399   score: 10.0   memory length: 585461   epsilon: 0.03878524000854071    steps: 573    lr: 2.560000000000001e-06     evaluation reward: 8.29\n",
      "episode: 2400   score: 9.0   memory length: 585772   epsilon: 0.03816946000854113    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 8.35\n",
      "episode: 2401   score: 9.0   memory length: 586262   epsilon: 0.03719926000854179    steps: 490    lr: 2.560000000000001e-06     evaluation reward: 8.41\n",
      "episode: 2402   score: 11.0   memory length: 586616   epsilon: 0.03649834000854227    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 8.47\n",
      "episode: 2403   score: 11.0   memory length: 587180   epsilon: 0.03538162000854303    steps: 564    lr: 2.560000000000001e-06     evaluation reward: 8.52\n",
      "episode: 2404   score: 7.0   memory length: 587547   epsilon: 0.034654960008543526    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 8.53\n",
      "episode: 2405   score: 11.0   memory length: 587919   epsilon: 0.03391840000854403    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 8.57\n",
      "episode: 2406   score: 12.0   memory length: 588506   epsilon: 0.03275614000854482    steps: 587    lr: 2.560000000000001e-06     evaluation reward: 8.63\n",
      "episode: 2407   score: 11.0   memory length: 588899   epsilon: 0.03197800000854535    steps: 393    lr: 2.560000000000001e-06     evaluation reward: 8.64\n",
      "episode: 2408   score: 13.0   memory length: 589456   epsilon: 0.030875140008546104    steps: 557    lr: 2.560000000000001e-06     evaluation reward: 8.69\n",
      "episode: 2409   score: 8.0   memory length: 589863   epsilon: 0.030069280008546653    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 8.66\n",
      "episode: 2410   score: 6.0   memory length: 590187   epsilon: 0.02942776000854709    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 8.67\n",
      "episode: 2411   score: 11.0   memory length: 590752   epsilon: 0.028309060008547854    steps: 565    lr: 2.560000000000001e-06     evaluation reward: 8.67\n",
      "episode: 2412   score: 12.0   memory length: 591139   epsilon: 0.027542800008548377    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 8.69\n",
      "episode: 2413   score: 12.0   memory length: 591763   epsilon: 0.02630728000854922    steps: 624    lr: 2.560000000000001e-06     evaluation reward: 8.73\n",
      "episode: 2414   score: 10.0   memory length: 592280   epsilon: 0.025283620008549917    steps: 517    lr: 2.560000000000001e-06     evaluation reward: 8.75\n",
      "episode: 2415   score: 6.0   memory length: 592623   epsilon: 0.02460448000855038    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 8.75\n",
      "episode: 2416   score: 11.0   memory length: 593158   epsilon: 0.023545180008551103    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 8.78\n",
      "episode: 2417   score: 7.0   memory length: 593513   epsilon: 0.022842280008551583    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 8.73\n",
      "episode: 2418   score: 9.0   memory length: 593949   epsilon: 0.02197900000855217    steps: 436    lr: 2.560000000000001e-06     evaluation reward: 8.72\n",
      "episode: 2419   score: 8.0   memory length: 594328   epsilon: 0.021228580008552683    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 8.71\n",
      "episode: 2420   score: 10.0   memory length: 594813   epsilon: 0.020268280008553338    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 8.72\n",
      "episode: 2421   score: 11.0   memory length: 595171   epsilon: 0.01955944000855382    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 8.75\n",
      "episode: 2422   score: 11.0   memory length: 595529   epsilon: 0.018850600008554305    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 8.78\n",
      "episode: 2423   score: 9.0   memory length: 595998   epsilon: 0.01792198000855494    steps: 469    lr: 2.560000000000001e-06     evaluation reward: 8.82\n",
      "episode: 2424   score: 10.0   memory length: 596412   epsilon: 0.017102260008555498    steps: 414    lr: 2.560000000000001e-06     evaluation reward: 8.86\n",
      "episode: 2425   score: 11.0   memory length: 596961   epsilon: 0.01601524000855624    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 8.9\n",
      "episode: 2426   score: 6.0   memory length: 597298   epsilon: 0.015347980008556451    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 8.91\n",
      "episode: 2427   score: 10.0   memory length: 597777   epsilon: 0.014399560008556267    steps: 479    lr: 2.560000000000001e-06     evaluation reward: 8.91\n",
      "episode: 2428   score: 10.0   memory length: 598293   epsilon: 0.013377880008556069    steps: 516    lr: 2.560000000000001e-06     evaluation reward: 8.91\n",
      "episode: 2429   score: 7.0   memory length: 598679   epsilon: 0.01261360000855592    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 8.9\n",
      "episode: 2430   score: 10.0   memory length: 599046   epsilon: 0.01188694000855578    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 8.92\n",
      "episode: 2431   score: 12.0   memory length: 599643   epsilon: 0.01070488000855555    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 8.99\n",
      "episode: 2432   score: 11.0   memory length: 600202   epsilon: 0.009998020008555413    steps: 559    lr: 1.0240000000000005e-06     evaluation reward: 8.97\n",
      "episode: 2433   score: 6.0   memory length: 600525   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 8.93\n",
      "episode: 2434   score: 7.0   memory length: 600904   epsilon: 0.009998020008555413    steps: 379    lr: 1.0240000000000005e-06     evaluation reward: 8.94\n",
      "episode: 2435   score: 7.0   memory length: 601249   epsilon: 0.009998020008555413    steps: 345    lr: 1.0240000000000005e-06     evaluation reward: 8.98\n",
      "episode: 2436   score: 12.0   memory length: 601725   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 9.04\n",
      "episode: 2437   score: 10.0   memory length: 602267   epsilon: 0.009998020008555413    steps: 542    lr: 1.0240000000000005e-06     evaluation reward: 9.07\n",
      "episode: 2438   score: 16.0   memory length: 602997   epsilon: 0.009998020008555413    steps: 730    lr: 1.0240000000000005e-06     evaluation reward: 9.11\n",
      "episode: 2439   score: 7.0   memory length: 603398   epsilon: 0.009998020008555413    steps: 401    lr: 1.0240000000000005e-06     evaluation reward: 9.1\n",
      "episode: 2440   score: 11.0   memory length: 603756   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 9.14\n",
      "episode: 2441   score: 13.0   memory length: 604427   epsilon: 0.009998020008555413    steps: 671    lr: 1.0240000000000005e-06     evaluation reward: 9.14\n",
      "episode: 2442   score: 6.0   memory length: 604733   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 9.14\n",
      "episode: 2443   score: 14.0   memory length: 605209   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 9.16\n",
      "episode: 2444   score: 16.0   memory length: 605837   epsilon: 0.009998020008555413    steps: 628    lr: 1.0240000000000005e-06     evaluation reward: 9.24\n",
      "episode: 2445   score: 11.0   memory length: 606428   epsilon: 0.009998020008555413    steps: 591    lr: 1.0240000000000005e-06     evaluation reward: 9.25\n",
      "episode: 2446   score: 6.0   memory length: 606755   epsilon: 0.009998020008555413    steps: 327    lr: 1.0240000000000005e-06     evaluation reward: 9.2\n",
      "episode: 2447   score: 15.0   memory length: 607352   epsilon: 0.009998020008555413    steps: 597    lr: 1.0240000000000005e-06     evaluation reward: 9.28\n",
      "episode: 2448   score: 10.0   memory length: 607710   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 9.31\n",
      "episode: 2449   score: 11.0   memory length: 608227   epsilon: 0.009998020008555413    steps: 517    lr: 1.0240000000000005e-06     evaluation reward: 9.39\n",
      "episode: 2450   score: 7.0   memory length: 608608   epsilon: 0.009998020008555413    steps: 381    lr: 1.0240000000000005e-06     evaluation reward: 9.35\n",
      "episode: 2451   score: 6.0   memory length: 608945   epsilon: 0.009998020008555413    steps: 337    lr: 1.0240000000000005e-06     evaluation reward: 9.34\n",
      "episode: 2452   score: 6.0   memory length: 609251   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 9.37\n",
      "episode: 2453   score: 3.0   memory length: 609464   epsilon: 0.009998020008555413    steps: 213    lr: 1.0240000000000005e-06     evaluation reward: 9.3\n",
      "episode: 2454   score: 17.0   memory length: 610000   epsilon: 0.009998020008555413    steps: 536    lr: 1.0240000000000005e-06     evaluation reward: 9.37\n",
      "episode: 2455   score: 12.0   memory length: 610458   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 9.39\n",
      "episode: 2456   score: 6.0   memory length: 610820   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 9.38\n",
      "episode: 2457   score: 12.0   memory length: 611266   epsilon: 0.009998020008555413    steps: 446    lr: 1.0240000000000005e-06     evaluation reward: 9.42\n",
      "episode: 2458   score: 8.0   memory length: 611720   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 9.42\n",
      "episode: 2459   score: 8.0   memory length: 612172   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 9.4\n",
      "episode: 2460   score: 19.0   memory length: 612757   epsilon: 0.009998020008555413    steps: 585    lr: 1.0240000000000005e-06     evaluation reward: 9.47\n",
      "episode: 2461   score: 8.0   memory length: 613168   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 9.48\n",
      "episode: 2462   score: 5.0   memory length: 613458   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 9.48\n",
      "episode: 2463   score: 9.0   memory length: 613912   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 9.51\n",
      "episode: 2464   score: 4.0   memory length: 614175   epsilon: 0.009998020008555413    steps: 263    lr: 1.0240000000000005e-06     evaluation reward: 9.44\n",
      "episode: 2465   score: 5.0   memory length: 614465   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 9.36\n",
      "episode: 2466   score: 11.0   memory length: 615028   epsilon: 0.009998020008555413    steps: 563    lr: 1.0240000000000005e-06     evaluation reward: 9.36\n",
      "episode: 2467   score: 10.0   memory length: 615507   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 9.4\n",
      "episode: 2468   score: 6.0   memory length: 615825   epsilon: 0.009998020008555413    steps: 318    lr: 1.0240000000000005e-06     evaluation reward: 9.41\n",
      "episode: 2469   score: 6.0   memory length: 616151   epsilon: 0.009998020008555413    steps: 326    lr: 1.0240000000000005e-06     evaluation reward: 9.42\n",
      "episode: 2470   score: 8.0   memory length: 616572   epsilon: 0.009998020008555413    steps: 421    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2471   score: 7.0   memory length: 616971   epsilon: 0.009998020008555413    steps: 399    lr: 1.0240000000000005e-06     evaluation reward: 9.43\n",
      "episode: 2472   score: 11.0   memory length: 617329   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 9.39\n",
      "episode: 2473   score: 11.0   memory length: 617885   epsilon: 0.009998020008555413    steps: 556    lr: 1.0240000000000005e-06     evaluation reward: 9.41\n",
      "episode: 2474   score: 17.0   memory length: 618658   epsilon: 0.009998020008555413    steps: 773    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2475   score: 7.0   memory length: 618995   epsilon: 0.009998020008555413    steps: 337    lr: 1.0240000000000005e-06     evaluation reward: 9.43\n",
      "episode: 2476   score: 9.0   memory length: 619456   epsilon: 0.009998020008555413    steps: 461    lr: 1.0240000000000005e-06     evaluation reward: 9.42\n",
      "episode: 2477   score: 5.0   memory length: 619766   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 9.39\n",
      "episode: 2478   score: 11.0   memory length: 620326   epsilon: 0.009998020008555413    steps: 560    lr: 1.0240000000000005e-06     evaluation reward: 9.41\n",
      "episode: 2479   score: 10.0   memory length: 620871   epsilon: 0.009998020008555413    steps: 545    lr: 1.0240000000000005e-06     evaluation reward: 9.39\n",
      "episode: 2480   score: 10.0   memory length: 621408   epsilon: 0.009998020008555413    steps: 537    lr: 1.0240000000000005e-06     evaluation reward: 9.41\n",
      "episode: 2481   score: 12.0   memory length: 621953   epsilon: 0.009998020008555413    steps: 545    lr: 1.0240000000000005e-06     evaluation reward: 9.4\n",
      "episode: 2482   score: 17.0   memory length: 622589   epsilon: 0.009998020008555413    steps: 636    lr: 1.0240000000000005e-06     evaluation reward: 9.54\n",
      "episode: 2483   score: 18.0   memory length: 623283   epsilon: 0.009998020008555413    steps: 694    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2484   score: 12.0   memory length: 623775   epsilon: 0.009998020008555413    steps: 492    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2485   score: 10.0   memory length: 624269   epsilon: 0.009998020008555413    steps: 494    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2486   score: 11.0   memory length: 624627   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2487   score: 5.0   memory length: 624937   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 9.57\n",
      "episode: 2488   score: 6.0   memory length: 625243   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 9.53\n",
      "episode: 2489   score: 6.0   memory length: 625568   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 9.48\n",
      "episode: 2490   score: 6.0   memory length: 625930   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 9.45\n",
      "episode: 2491   score: 12.0   memory length: 626317   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 9.49\n",
      "episode: 2492   score: 7.0   memory length: 626672   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 9.52\n",
      "episode: 2493   score: 13.0   memory length: 627166   epsilon: 0.009998020008555413    steps: 494    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2494   score: 10.0   memory length: 627662   epsilon: 0.009998020008555413    steps: 496    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2495   score: 10.0   memory length: 628189   epsilon: 0.009998020008555413    steps: 527    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2496   score: 10.0   memory length: 628514   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2497   score: 14.0   memory length: 628986   epsilon: 0.009998020008555413    steps: 472    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2498   score: 6.0   memory length: 629326   epsilon: 0.009998020008555413    steps: 340    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2499   score: 7.0   memory length: 629699   epsilon: 0.009998020008555413    steps: 373    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2500   score: 9.0   memory length: 630182   epsilon: 0.009998020008555413    steps: 483    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2501   score: 10.0   memory length: 630687   epsilon: 0.009998020008555413    steps: 505    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2502   score: 14.0   memory length: 631233   epsilon: 0.009998020008555413    steps: 546    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2503   score: 10.0   memory length: 631748   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2504   score: 8.0   memory length: 632174   epsilon: 0.009998020008555413    steps: 426    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2505   score: 6.0   memory length: 632481   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2506   score: 12.0   memory length: 633086   epsilon: 0.009998020008555413    steps: 605    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2507   score: 9.0   memory length: 633521   epsilon: 0.009998020008555413    steps: 435    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2508   score: 6.0   memory length: 633827   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2509   score: 12.0   memory length: 634384   epsilon: 0.009998020008555413    steps: 557    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2510   score: 9.0   memory length: 634860   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2511   score: 7.0   memory length: 635212   epsilon: 0.009998020008555413    steps: 352    lr: 1.0240000000000005e-06     evaluation reward: 9.59\n",
      "episode: 2512   score: 14.0   memory length: 635669   epsilon: 0.009998020008555413    steps: 457    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2513   score: 14.0   memory length: 636068   epsilon: 0.009998020008555413    steps: 399    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2514   score: 13.0   memory length: 636722   epsilon: 0.009998020008555413    steps: 654    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2515   score: 12.0   memory length: 637188   epsilon: 0.009998020008555413    steps: 466    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2516   score: 9.0   memory length: 637647   epsilon: 0.009998020008555413    steps: 459    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2517   score: 8.0   memory length: 638076   epsilon: 0.009998020008555413    steps: 429    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2518   score: 4.0   memory length: 638339   epsilon: 0.009998020008555413    steps: 263    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2519   score: 8.0   memory length: 638815   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2520   score: 12.0   memory length: 639202   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2521   score: 6.0   memory length: 639508   epsilon: 0.009998020008555413    steps: 306    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2522   score: 7.0   memory length: 639880   epsilon: 0.009998020008555413    steps: 372    lr: 1.0240000000000005e-06     evaluation reward: 9.59\n",
      "episode: 2523   score: 12.0   memory length: 640484   epsilon: 0.009998020008555413    steps: 604    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2524   score: 10.0   memory length: 640986   epsilon: 0.009998020008555413    steps: 502    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2525   score: 12.0   memory length: 641529   epsilon: 0.009998020008555413    steps: 543    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2526   score: 14.0   memory length: 642055   epsilon: 0.009998020008555413    steps: 526    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2527   score: 6.0   memory length: 642380   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2528   score: 8.0   memory length: 642806   epsilon: 0.009998020008555413    steps: 426    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2529   score: 11.0   memory length: 643348   epsilon: 0.009998020008555413    steps: 542    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2530   score: 12.0   memory length: 643753   epsilon: 0.009998020008555413    steps: 405    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2531   score: 7.0   memory length: 644111   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2532   score: 6.0   memory length: 644473   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2533   score: 11.0   memory length: 644871   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2534   score: 12.0   memory length: 645494   epsilon: 0.009998020008555413    steps: 623    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2535   score: 9.0   memory length: 645924   epsilon: 0.009998020008555413    steps: 430    lr: 1.0240000000000005e-06     evaluation reward: 9.73\n",
      "episode: 2536   score: 11.0   memory length: 646430   epsilon: 0.009998020008555413    steps: 506    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2537   score: 8.0   memory length: 646834   epsilon: 0.009998020008555413    steps: 404    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2538   score: 14.0   memory length: 647375   epsilon: 0.009998020008555413    steps: 541    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2539   score: 10.0   memory length: 647929   epsilon: 0.009998020008555413    steps: 554    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2540   score: 9.0   memory length: 648411   epsilon: 0.009998020008555413    steps: 482    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2541   score: 12.0   memory length: 648862   epsilon: 0.009998020008555413    steps: 451    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2542   score: 10.0   memory length: 649377   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2543   score: 9.0   memory length: 649825   epsilon: 0.009998020008555413    steps: 448    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2544   score: 7.0   memory length: 650214   epsilon: 0.009998020008555413    steps: 389    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2545   score: 13.0   memory length: 650905   epsilon: 0.009998020008555413    steps: 691    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2546   score: 13.0   memory length: 651382   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2547   score: 10.0   memory length: 651897   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2548   score: 6.0   memory length: 652221   epsilon: 0.009998020008555413    steps: 324    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2549   score: 11.0   memory length: 652698   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2550   score: 12.0   memory length: 653145   epsilon: 0.009998020008555413    steps: 447    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2551   score: 13.0   memory length: 653622   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2552   score: 15.0   memory length: 654145   epsilon: 0.009998020008555413    steps: 523    lr: 1.0240000000000005e-06     evaluation reward: 9.79\n",
      "episode: 2553   score: 6.0   memory length: 654487   epsilon: 0.009998020008555413    steps: 342    lr: 1.0240000000000005e-06     evaluation reward: 9.82\n",
      "episode: 2554   score: 17.0   memory length: 655124   epsilon: 0.009998020008555413    steps: 637    lr: 1.0240000000000005e-06     evaluation reward: 9.82\n",
      "episode: 2555   score: 5.0   memory length: 655434   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 9.75\n",
      "episode: 2556   score: 7.0   memory length: 655802   epsilon: 0.009998020008555413    steps: 368    lr: 1.0240000000000005e-06     evaluation reward: 9.76\n",
      "episode: 2557   score: 12.0   memory length: 656408   epsilon: 0.009998020008555413    steps: 606    lr: 1.0240000000000005e-06     evaluation reward: 9.76\n",
      "episode: 2558   score: 6.0   memory length: 656726   epsilon: 0.009998020008555413    steps: 318    lr: 1.0240000000000005e-06     evaluation reward: 9.74\n",
      "episode: 2559   score: 9.0   memory length: 657203   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 9.75\n",
      "episode: 2560   score: 3.0   memory length: 657416   epsilon: 0.009998020008555413    steps: 213    lr: 1.0240000000000005e-06     evaluation reward: 9.59\n",
      "episode: 2561   score: 11.0   memory length: 657838   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2562   score: 11.0   memory length: 658397   epsilon: 0.009998020008555413    steps: 559    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2563   score: 7.0   memory length: 658754   epsilon: 0.009998020008555413    steps: 357    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2564   score: 11.0   memory length: 659290   epsilon: 0.009998020008555413    steps: 536    lr: 1.0240000000000005e-06     evaluation reward: 9.73\n",
      "episode: 2565   score: 11.0   memory length: 659781   epsilon: 0.009998020008555413    steps: 491    lr: 1.0240000000000005e-06     evaluation reward: 9.79\n",
      "episode: 2566   score: 7.0   memory length: 660163   epsilon: 0.009998020008555413    steps: 382    lr: 1.0240000000000005e-06     evaluation reward: 9.75\n",
      "episode: 2567   score: 8.0   memory length: 660552   epsilon: 0.009998020008555413    steps: 389    lr: 1.0240000000000005e-06     evaluation reward: 9.73\n",
      "episode: 2568   score: 5.0   memory length: 660842   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2569   score: 10.0   memory length: 661387   epsilon: 0.009998020008555413    steps: 545    lr: 1.0240000000000005e-06     evaluation reward: 9.76\n",
      "episode: 2570   score: 7.0   memory length: 661797   epsilon: 0.009998020008555413    steps: 410    lr: 1.0240000000000005e-06     evaluation reward: 9.75\n",
      "episode: 2571   score: 10.0   memory length: 662312   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.78\n",
      "episode: 2572   score: 11.0   memory length: 662684   epsilon: 0.009998020008555413    steps: 372    lr: 1.0240000000000005e-06     evaluation reward: 9.78\n",
      "episode: 2573   score: 8.0   memory length: 663091   epsilon: 0.009998020008555413    steps: 407    lr: 1.0240000000000005e-06     evaluation reward: 9.75\n",
      "episode: 2574   score: 6.0   memory length: 663432   epsilon: 0.009998020008555413    steps: 341    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2575   score: 9.0   memory length: 663936   epsilon: 0.009998020008555413    steps: 504    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2576   score: 8.0   memory length: 664389   epsilon: 0.009998020008555413    steps: 453    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2577   score: 8.0   memory length: 664845   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2578   score: 12.0   memory length: 665442   epsilon: 0.009998020008555413    steps: 597    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2579   score: 9.0   memory length: 665919   epsilon: 0.009998020008555413    steps: 477    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2580   score: 7.0   memory length: 666293   epsilon: 0.009998020008555413    steps: 374    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2581   score: 8.0   memory length: 666745   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2582   score: 11.0   memory length: 667269   epsilon: 0.009998020008555413    steps: 524    lr: 1.0240000000000005e-06     evaluation reward: 9.55\n",
      "episode: 2583   score: 13.0   memory length: 667727   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 9.5\n",
      "episode: 2584   score: 13.0   memory length: 668398   epsilon: 0.009998020008555413    steps: 671    lr: 1.0240000000000005e-06     evaluation reward: 9.51\n",
      "episode: 2585   score: 12.0   memory length: 669042   epsilon: 0.009998020008555413    steps: 644    lr: 1.0240000000000005e-06     evaluation reward: 9.53\n",
      "episode: 2586   score: 13.0   memory length: 669534   epsilon: 0.009998020008555413    steps: 492    lr: 1.0240000000000005e-06     evaluation reward: 9.55\n",
      "episode: 2587   score: 6.0   memory length: 669890   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2588   score: 10.0   memory length: 670395   epsilon: 0.009998020008555413    steps: 505    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2589   score: 7.0   memory length: 670805   epsilon: 0.009998020008555413    steps: 410    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2590   score: 8.0   memory length: 671205   epsilon: 0.009998020008555413    steps: 400    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2591   score: 12.0   memory length: 671592   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2592   score: 10.0   memory length: 672088   epsilon: 0.009998020008555413    steps: 496    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2593   score: 9.0   memory length: 672517   epsilon: 0.009998020008555413    steps: 429    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2594   score: 5.0   memory length: 672807   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 9.57\n",
      "episode: 2595   score: 14.0   memory length: 673372   epsilon: 0.009998020008555413    steps: 565    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2596   score: 11.0   memory length: 673770   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2597   score: 10.0   memory length: 674273   epsilon: 0.009998020008555413    steps: 503    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2598   score: 6.0   memory length: 674595   epsilon: 0.009998020008555413    steps: 322    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2599   score: 7.0   memory length: 675006   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2600   score: 15.0   memory length: 675533   epsilon: 0.009998020008555413    steps: 527    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2601   score: 9.0   memory length: 676016   epsilon: 0.009998020008555413    steps: 483    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2602   score: 7.0   memory length: 676386   epsilon: 0.009998020008555413    steps: 370    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2603   score: 7.0   memory length: 676775   epsilon: 0.009998020008555413    steps: 389    lr: 1.0240000000000005e-06     evaluation reward: 9.53\n",
      "episode: 2604   score: 10.0   memory length: 677329   epsilon: 0.009998020008555413    steps: 554    lr: 1.0240000000000005e-06     evaluation reward: 9.55\n",
      "episode: 2605   score: 11.0   memory length: 677855   epsilon: 0.009998020008555413    steps: 526    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2606   score: 8.0   memory length: 678150   epsilon: 0.009998020008555413    steps: 295    lr: 1.0240000000000005e-06     evaluation reward: 9.56\n",
      "episode: 2607   score: 10.0   memory length: 678665   epsilon: 0.009998020008555413    steps: 515    lr: 1.0240000000000005e-06     evaluation reward: 9.57\n",
      "episode: 2608   score: 7.0   memory length: 679041   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2609   score: 15.0   memory length: 679641   epsilon: 0.009998020008555413    steps: 600    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2610   score: 6.0   memory length: 679959   epsilon: 0.009998020008555413    steps: 318    lr: 1.0240000000000005e-06     evaluation reward: 9.58\n",
      "episode: 2611   score: 15.0   memory length: 680399   epsilon: 0.009998020008555413    steps: 440    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2612   score: 9.0   memory length: 680862   epsilon: 0.009998020008555413    steps: 463    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2613   score: 7.0   memory length: 681246   epsilon: 0.009998020008555413    steps: 384    lr: 1.0240000000000005e-06     evaluation reward: 9.54\n",
      "episode: 2614   score: 11.0   memory length: 681818   epsilon: 0.009998020008555413    steps: 572    lr: 1.0240000000000005e-06     evaluation reward: 9.52\n",
      "episode: 2615   score: 15.0   memory length: 682321   epsilon: 0.009998020008555413    steps: 503    lr: 1.0240000000000005e-06     evaluation reward: 9.55\n",
      "episode: 2616   score: 11.0   memory length: 682892   epsilon: 0.009998020008555413    steps: 571    lr: 1.0240000000000005e-06     evaluation reward: 9.57\n",
      "episode: 2617   score: 11.0   memory length: 683469   epsilon: 0.009998020008555413    steps: 577    lr: 1.0240000000000005e-06     evaluation reward: 9.6\n",
      "episode: 2618   score: 8.0   memory length: 683923   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2619   score: 14.0   memory length: 684635   epsilon: 0.009998020008555413    steps: 712    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2620   score: 8.0   memory length: 685088   epsilon: 0.009998020008555413    steps: 453    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2621   score: 10.0   memory length: 685543   epsilon: 0.009998020008555413    steps: 455    lr: 1.0240000000000005e-06     evaluation reward: 9.7\n",
      "episode: 2622   score: 6.0   memory length: 685889   epsilon: 0.009998020008555413    steps: 346    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2623   score: 12.0   memory length: 686492   epsilon: 0.009998020008555413    steps: 603    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2624   score: 5.0   memory length: 686792   epsilon: 0.009998020008555413    steps: 300    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2625   score: 12.0   memory length: 687390   epsilon: 0.009998020008555413    steps: 598    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2626   score: 7.0   memory length: 687763   epsilon: 0.009998020008555413    steps: 373    lr: 1.0240000000000005e-06     evaluation reward: 9.57\n",
      "episode: 2627   score: 12.0   memory length: 688341   epsilon: 0.009998020008555413    steps: 578    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2628   score: 13.0   memory length: 688838   epsilon: 0.009998020008555413    steps: 497    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2629   score: 7.0   memory length: 689249   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 9.64\n",
      "episode: 2630   score: 7.0   memory length: 689614   epsilon: 0.009998020008555413    steps: 365    lr: 1.0240000000000005e-06     evaluation reward: 9.59\n",
      "episode: 2631   score: 10.0   memory length: 690097   epsilon: 0.009998020008555413    steps: 483    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2632   score: 10.0   memory length: 690584   epsilon: 0.009998020008555413    steps: 487    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2633   score: 11.0   memory length: 691053   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2634   score: 8.0   memory length: 691508   epsilon: 0.009998020008555413    steps: 455    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2635   score: 15.0   memory length: 692094   epsilon: 0.009998020008555413    steps: 586    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2636   score: 11.0   memory length: 692619   epsilon: 0.009998020008555413    steps: 525    lr: 1.0240000000000005e-06     evaluation reward: 9.68\n",
      "episode: 2637   score: 13.0   memory length: 693273   epsilon: 0.009998020008555413    steps: 654    lr: 1.0240000000000005e-06     evaluation reward: 9.73\n",
      "episode: 2638   score: 12.0   memory length: 693886   epsilon: 0.009998020008555413    steps: 613    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2639   score: 10.0   memory length: 694386   epsilon: 0.009998020008555413    steps: 500    lr: 1.0240000000000005e-06     evaluation reward: 9.71\n",
      "episode: 2640   score: 10.0   memory length: 694891   epsilon: 0.009998020008555413    steps: 505    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2641   score: 7.0   memory length: 695302   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2642   score: 9.0   memory length: 695769   epsilon: 0.009998020008555413    steps: 467    lr: 1.0240000000000005e-06     evaluation reward: 9.66\n",
      "episode: 2643   score: 10.0   memory length: 696311   epsilon: 0.009998020008555413    steps: 542    lr: 1.0240000000000005e-06     evaluation reward: 9.67\n",
      "episode: 2644   score: 12.0   memory length: 696878   epsilon: 0.009998020008555413    steps: 567    lr: 1.0240000000000005e-06     evaluation reward: 9.72\n",
      "episode: 2645   score: 10.0   memory length: 697423   epsilon: 0.009998020008555413    steps: 545    lr: 1.0240000000000005e-06     evaluation reward: 9.69\n",
      "episode: 2646   score: 7.0   memory length: 697825   epsilon: 0.009998020008555413    steps: 402    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2647   score: 8.0   memory length: 698225   epsilon: 0.009998020008555413    steps: 400    lr: 1.0240000000000005e-06     evaluation reward: 9.61\n",
      "episode: 2648   score: 10.0   memory length: 698749   epsilon: 0.009998020008555413    steps: 524    lr: 1.0240000000000005e-06     evaluation reward: 9.65\n",
      "episode: 2649   score: 9.0   memory length: 699230   epsilon: 0.009998020008555413    steps: 481    lr: 1.0240000000000005e-06     evaluation reward: 9.63\n",
      "episode: 2650   score: 11.0   memory length: 699604   epsilon: 0.009998020008555413    steps: 374    lr: 1.0240000000000005e-06     evaluation reward: 9.62\n",
      "episode: 2651   score: 11.0   memory length: 700141   epsilon: 0.009998020008555413    steps: 537    lr: 4.0960000000000023e-07     evaluation reward: 9.6\n",
      "episode: 2652   score: 8.0   memory length: 700574   epsilon: 0.009998020008555413    steps: 433    lr: 4.0960000000000023e-07     evaluation reward: 9.53\n",
      "episode: 2653   score: 6.0   memory length: 700915   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.53\n",
      "episode: 2654   score: 10.0   memory length: 701437   epsilon: 0.009998020008555413    steps: 522    lr: 4.0960000000000023e-07     evaluation reward: 9.46\n",
      "episode: 2655   score: 12.0   memory length: 701982   epsilon: 0.009998020008555413    steps: 545    lr: 4.0960000000000023e-07     evaluation reward: 9.53\n",
      "episode: 2656   score: 12.0   memory length: 702527   epsilon: 0.009998020008555413    steps: 545    lr: 4.0960000000000023e-07     evaluation reward: 9.58\n",
      "episode: 2657   score: 12.0   memory length: 703072   epsilon: 0.009998020008555413    steps: 545    lr: 4.0960000000000023e-07     evaluation reward: 9.58\n",
      "episode: 2658   score: 7.0   memory length: 703441   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.59\n",
      "episode: 2659   score: 13.0   memory length: 704055   epsilon: 0.009998020008555413    steps: 614    lr: 4.0960000000000023e-07     evaluation reward: 9.63\n",
      "episode: 2660   score: 8.0   memory length: 704455   epsilon: 0.009998020008555413    steps: 400    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2661   score: 8.0   memory length: 704855   epsilon: 0.009998020008555413    steps: 400    lr: 4.0960000000000023e-07     evaluation reward: 9.65\n",
      "episode: 2662   score: 12.0   memory length: 705427   epsilon: 0.009998020008555413    steps: 572    lr: 4.0960000000000023e-07     evaluation reward: 9.66\n",
      "episode: 2663   score: 15.0   memory length: 706016   epsilon: 0.009998020008555413    steps: 589    lr: 4.0960000000000023e-07     evaluation reward: 9.74\n",
      "episode: 2664   score: 12.0   memory length: 706553   epsilon: 0.009998020008555413    steps: 537    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2665   score: 3.0   memory length: 706766   epsilon: 0.009998020008555413    steps: 213    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2666   score: 15.0   memory length: 707329   epsilon: 0.009998020008555413    steps: 563    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2667   score: 7.0   memory length: 707719   epsilon: 0.009998020008555413    steps: 390    lr: 4.0960000000000023e-07     evaluation reward: 9.74\n",
      "episode: 2668   score: 8.0   memory length: 708119   epsilon: 0.009998020008555413    steps: 400    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2669   score: 15.0   memory length: 708705   epsilon: 0.009998020008555413    steps: 586    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2670   score: 6.0   memory length: 709011   epsilon: 0.009998020008555413    steps: 306    lr: 4.0960000000000023e-07     evaluation reward: 9.81\n",
      "episode: 2671   score: 7.0   memory length: 709399   epsilon: 0.009998020008555413    steps: 388    lr: 4.0960000000000023e-07     evaluation reward: 9.78\n",
      "episode: 2672   score: 11.0   memory length: 709992   epsilon: 0.009998020008555413    steps: 593    lr: 4.0960000000000023e-07     evaluation reward: 9.78\n",
      "episode: 2673   score: 14.0   memory length: 710517   epsilon: 0.009998020008555413    steps: 525    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2674   score: 13.0   memory length: 711026   epsilon: 0.009998020008555413    steps: 509    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2675   score: 10.0   memory length: 711482   epsilon: 0.009998020008555413    steps: 456    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2676   score: 9.0   memory length: 711947   epsilon: 0.009998020008555413    steps: 465    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2677   score: 11.0   memory length: 712490   epsilon: 0.009998020008555413    steps: 543    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2678   score: 10.0   memory length: 712975   epsilon: 0.009998020008555413    steps: 485    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2679   score: 13.0   memory length: 713455   epsilon: 0.009998020008555413    steps: 480    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2680   score: 9.0   memory length: 713930   epsilon: 0.009998020008555413    steps: 475    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2681   score: 8.0   memory length: 714349   epsilon: 0.009998020008555413    steps: 419    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2682   score: 16.0   memory length: 714978   epsilon: 0.009998020008555413    steps: 629    lr: 4.0960000000000023e-07     evaluation reward: 10.05\n",
      "episode: 2683   score: 13.0   memory length: 715647   epsilon: 0.009998020008555413    steps: 669    lr: 4.0960000000000023e-07     evaluation reward: 10.05\n",
      "episode: 2684   score: 13.0   memory length: 716116   epsilon: 0.009998020008555413    steps: 469    lr: 4.0960000000000023e-07     evaluation reward: 10.05\n",
      "episode: 2685   score: 6.0   memory length: 716491   epsilon: 0.009998020008555413    steps: 375    lr: 4.0960000000000023e-07     evaluation reward: 9.99\n",
      "episode: 2686   score: 6.0   memory length: 716832   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2687   score: 10.0   memory length: 717349   epsilon: 0.009998020008555413    steps: 517    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2688   score: 7.0   memory length: 717760   epsilon: 0.009998020008555413    steps: 411    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2689   score: 10.0   memory length: 718313   epsilon: 0.009998020008555413    steps: 553    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2690   score: 7.0   memory length: 718704   epsilon: 0.009998020008555413    steps: 391    lr: 4.0960000000000023e-07     evaluation reward: 9.95\n",
      "episode: 2691   score: 5.0   memory length: 719000   epsilon: 0.009998020008555413    steps: 296    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2692   score: 14.0   memory length: 719484   epsilon: 0.009998020008555413    steps: 484    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2693   score: 8.0   memory length: 719938   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2694   score: 10.0   memory length: 720454   epsilon: 0.009998020008555413    steps: 516    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2695   score: 9.0   memory length: 720901   epsilon: 0.009998020008555413    steps: 447    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2696   score: 9.0   memory length: 721343   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.89\n",
      "episode: 2697   score: 6.0   memory length: 721684   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2698   score: 12.0   memory length: 722260   epsilon: 0.009998020008555413    steps: 576    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2699   score: 12.0   memory length: 722854   epsilon: 0.009998020008555413    steps: 594    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2700   score: 12.0   memory length: 723433   epsilon: 0.009998020008555413    steps: 579    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2701   score: 6.0   memory length: 723774   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2702   score: 9.0   memory length: 724278   epsilon: 0.009998020008555413    steps: 504    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2703   score: 10.0   memory length: 724675   epsilon: 0.009998020008555413    steps: 397    lr: 4.0960000000000023e-07     evaluation reward: 9.95\n",
      "episode: 2704   score: 11.0   memory length: 725266   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2705   score: 11.0   memory length: 725787   epsilon: 0.009998020008555413    steps: 521    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2706   score: 8.0   memory length: 726065   epsilon: 0.009998020008555413    steps: 278    lr: 4.0960000000000023e-07     evaluation reward: 9.96\n",
      "episode: 2707   score: 11.0   memory length: 726667   epsilon: 0.009998020008555413    steps: 602    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2708   score: 7.0   memory length: 727013   epsilon: 0.009998020008555413    steps: 346    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2709   score: 5.0   memory length: 727303   epsilon: 0.009998020008555413    steps: 290    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2710   score: 12.0   memory length: 727726   epsilon: 0.009998020008555413    steps: 423    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2711   score: 10.0   memory length: 728273   epsilon: 0.009998020008555413    steps: 547    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2712   score: 10.0   memory length: 728598   epsilon: 0.009998020008555413    steps: 325    lr: 4.0960000000000023e-07     evaluation reward: 9.89\n",
      "episode: 2713   score: 9.0   memory length: 729119   epsilon: 0.009998020008555413    steps: 521    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2714   score: 10.0   memory length: 729629   epsilon: 0.009998020008555413    steps: 510    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2715   score: 16.0   memory length: 730127   epsilon: 0.009998020008555413    steps: 498    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2716   score: 7.0   memory length: 730520   epsilon: 0.009998020008555413    steps: 393    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2717   score: 9.0   memory length: 730987   epsilon: 0.009998020008555413    steps: 467    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2718   score: 10.0   memory length: 731483   epsilon: 0.009998020008555413    steps: 496    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2719   score: 9.0   memory length: 731932   epsilon: 0.009998020008555413    steps: 449    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2720   score: 9.0   memory length: 732423   epsilon: 0.009998020008555413    steps: 491    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2721   score: 11.0   memory length: 732976   epsilon: 0.009998020008555413    steps: 553    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2722   score: 11.0   memory length: 733373   epsilon: 0.009998020008555413    steps: 397    lr: 4.0960000000000023e-07     evaluation reward: 9.89\n",
      "episode: 2723   score: 11.0   memory length: 733892   epsilon: 0.009998020008555413    steps: 519    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2724   score: 8.0   memory length: 734340   epsilon: 0.009998020008555413    steps: 448    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2725   score: 9.0   memory length: 734856   epsilon: 0.009998020008555413    steps: 516    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2726   score: 9.0   memory length: 735350   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2727   score: 10.0   memory length: 735850   epsilon: 0.009998020008555413    steps: 500    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2728   score: 19.0   memory length: 736480   epsilon: 0.009998020008555413    steps: 630    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2729   score: 7.0   memory length: 736891   epsilon: 0.009998020008555413    steps: 411    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2730   score: 13.0   memory length: 737513   epsilon: 0.009998020008555413    steps: 622    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2731   score: 10.0   memory length: 738067   epsilon: 0.009998020008555413    steps: 554    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2732   score: 8.0   memory length: 738487   epsilon: 0.009998020008555413    steps: 420    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2733   score: 14.0   memory length: 739196   epsilon: 0.009998020008555413    steps: 709    lr: 4.0960000000000023e-07     evaluation reward: 10.01\n",
      "episode: 2734   score: 10.0   memory length: 739696   epsilon: 0.009998020008555413    steps: 500    lr: 4.0960000000000023e-07     evaluation reward: 10.03\n",
      "episode: 2735   score: 10.0   memory length: 740238   epsilon: 0.009998020008555413    steps: 542    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2736   score: 8.0   memory length: 740676   epsilon: 0.009998020008555413    steps: 438    lr: 4.0960000000000023e-07     evaluation reward: 9.95\n",
      "episode: 2737   score: 12.0   memory length: 741219   epsilon: 0.009998020008555413    steps: 543    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2738   score: 8.0   memory length: 741661   epsilon: 0.009998020008555413    steps: 442    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2739   score: 7.0   memory length: 742063   epsilon: 0.009998020008555413    steps: 402    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2740   score: 15.0   memory length: 742613   epsilon: 0.009998020008555413    steps: 550    lr: 4.0960000000000023e-07     evaluation reward: 9.92\n",
      "episode: 2741   score: 13.0   memory length: 743237   epsilon: 0.009998020008555413    steps: 624    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2742   score: 10.0   memory length: 743761   epsilon: 0.009998020008555413    steps: 524    lr: 4.0960000000000023e-07     evaluation reward: 9.99\n",
      "episode: 2743   score: 15.0   memory length: 744522   epsilon: 0.009998020008555413    steps: 761    lr: 4.0960000000000023e-07     evaluation reward: 10.04\n",
      "episode: 2744   score: 6.0   memory length: 744863   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2745   score: 12.0   memory length: 745284   epsilon: 0.009998020008555413    steps: 421    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2746   score: 11.0   memory length: 745845   epsilon: 0.009998020008555413    steps: 561    lr: 4.0960000000000023e-07     evaluation reward: 10.04\n",
      "episode: 2747   score: 15.0   memory length: 746571   epsilon: 0.009998020008555413    steps: 726    lr: 4.0960000000000023e-07     evaluation reward: 10.11\n",
      "episode: 2748   score: 12.0   memory length: 747139   epsilon: 0.009998020008555413    steps: 568    lr: 4.0960000000000023e-07     evaluation reward: 10.13\n",
      "episode: 2749   score: 11.0   memory length: 747700   epsilon: 0.009998020008555413    steps: 561    lr: 4.0960000000000023e-07     evaluation reward: 10.15\n",
      "episode: 2750   score: 10.0   memory length: 748184   epsilon: 0.009998020008555413    steps: 484    lr: 4.0960000000000023e-07     evaluation reward: 10.14\n",
      "episode: 2751   score: 10.0   memory length: 748689   epsilon: 0.009998020008555413    steps: 505    lr: 4.0960000000000023e-07     evaluation reward: 10.13\n",
      "episode: 2752   score: 12.0   memory length: 749155   epsilon: 0.009998020008555413    steps: 466    lr: 4.0960000000000023e-07     evaluation reward: 10.17\n",
      "episode: 2753   score: 13.0   memory length: 749656   epsilon: 0.009998020008555413    steps: 501    lr: 4.0960000000000023e-07     evaluation reward: 10.24\n",
      "episode: 2754   score: 11.0   memory length: 750199   epsilon: 0.009998020008555413    steps: 543    lr: 4.0960000000000023e-07     evaluation reward: 10.25\n",
      "episode: 2755   score: 7.0   memory length: 750568   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 10.2\n",
      "episode: 2756   score: 13.0   memory length: 751167   epsilon: 0.009998020008555413    steps: 599    lr: 4.0960000000000023e-07     evaluation reward: 10.21\n",
      "episode: 2757   score: 7.0   memory length: 751536   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 10.16\n",
      "episode: 2758   score: 8.0   memory length: 751990   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 10.17\n",
      "episode: 2759   score: 7.0   memory length: 752374   epsilon: 0.009998020008555413    steps: 384    lr: 4.0960000000000023e-07     evaluation reward: 10.11\n",
      "episode: 2760   score: 10.0   memory length: 752871   epsilon: 0.009998020008555413    steps: 497    lr: 4.0960000000000023e-07     evaluation reward: 10.13\n",
      "episode: 2761   score: 6.0   memory length: 753247   epsilon: 0.009998020008555413    steps: 376    lr: 4.0960000000000023e-07     evaluation reward: 10.11\n",
      "episode: 2762   score: 6.0   memory length: 753622   epsilon: 0.009998020008555413    steps: 375    lr: 4.0960000000000023e-07     evaluation reward: 10.05\n",
      "episode: 2763   score: 7.0   memory length: 753991   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2764   score: 12.0   memory length: 754517   epsilon: 0.009998020008555413    steps: 526    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2765   score: 8.0   memory length: 754812   epsilon: 0.009998020008555413    steps: 295    lr: 4.0960000000000023e-07     evaluation reward: 10.02\n",
      "episode: 2766   score: 10.0   memory length: 755317   epsilon: 0.009998020008555413    steps: 505    lr: 4.0960000000000023e-07     evaluation reward: 9.97\n",
      "episode: 2767   score: 8.0   memory length: 755717   epsilon: 0.009998020008555413    steps: 400    lr: 4.0960000000000023e-07     evaluation reward: 9.98\n",
      "episode: 2768   score: 11.0   memory length: 756269   epsilon: 0.009998020008555413    steps: 552    lr: 4.0960000000000023e-07     evaluation reward: 10.01\n",
      "episode: 2769   score: 8.0   memory length: 756744   epsilon: 0.009998020008555413    steps: 475    lr: 4.0960000000000023e-07     evaluation reward: 9.94\n",
      "episode: 2770   score: 12.0   memory length: 757304   epsilon: 0.009998020008555413    steps: 560    lr: 4.0960000000000023e-07     evaluation reward: 10.0\n",
      "episode: 2771   score: 10.0   memory length: 757818   epsilon: 0.009998020008555413    steps: 514    lr: 4.0960000000000023e-07     evaluation reward: 10.03\n",
      "episode: 2772   score: 11.0   memory length: 758418   epsilon: 0.009998020008555413    steps: 600    lr: 4.0960000000000023e-07     evaluation reward: 10.03\n",
      "episode: 2773   score: 10.0   memory length: 758967   epsilon: 0.009998020008555413    steps: 549    lr: 4.0960000000000023e-07     evaluation reward: 9.99\n",
      "episode: 2774   score: 7.0   memory length: 759357   epsilon: 0.009998020008555413    steps: 390    lr: 4.0960000000000023e-07     evaluation reward: 9.93\n",
      "episode: 2775   score: 6.0   memory length: 759698   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.89\n",
      "episode: 2776   score: 11.0   memory length: 760257   epsilon: 0.009998020008555413    steps: 559    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2777   score: 7.0   memory length: 760648   epsilon: 0.009998020008555413    steps: 391    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2778   score: 11.0   memory length: 761239   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.88\n",
      "episode: 2779   score: 10.0   memory length: 761781   epsilon: 0.009998020008555413    steps: 542    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2780   score: 14.0   memory length: 762316   epsilon: 0.009998020008555413    steps: 535    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2781   score: 9.0   memory length: 762761   epsilon: 0.009998020008555413    steps: 445    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2782   score: 5.0   memory length: 763051   epsilon: 0.009998020008555413    steps: 290    lr: 4.0960000000000023e-07     evaluation reward: 9.8\n",
      "episode: 2783   score: 15.0   memory length: 763618   epsilon: 0.009998020008555413    steps: 567    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2784   score: 12.0   memory length: 764216   epsilon: 0.009998020008555413    steps: 598    lr: 4.0960000000000023e-07     evaluation reward: 9.81\n",
      "episode: 2785   score: 11.0   memory length: 764807   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.86\n",
      "episode: 2786   score: 11.0   memory length: 765363   epsilon: 0.009998020008555413    steps: 556    lr: 4.0960000000000023e-07     evaluation reward: 9.91\n",
      "episode: 2787   score: 6.0   memory length: 765704   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2788   score: 7.0   memory length: 766073   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2789   score: 7.0   memory length: 766479   epsilon: 0.009998020008555413    steps: 406    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2790   score: 7.0   memory length: 766853   epsilon: 0.009998020008555413    steps: 374    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2791   score: 6.0   memory length: 767194   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2792   score: 10.0   memory length: 767730   epsilon: 0.009998020008555413    steps: 536    lr: 4.0960000000000023e-07     evaluation reward: 9.81\n",
      "episode: 2793   score: 12.0   memory length: 768360   epsilon: 0.009998020008555413    steps: 630    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2794   score: 7.0   memory length: 768731   epsilon: 0.009998020008555413    steps: 371    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2795   score: 7.0   memory length: 769100   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.8\n",
      "episode: 2796   score: 13.0   memory length: 769550   epsilon: 0.009998020008555413    steps: 450    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2797   score: 12.0   memory length: 769996   epsilon: 0.009998020008555413    steps: 446    lr: 4.0960000000000023e-07     evaluation reward: 9.9\n",
      "episode: 2798   score: 8.0   memory length: 770440   epsilon: 0.009998020008555413    steps: 444    lr: 4.0960000000000023e-07     evaluation reward: 9.86\n",
      "episode: 2799   score: 8.0   memory length: 770879   epsilon: 0.009998020008555413    steps: 439    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2800   score: 12.0   memory length: 771464   epsilon: 0.009998020008555413    steps: 585    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2801   score: 13.0   memory length: 771900   epsilon: 0.009998020008555413    steps: 436    lr: 4.0960000000000023e-07     evaluation reward: 9.89\n",
      "episode: 2802   score: 7.0   memory length: 772269   epsilon: 0.009998020008555413    steps: 369    lr: 4.0960000000000023e-07     evaluation reward: 9.87\n",
      "episode: 2803   score: 7.0   memory length: 772641   epsilon: 0.009998020008555413    steps: 372    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2804   score: 11.0   memory length: 773203   epsilon: 0.009998020008555413    steps: 562    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2805   score: 11.0   memory length: 773794   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2806   score: 7.0   memory length: 774148   epsilon: 0.009998020008555413    steps: 354    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2807   score: 9.0   memory length: 774587   epsilon: 0.009998020008555413    steps: 439    lr: 4.0960000000000023e-07     evaluation reward: 9.81\n",
      "episode: 2808   score: 10.0   memory length: 775101   epsilon: 0.009998020008555413    steps: 514    lr: 4.0960000000000023e-07     evaluation reward: 9.84\n",
      "episode: 2809   score: 6.0   memory length: 775442   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2810   score: 10.0   memory length: 775925   epsilon: 0.009998020008555413    steps: 483    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2811   score: 6.0   memory length: 776266   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2812   score: 6.0   memory length: 776605   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2813   score: 9.0   memory length: 777073   epsilon: 0.009998020008555413    steps: 468    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2814   score: 6.0   memory length: 777414   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.71\n",
      "episode: 2815   score: 10.0   memory length: 777926   epsilon: 0.009998020008555413    steps: 512    lr: 4.0960000000000023e-07     evaluation reward: 9.65\n",
      "episode: 2816   score: 10.0   memory length: 778445   epsilon: 0.009998020008555413    steps: 519    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2817   score: 20.0   memory length: 779085   epsilon: 0.009998020008555413    steps: 640    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2818   score: 10.0   memory length: 779607   epsilon: 0.009998020008555413    steps: 522    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2819   score: 9.0   memory length: 780078   epsilon: 0.009998020008555413    steps: 471    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2820   score: 13.0   memory length: 780712   epsilon: 0.009998020008555413    steps: 634    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2821   score: 13.0   memory length: 781287   epsilon: 0.009998020008555413    steps: 575    lr: 4.0960000000000023e-07     evaluation reward: 9.85\n",
      "episode: 2822   score: 9.0   memory length: 781745   epsilon: 0.009998020008555413    steps: 458    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2823   score: 9.0   memory length: 782210   epsilon: 0.009998020008555413    steps: 465    lr: 4.0960000000000023e-07     evaluation reward: 9.81\n",
      "episode: 2824   score: 9.0   memory length: 782681   epsilon: 0.009998020008555413    steps: 471    lr: 4.0960000000000023e-07     evaluation reward: 9.82\n",
      "episode: 2825   score: 6.0   memory length: 783045   epsilon: 0.009998020008555413    steps: 364    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2826   score: 10.0   memory length: 783557   epsilon: 0.009998020008555413    steps: 512    lr: 4.0960000000000023e-07     evaluation reward: 9.8\n",
      "episode: 2827   score: 13.0   memory length: 784072   epsilon: 0.009998020008555413    steps: 515    lr: 4.0960000000000023e-07     evaluation reward: 9.83\n",
      "episode: 2828   score: 13.0   memory length: 784742   epsilon: 0.009998020008555413    steps: 670    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2829   score: 7.0   memory length: 785116   epsilon: 0.009998020008555413    steps: 374    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2830   score: 11.0   memory length: 785707   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.75\n",
      "episode: 2831   score: 12.0   memory length: 786138   epsilon: 0.009998020008555413    steps: 431    lr: 4.0960000000000023e-07     evaluation reward: 9.77\n",
      "episode: 2832   score: 10.0   memory length: 786680   epsilon: 0.009998020008555413    steps: 542    lr: 4.0960000000000023e-07     evaluation reward: 9.79\n",
      "episode: 2833   score: 6.0   memory length: 787021   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.71\n",
      "episode: 2834   score: 6.0   memory length: 787362   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2835   score: 6.0   memory length: 787703   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.63\n",
      "episode: 2836   score: 12.0   memory length: 788270   epsilon: 0.009998020008555413    steps: 567    lr: 4.0960000000000023e-07     evaluation reward: 9.67\n",
      "episode: 2837   score: 6.0   memory length: 788611   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.61\n",
      "episode: 2838   score: 8.0   memory length: 789029   epsilon: 0.009998020008555413    steps: 418    lr: 4.0960000000000023e-07     evaluation reward: 9.61\n",
      "episode: 2839   score: 11.0   memory length: 789620   epsilon: 0.009998020008555413    steps: 591    lr: 4.0960000000000023e-07     evaluation reward: 9.65\n",
      "episode: 2840   score: 7.0   memory length: 789979   epsilon: 0.009998020008555413    steps: 359    lr: 4.0960000000000023e-07     evaluation reward: 9.57\n",
      "episode: 2841   score: 16.0   memory length: 790562   epsilon: 0.009998020008555413    steps: 583    lr: 4.0960000000000023e-07     evaluation reward: 9.6\n",
      "episode: 2842   score: 12.0   memory length: 790993   epsilon: 0.009998020008555413    steps: 431    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2843   score: 6.0   memory length: 791354   epsilon: 0.009998020008555413    steps: 361    lr: 4.0960000000000023e-07     evaluation reward: 9.53\n",
      "episode: 2844   score: 13.0   memory length: 791972   epsilon: 0.009998020008555413    steps: 618    lr: 4.0960000000000023e-07     evaluation reward: 9.6\n",
      "episode: 2845   score: 12.0   memory length: 792440   epsilon: 0.009998020008555413    steps: 468    lr: 4.0960000000000023e-07     evaluation reward: 9.6\n",
      "episode: 2846   score: 13.0   memory length: 792931   epsilon: 0.009998020008555413    steps: 491    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2847   score: 14.0   memory length: 793460   epsilon: 0.009998020008555413    steps: 529    lr: 4.0960000000000023e-07     evaluation reward: 9.61\n",
      "episode: 2848   score: 13.0   memory length: 793957   epsilon: 0.009998020008555413    steps: 497    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2849   score: 7.0   memory length: 794364   epsilon: 0.009998020008555413    steps: 407    lr: 4.0960000000000023e-07     evaluation reward: 9.58\n",
      "episode: 2850   score: 6.0   memory length: 794709   epsilon: 0.009998020008555413    steps: 345    lr: 4.0960000000000023e-07     evaluation reward: 9.54\n",
      "episode: 2851   score: 12.0   memory length: 795340   epsilon: 0.009998020008555413    steps: 631    lr: 4.0960000000000023e-07     evaluation reward: 9.56\n",
      "episode: 2852   score: 16.0   memory length: 795934   epsilon: 0.009998020008555413    steps: 594    lr: 4.0960000000000023e-07     evaluation reward: 9.6\n",
      "episode: 2853   score: 6.0   memory length: 796275   epsilon: 0.009998020008555413    steps: 341    lr: 4.0960000000000023e-07     evaluation reward: 9.53\n",
      "episode: 2854   score: 14.0   memory length: 796837   epsilon: 0.009998020008555413    steps: 562    lr: 4.0960000000000023e-07     evaluation reward: 9.56\n",
      "episode: 2855   score: 13.0   memory length: 797483   epsilon: 0.009998020008555413    steps: 646    lr: 4.0960000000000023e-07     evaluation reward: 9.62\n",
      "episode: 2856   score: 14.0   memory length: 798152   epsilon: 0.009998020008555413    steps: 669    lr: 4.0960000000000023e-07     evaluation reward: 9.63\n",
      "episode: 2857   score: 14.0   memory length: 798625   epsilon: 0.009998020008555413    steps: 473    lr: 4.0960000000000023e-07     evaluation reward: 9.7\n",
      "episode: 2858   score: 6.0   memory length: 798962   epsilon: 0.009998020008555413    steps: 337    lr: 4.0960000000000023e-07     evaluation reward: 9.68\n",
      "episode: 2859   score: 15.0   memory length: 799614   epsilon: 0.009998020008555413    steps: 652    lr: 4.0960000000000023e-07     evaluation reward: 9.76\n",
      "episode: 2860   score: 14.0   memory length: 800214   epsilon: 0.009998020008555413    steps: 600    lr: 1.638400000000001e-07     evaluation reward: 9.8\n",
      "episode: 2861   score: 7.0   memory length: 800588   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.81\n",
      "episode: 2862   score: 18.0   memory length: 801255   epsilon: 0.009998020008555413    steps: 667    lr: 1.638400000000001e-07     evaluation reward: 9.93\n",
      "episode: 2863   score: 7.0   memory length: 801629   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.93\n",
      "episode: 2864   score: 14.0   memory length: 802111   epsilon: 0.009998020008555413    steps: 482    lr: 1.638400000000001e-07     evaluation reward: 9.95\n",
      "episode: 2865   score: 7.0   memory length: 802480   epsilon: 0.009998020008555413    steps: 369    lr: 1.638400000000001e-07     evaluation reward: 9.94\n",
      "episode: 2866   score: 7.0   memory length: 802854   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.91\n",
      "episode: 2867   score: 9.0   memory length: 803348   epsilon: 0.009998020008555413    steps: 494    lr: 1.638400000000001e-07     evaluation reward: 9.92\n",
      "episode: 2868   score: 6.0   memory length: 803689   epsilon: 0.009998020008555413    steps: 341    lr: 1.638400000000001e-07     evaluation reward: 9.87\n",
      "episode: 2869   score: 12.0   memory length: 804236   epsilon: 0.009998020008555413    steps: 547    lr: 1.638400000000001e-07     evaluation reward: 9.91\n",
      "episode: 2870   score: 7.0   memory length: 804605   epsilon: 0.009998020008555413    steps: 369    lr: 1.638400000000001e-07     evaluation reward: 9.86\n",
      "episode: 2871   score: 7.0   memory length: 804979   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.83\n",
      "episode: 2872   score: 10.0   memory length: 805491   epsilon: 0.009998020008555413    steps: 512    lr: 1.638400000000001e-07     evaluation reward: 9.82\n",
      "episode: 2873   score: 9.0   memory length: 805981   epsilon: 0.009998020008555413    steps: 490    lr: 1.638400000000001e-07     evaluation reward: 9.81\n",
      "episode: 2874   score: 12.0   memory length: 806547   epsilon: 0.009998020008555413    steps: 566    lr: 1.638400000000001e-07     evaluation reward: 9.86\n",
      "episode: 2875   score: 14.0   memory length: 807084   epsilon: 0.009998020008555413    steps: 537    lr: 1.638400000000001e-07     evaluation reward: 9.94\n",
      "episode: 2876   score: 10.0   memory length: 807603   epsilon: 0.009998020008555413    steps: 519    lr: 1.638400000000001e-07     evaluation reward: 9.93\n",
      "episode: 2877   score: 10.0   memory length: 808115   epsilon: 0.009998020008555413    steps: 512    lr: 1.638400000000001e-07     evaluation reward: 9.96\n",
      "episode: 2878   score: 12.0   memory length: 808583   epsilon: 0.009998020008555413    steps: 468    lr: 1.638400000000001e-07     evaluation reward: 9.97\n",
      "episode: 2879   score: 14.0   memory length: 809260   epsilon: 0.009998020008555413    steps: 677    lr: 1.638400000000001e-07     evaluation reward: 10.01\n",
      "episode: 2880   score: 13.0   memory length: 809835   epsilon: 0.009998020008555413    steps: 575    lr: 1.638400000000001e-07     evaluation reward: 10.0\n",
      "episode: 2881   score: 7.0   memory length: 810209   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.98\n",
      "episode: 2882   score: 6.0   memory length: 810585   epsilon: 0.009998020008555413    steps: 376    lr: 1.638400000000001e-07     evaluation reward: 9.99\n",
      "episode: 2883   score: 10.0   memory length: 811101   epsilon: 0.009998020008555413    steps: 516    lr: 1.638400000000001e-07     evaluation reward: 9.94\n",
      "episode: 2884   score: 10.0   memory length: 811549   epsilon: 0.009998020008555413    steps: 448    lr: 1.638400000000001e-07     evaluation reward: 9.92\n",
      "episode: 2885   score: 8.0   memory length: 812002   epsilon: 0.009998020008555413    steps: 453    lr: 1.638400000000001e-07     evaluation reward: 9.89\n",
      "episode: 2886   score: 7.0   memory length: 812376   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 9.85\n",
      "episode: 2887   score: 11.0   memory length: 812946   epsilon: 0.009998020008555413    steps: 570    lr: 1.638400000000001e-07     evaluation reward: 9.9\n",
      "episode: 2888   score: 9.0   memory length: 813443   epsilon: 0.009998020008555413    steps: 497    lr: 1.638400000000001e-07     evaluation reward: 9.92\n",
      "episode: 2889   score: 9.0   memory length: 813868   epsilon: 0.009998020008555413    steps: 425    lr: 1.638400000000001e-07     evaluation reward: 9.94\n",
      "episode: 2890   score: 8.0   memory length: 814320   epsilon: 0.009998020008555413    steps: 452    lr: 1.638400000000001e-07     evaluation reward: 9.95\n",
      "episode: 2891   score: 12.0   memory length: 814788   epsilon: 0.009998020008555413    steps: 468    lr: 1.638400000000001e-07     evaluation reward: 10.01\n",
      "episode: 2892   score: 11.0   memory length: 815162   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 10.02\n",
      "episode: 2893   score: 10.0   memory length: 815697   epsilon: 0.009998020008555413    steps: 535    lr: 1.638400000000001e-07     evaluation reward: 10.0\n",
      "episode: 2894   score: 13.0   memory length: 816318   epsilon: 0.009998020008555413    steps: 621    lr: 1.638400000000001e-07     evaluation reward: 10.06\n",
      "episode: 2895   score: 7.0   memory length: 816687   epsilon: 0.009998020008555413    steps: 369    lr: 1.638400000000001e-07     evaluation reward: 10.06\n",
      "episode: 2896   score: 11.0   memory length: 817278   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.04\n",
      "episode: 2897   score: 11.0   memory length: 817869   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.03\n",
      "episode: 2898   score: 20.0   memory length: 818431   epsilon: 0.009998020008555413    steps: 562    lr: 1.638400000000001e-07     evaluation reward: 10.15\n",
      "episode: 2899   score: 7.0   memory length: 818805   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 10.14\n",
      "episode: 2900   score: 11.0   memory length: 819396   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.13\n",
      "episode: 2901   score: 17.0   memory length: 820017   epsilon: 0.009998020008555413    steps: 621    lr: 1.638400000000001e-07     evaluation reward: 10.17\n",
      "episode: 2902   score: 15.0   memory length: 820541   epsilon: 0.009998020008555413    steps: 524    lr: 1.638400000000001e-07     evaluation reward: 10.25\n",
      "episode: 2903   score: 13.0   memory length: 821118   epsilon: 0.009998020008555413    steps: 577    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2904   score: 8.0   memory length: 821396   epsilon: 0.009998020008555413    steps: 278    lr: 1.638400000000001e-07     evaluation reward: 10.28\n",
      "episode: 2905   score: 13.0   memory length: 822067   epsilon: 0.009998020008555413    steps: 671    lr: 1.638400000000001e-07     evaluation reward: 10.3\n",
      "episode: 2906   score: 12.0   memory length: 822665   epsilon: 0.009998020008555413    steps: 598    lr: 1.638400000000001e-07     evaluation reward: 10.35\n",
      "episode: 2907   score: 5.0   memory length: 822961   epsilon: 0.009998020008555413    steps: 296    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2908   score: 11.0   memory length: 823457   epsilon: 0.009998020008555413    steps: 496    lr: 1.638400000000001e-07     evaluation reward: 10.32\n",
      "episode: 2909   score: 7.0   memory length: 823831   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 10.33\n",
      "episode: 2910   score: 6.0   memory length: 824176   epsilon: 0.009998020008555413    steps: 345    lr: 1.638400000000001e-07     evaluation reward: 10.29\n",
      "episode: 2911   score: 7.0   memory length: 824550   epsilon: 0.009998020008555413    steps: 374    lr: 1.638400000000001e-07     evaluation reward: 10.3\n",
      "episode: 2912   score: 15.0   memory length: 825293   epsilon: 0.009998020008555413    steps: 743    lr: 1.638400000000001e-07     evaluation reward: 10.39\n",
      "episode: 2913   score: 13.0   memory length: 825914   epsilon: 0.009998020008555413    steps: 621    lr: 1.638400000000001e-07     evaluation reward: 10.43\n",
      "episode: 2914   score: 6.0   memory length: 826259   epsilon: 0.009998020008555413    steps: 345    lr: 1.638400000000001e-07     evaluation reward: 10.43\n",
      "episode: 2915   score: 9.0   memory length: 826765   epsilon: 0.009998020008555413    steps: 506    lr: 1.638400000000001e-07     evaluation reward: 10.42\n",
      "episode: 2916   score: 12.0   memory length: 827304   epsilon: 0.009998020008555413    steps: 539    lr: 1.638400000000001e-07     evaluation reward: 10.44\n",
      "episode: 2917   score: 6.0   memory length: 827649   epsilon: 0.009998020008555413    steps: 345    lr: 1.638400000000001e-07     evaluation reward: 10.3\n",
      "episode: 2918   score: 13.0   memory length: 828286   epsilon: 0.009998020008555413    steps: 637    lr: 1.638400000000001e-07     evaluation reward: 10.33\n",
      "episode: 2919   score: 7.0   memory length: 828663   epsilon: 0.009998020008555413    steps: 377    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2920   score: 17.0   memory length: 829373   epsilon: 0.009998020008555413    steps: 710    lr: 1.638400000000001e-07     evaluation reward: 10.35\n",
      "episode: 2921   score: 13.0   memory length: 829948   epsilon: 0.009998020008555413    steps: 575    lr: 1.638400000000001e-07     evaluation reward: 10.35\n",
      "episode: 2922   score: 6.0   memory length: 830293   epsilon: 0.009998020008555413    steps: 345    lr: 1.638400000000001e-07     evaluation reward: 10.32\n",
      "episode: 2923   score: 6.0   memory length: 830638   epsilon: 0.009998020008555413    steps: 345    lr: 1.638400000000001e-07     evaluation reward: 10.29\n",
      "episode: 2924   score: 14.0   memory length: 831379   epsilon: 0.009998020008555413    steps: 741    lr: 1.638400000000001e-07     evaluation reward: 10.34\n",
      "episode: 2925   score: 11.0   memory length: 831970   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.39\n",
      "episode: 2926   score: 5.0   memory length: 832298   epsilon: 0.009998020008555413    steps: 328    lr: 1.638400000000001e-07     evaluation reward: 10.34\n",
      "episode: 2927   score: 10.0   memory length: 832810   epsilon: 0.009998020008555413    steps: 512    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2928   score: 11.0   memory length: 833328   epsilon: 0.009998020008555413    steps: 518    lr: 1.638400000000001e-07     evaluation reward: 10.29\n",
      "episode: 2929   score: 9.0   memory length: 833814   epsilon: 0.009998020008555413    steps: 486    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2930   score: 11.0   memory length: 834405   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2931   score: 16.0   memory length: 835192   epsilon: 0.009998020008555413    steps: 787    lr: 1.638400000000001e-07     evaluation reward: 10.35\n",
      "episode: 2932   score: 6.0   memory length: 835539   epsilon: 0.009998020008555413    steps: 347    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2933   score: 6.0   memory length: 835880   epsilon: 0.009998020008555413    steps: 341    lr: 1.638400000000001e-07     evaluation reward: 10.31\n",
      "episode: 2934   score: 13.0   memory length: 836340   epsilon: 0.009998020008555413    steps: 460    lr: 1.638400000000001e-07     evaluation reward: 10.38\n",
      "episode: 2935   score: 11.0   memory length: 836931   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.43\n",
      "episode: 2936   score: 7.0   memory length: 837300   epsilon: 0.009998020008555413    steps: 369    lr: 1.638400000000001e-07     evaluation reward: 10.38\n",
      "episode: 2937   score: 14.0   memory length: 838010   epsilon: 0.009998020008555413    steps: 710    lr: 1.638400000000001e-07     evaluation reward: 10.46\n",
      "episode: 2938   score: 11.0   memory length: 838601   epsilon: 0.009998020008555413    steps: 591    lr: 1.638400000000001e-07     evaluation reward: 10.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Start training after random sample generation\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(frame \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_frame): \u001b[38;5;66;03m# You can set train_frame to a lower value while testing your starts training earlier\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Update the target network only for Double DQN only\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m double_dqn \u001b[38;5;129;01mand\u001b[39;00m (frame \u001b[38;5;241m%\u001b[39m update_target_network_frequency)\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/private/UIUC-CS444-DL-for-cv/assignment5_materials/agent.py:74\u001b[0m, in \u001b[0;36mAgent.train_policy_net\u001b[0;34m(self, frame, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay\n\u001b[0;32m---> 74\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_mini_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mini_batch, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m     77\u001b[0m history \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(mini_batch[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/private/UIUC-CS444-DL-for-cv/assignment5_materials/memory.py:28\u001b[0m, in \u001b[0;36mReplayMemory.sample_mini_batch\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     26\u001b[0m sample \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(HISTORY_SIZE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sample, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m     31\u001b[0m mini_batch\u001b[38;5;241m.\u001b[39mappend((np\u001b[38;5;241m.\u001b[39mstack(sample[:, \u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], sample[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255., device)\n",
    "        state = next_state\n",
    "        next_state, reward, done, truncated, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "        # history = torch.from_numpy(history).to(device)\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame, device)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import * \n",
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "# agent.policy_net = torch.load(\"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials/save_model/breakout_dqn.pth\", map_location=device)\n",
    "\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 1   score: 8.0   memory length: 560   epsilon: 1.0    steps: 437    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 2   score: 3.0   memory length: 786   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 3.6666666666666665\n",
      "episode: 3   score: 2.0   memory length: 984   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 4   score: 0.0   memory length: 1107   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 5   score: 3.0   memory length: 1374   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 2.6666666666666665\n",
      "episode: 6   score: 1.0   memory length: 1543   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 2.4285714285714284\n",
      "episode: 7   score: 1.0   memory length: 1694   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 8   score: 2.0   memory length: 1912   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 2.2222222222222223\n",
      "episode: 9   score: 1.0   memory length: 2080   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 10   score: 1.0   memory length: 2231   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 11   score: 5.0   memory length: 2555   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 12   score: 0.0   memory length: 2677   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 2.076923076923077\n",
      "episode: 13   score: 0.0   memory length: 2800   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.9285714285714286\n",
      "episode: 14   score: 0.0   memory length: 2922   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 15   score: 2.0   memory length: 3119   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8125\n",
      "episode: 16   score: 2.0   memory length: 3319   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.8235294117647058\n",
      "episode: 17   score: 1.0   memory length: 3490   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 18   score: 0.0   memory length: 3612   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
      "episode: 19   score: 0.0   memory length: 3735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 20   score: 2.0   memory length: 3933   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 21   score: 2.0   memory length: 4134   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 22   score: 0.0   memory length: 4257   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.565217391304348\n",
      "episode: 23   score: 1.0   memory length: 4426   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5416666666666667\n",
      "episode: 24   score: 4.0   memory length: 4703   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 25   score: 0.0   memory length: 4826   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5769230769230769\n",
      "episode: 26   score: 0.0   memory length: 4948   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5185185185185186\n",
      "episode: 27   score: 2.0   memory length: 5150   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.5357142857142858\n",
      "episode: 28   score: 6.0   memory length: 5400   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6896551724137931\n",
      "episode: 29   score: 0.0   memory length: 5523   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
      "episode: 30   score: 1.0   memory length: 5674   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6129032258064515\n",
      "episode: 31   score: 1.0   memory length: 5825   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59375\n",
      "episode: 32   score: 1.0   memory length: 5976   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5757575757575757\n",
      "episode: 33   score: 2.0   memory length: 6196   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 34   score: 0.0   memory length: 6319   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.542857142857143\n",
      "episode: 35   score: 0.0   memory length: 6442   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 36   score: 2.0   memory length: 6660   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5135135135135136\n",
      "episode: 37   score: 0.0   memory length: 6783   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4736842105263157\n",
      "episode: 38   score: 5.0   memory length: 7125   epsilon: 1.0    steps: 342    lr: 0.0001     evaluation reward: 1.564102564102564\n",
      "episode: 39   score: 0.0   memory length: 7247   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.525\n",
      "episode: 40   score: 0.0   memory length: 7369   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4878048780487805\n",
      "episode: 41   score: 1.0   memory length: 7537   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4761904761904763\n",
      "episode: 42   score: 2.0   memory length: 7735   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4883720930232558\n",
      "episode: 43   score: 1.0   memory length: 7904   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4772727272727273\n",
      "episode: 44   score: 1.0   memory length: 8073   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4666666666666666\n",
      "episode: 45   score: 3.0   memory length: 8319   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 46   score: 7.0   memory length: 8618   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.6170212765957446\n",
      "episode: 47   score: 2.0   memory length: 8834   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 48   score: 2.0   memory length: 9052   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6326530612244898\n",
      "episode: 49   score: 1.0   memory length: 9203   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 50   score: 0.0   memory length: 9326   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 51   score: 0.0   memory length: 9448   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5576923076923077\n",
      "episode: 52   score: 1.0   memory length: 9618   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5471698113207548\n",
      "episode: 53   score: 1.0   memory length: 9788   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.537037037037037\n",
      "episode: 54   score: 3.0   memory length: 10018   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.5636363636363637\n",
      "episode: 55   score: 0.0   memory length: 10140   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5357142857142858\n",
      "episode: 56   score: 2.0   memory length: 10357   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.543859649122807\n",
      "episode: 57   score: 0.0   memory length: 10480   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5172413793103448\n",
      "episode: 58   score: 4.0   memory length: 10738   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.5593220338983051\n",
      "episode: 59   score: 2.0   memory length: 10956   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5666666666666667\n",
      "episode: 60   score: 4.0   memory length: 11250   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.6065573770491803\n",
      "episode: 61   score: 1.0   memory length: 11420   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.596774193548387\n",
      "episode: 62   score: 1.0   memory length: 11590   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5873015873015872\n",
      "episode: 63   score: 2.0   memory length: 11810   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.59375\n",
      "episode: 64   score: 1.0   memory length: 11978   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5846153846153845\n",
      "episode: 65   score: 1.0   memory length: 12149   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.5757575757575757\n",
      "episode: 66   score: 1.0   memory length: 12319   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5671641791044777\n",
      "episode: 67   score: 3.0   memory length: 12544   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 68   score: 2.0   memory length: 12742   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5942028985507246\n",
      "episode: 69   score: 2.0   memory length: 12940   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 70   score: 0.0   memory length: 13062   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5774647887323943\n",
      "episode: 71   score: 7.0   memory length: 13445   epsilon: 1.0    steps: 383    lr: 0.0001     evaluation reward: 1.6527777777777777\n",
      "episode: 72   score: 3.0   memory length: 13692   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6712328767123288\n",
      "episode: 73   score: 1.0   memory length: 13860   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.662162162162162\n",
      "episode: 74   score: 0.0   memory length: 13983   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 75   score: 0.0   memory length: 14106   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.618421052631579\n",
      "episode: 76   score: 3.0   memory length: 14334   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 77   score: 1.0   memory length: 14484   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6282051282051282\n",
      "episode: 78   score: 4.0   memory length: 14735   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.6582278481012658\n",
      "episode: 79   score: 2.0   memory length: 14914   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.6625\n",
      "episode: 80   score: 2.0   memory length: 15130   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 81   score: 2.0   memory length: 15329   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.670731707317073\n",
      "episode: 82   score: 6.0   memory length: 15708   epsilon: 1.0    steps: 379    lr: 0.0001     evaluation reward: 1.7228915662650603\n",
      "episode: 83   score: 0.0   memory length: 15831   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7023809523809523\n",
      "episode: 84   score: 2.0   memory length: 16047   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.7058823529411764\n",
      "episode: 85   score: 0.0   memory length: 16170   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.686046511627907\n",
      "episode: 86   score: 1.0   memory length: 16339   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6781609195402298\n",
      "episode: 87   score: 1.0   memory length: 16490   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6704545454545454\n",
      "episode: 88   score: 0.0   memory length: 16612   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.651685393258427\n",
      "episode: 89   score: 0.0   memory length: 16735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6333333333333333\n",
      "episode: 90   score: 4.0   memory length: 17029   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.6593406593406594\n",
      "episode: 91   score: 3.0   memory length: 17274   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.673913043478261\n",
      "episode: 92   score: 0.0   memory length: 17397   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6559139784946237\n",
      "episode: 93   score: 2.0   memory length: 17594   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6595744680851063\n",
      "episode: 94   score: 1.0   memory length: 17764   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6526315789473685\n",
      "episode: 95   score: 2.0   memory length: 17961   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65625\n",
      "episode: 96   score: 0.0   memory length: 18083   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6391752577319587\n",
      "episode: 97   score: 2.0   memory length: 18280   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 98   score: 2.0   memory length: 18499   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6464646464646464\n",
      "episode: 99   score: 2.0   memory length: 18683   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 100   score: 0.0   memory length: 18805   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 101   score: 0.0   memory length: 18928   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 102   score: 0.0   memory length: 19051   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 103   score: 3.0   memory length: 19317   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 104   score: 2.0   memory length: 19515   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 105   score: 0.0   memory length: 19638   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 106   score: 0.0   memory length: 19761   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 107   score: 2.0   memory length: 19959   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 108   score: 0.0   memory length: 20082   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 109   score: 2.0   memory length: 20280   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 110   score: 3.0   memory length: 20547   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 111   score: 3.0   memory length: 20792   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 112   score: 0.0   memory length: 20915   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 113   score: 3.0   memory length: 21160   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 114   score: 2.0   memory length: 21358   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 115   score: 0.0   memory length: 21481   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 116   score: 0.0   memory length: 21603   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 117   score: 1.0   memory length: 21772   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 118   score: 1.0   memory length: 21941   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 119   score: 1.0   memory length: 22092   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 120   score: 3.0   memory length: 22335   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 121   score: 4.0   memory length: 22607   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 122   score: 2.0   memory length: 22787   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 123   score: 0.0   memory length: 22910   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 124   score: 3.0   memory length: 23136   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 125   score: 2.0   memory length: 23319   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 126   score: 4.0   memory length: 23613   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 127   score: 0.0   memory length: 23736   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 128   score: 1.0   memory length: 23905   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 129   score: 0.0   memory length: 24028   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 130   score: 0.0   memory length: 24150   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 131   score: 1.0   memory length: 24300   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 132   score: 0.0   memory length: 24423   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 133   score: 3.0   memory length: 24648   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 134   score: 0.0   memory length: 24771   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 135   score: 0.0   memory length: 24893   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 136   score: 0.0   memory length: 25015   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 137   score: 0.0   memory length: 25138   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 138   score: 1.0   memory length: 25289   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 139   score: 0.0   memory length: 25412   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 140   score: 1.0   memory length: 25581   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 141   score: 3.0   memory length: 25848   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 142   score: 1.0   memory length: 26019   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 143   score: 0.0   memory length: 26141   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 144   score: 1.0   memory length: 26310   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 145   score: 2.0   memory length: 26511   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 146   score: 2.0   memory length: 26729   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 147   score: 0.0   memory length: 26852   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 148   score: 0.0   memory length: 26975   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 149   score: 2.0   memory length: 27177   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 150   score: 6.0   memory length: 27518   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 151   score: 2.0   memory length: 27716   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 152   score: 0.0   memory length: 27839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 153   score: 0.0   memory length: 27962   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 154   score: 5.0   memory length: 28303   epsilon: 1.0    steps: 341    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 155   score: 2.0   memory length: 28501   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 156   score: 2.0   memory length: 28717   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 157   score: 1.0   memory length: 28887   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 158   score: 0.0   memory length: 29010   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 159   score: 0.0   memory length: 29133   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 160   score: 2.0   memory length: 29331   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 161   score: 2.0   memory length: 29531   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 162   score: 0.0   memory length: 29654   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 163   score: 3.0   memory length: 29900   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 164   score: 2.0   memory length: 30082   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 165   score: 1.0   memory length: 30250   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 166   score: 1.0   memory length: 30401   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 167   score: 1.0   memory length: 30573   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 168   score: 2.0   memory length: 30791   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 169   score: 0.0   memory length: 30913   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 170   score: 1.0   memory length: 31064   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 171   score: 0.0   memory length: 31187   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 172   score: 2.0   memory length: 31385   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 173   score: 1.0   memory length: 31536   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 174   score: 1.0   memory length: 31687   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 175   score: 2.0   memory length: 31902   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 176   score: 1.0   memory length: 32074   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 177   score: 1.0   memory length: 32246   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 178   score: 5.0   memory length: 32571   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 179   score: 2.0   memory length: 32769   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 180   score: 1.0   memory length: 32940   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 181   score: 2.0   memory length: 33122   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 182   score: 0.0   memory length: 33245   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 183   score: 0.0   memory length: 33368   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 184   score: 4.0   memory length: 33665   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 185   score: 3.0   memory length: 33891   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 186   score: 0.0   memory length: 34014   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 187   score: 3.0   memory length: 34275   epsilon: 1.0    steps: 261    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 188   score: 0.0   memory length: 34398   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 189   score: 0.0   memory length: 34521   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 190   score: 0.0   memory length: 34643   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 191   score: 2.0   memory length: 34861   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 192   score: 2.0   memory length: 35082   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 193   score: 3.0   memory length: 35330   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 194   score: 3.0   memory length: 35577   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 195   score: 1.0   memory length: 35746   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 196   score: 1.0   memory length: 35915   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 197   score: 2.0   memory length: 36113   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 198   score: 0.0   memory length: 36235   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 199   score: 1.0   memory length: 36405   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 200   score: 2.0   memory length: 36602   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 201   score: 1.0   memory length: 36771   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 202   score: 1.0   memory length: 36922   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 203   score: 1.0   memory length: 37072   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 204   score: 4.0   memory length: 37368   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 205   score: 0.0   memory length: 37490   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 206   score: 2.0   memory length: 37688   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 207   score: 1.0   memory length: 37856   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 208   score: 4.0   memory length: 38148   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 209   score: 1.0   memory length: 38319   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 210   score: 1.0   memory length: 38491   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 211   score: 4.0   memory length: 38767   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 212   score: 0.0   memory length: 38890   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 213   score: 2.0   memory length: 39107   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 214   score: 2.0   memory length: 39287   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 215   score: 2.0   memory length: 39485   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 216   score: 1.0   memory length: 39656   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 217   score: 0.0   memory length: 39778   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 218   score: 0.0   memory length: 39901   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 219   score: 1.0   memory length: 40051   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 220   score: 1.0   memory length: 40203   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 221   score: 8.0   memory length: 40543   epsilon: 1.0    steps: 340    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 222   score: 2.0   memory length: 40762   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 223   score: 2.0   memory length: 40978   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 224   score: 2.0   memory length: 41196   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 225   score: 2.0   memory length: 41412   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 226   score: 2.0   memory length: 41630   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 227   score: 2.0   memory length: 41827   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 228   score: 3.0   memory length: 42072   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 229   score: 0.0   memory length: 42195   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 230   score: 0.0   memory length: 42318   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 231   score: 3.0   memory length: 42565   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 232   score: 5.0   memory length: 42910   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 233   score: 1.0   memory length: 43060   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 234   score: 2.0   memory length: 43258   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 235   score: 2.0   memory length: 43457   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 236   score: 0.0   memory length: 43579   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 237   score: 2.0   memory length: 43777   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 238   score: 3.0   memory length: 43988   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 239   score: 3.0   memory length: 44234   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 240   score: 0.0   memory length: 44357   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 241   score: 1.0   memory length: 44509   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 242   score: 0.0   memory length: 44632   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 243   score: 1.0   memory length: 44782   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 244   score: 2.0   memory length: 44981   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 245   score: 2.0   memory length: 45161   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 246   score: 2.0   memory length: 45341   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 247   score: 1.0   memory length: 45492   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 248   score: 2.0   memory length: 45690   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 249   score: 2.0   memory length: 45888   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 250   score: 2.0   memory length: 46105   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 251   score: 3.0   memory length: 46371   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 252   score: 4.0   memory length: 46659   epsilon: 1.0    steps: 288    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 253   score: 2.0   memory length: 46876   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 254   score: 0.0   memory length: 46999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 255   score: 2.0   memory length: 47196   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 256   score: 4.0   memory length: 47473   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 257   score: 3.0   memory length: 47718   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 258   score: 0.0   memory length: 47841   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 259   score: 1.0   memory length: 48009   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 260   score: 1.0   memory length: 48178   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 261   score: 3.0   memory length: 48424   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 262   score: 3.0   memory length: 48672   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 263   score: 0.0   memory length: 48794   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 264   score: 1.0   memory length: 48945   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 265   score: 1.0   memory length: 49114   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 266   score: 1.0   memory length: 49282   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 267   score: 2.0   memory length: 49500   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 268   score: 2.0   memory length: 49698   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 269   score: 2.0   memory length: 49918   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 270   score: 0.0   memory length: 50041   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 271   score: 2.0   memory length: 50242   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 272   score: 2.0   memory length: 50440   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 273   score: 2.0   memory length: 50637   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 274   score: 1.0   memory length: 50805   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 275   score: 0.0   memory length: 50927   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 276   score: 0.0   memory length: 51049   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 277   score: 2.0   memory length: 51246   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 278   score: 6.0   memory length: 51631   epsilon: 1.0    steps: 385    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 279   score: 0.0   memory length: 51754   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 280   score: 2.0   memory length: 51952   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 281   score: 2.0   memory length: 52132   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 282   score: 2.0   memory length: 52329   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 283   score: 4.0   memory length: 52627   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 284   score: 2.0   memory length: 52827   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 285   score: 2.0   memory length: 53025   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 286   score: 2.0   memory length: 53223   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 287   score: 1.0   memory length: 53374   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 288   score: 0.0   memory length: 53497   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 289   score: 1.0   memory length: 53647   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 290   score: 0.0   memory length: 53770   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 291   score: 2.0   memory length: 53968   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 292   score: 0.0   memory length: 54091   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 293   score: 1.0   memory length: 54260   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 294   score: 0.0   memory length: 54382   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 295   score: 0.0   memory length: 54505   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 296   score: 1.0   memory length: 54674   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 297   score: 2.0   memory length: 54893   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 298   score: 0.0   memory length: 55016   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 299   score: 2.0   memory length: 55213   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 300   score: 3.0   memory length: 55462   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 301   score: 0.0   memory length: 55585   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 302   score: 1.0   memory length: 55756   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 303   score: 1.0   memory length: 55925   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 304   score: 1.0   memory length: 56075   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 305   score: 2.0   memory length: 56292   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 306   score: 1.0   memory length: 56461   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 307   score: 0.0   memory length: 56583   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 308   score: 3.0   memory length: 56831   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 309   score: 0.0   memory length: 56954   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 310   score: 0.0   memory length: 57076   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 311   score: 2.0   memory length: 57293   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 312   score: 1.0   memory length: 57444   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 313   score: 3.0   memory length: 57707   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 314   score: 2.0   memory length: 57905   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 315   score: 2.0   memory length: 58123   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 316   score: 0.0   memory length: 58246   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 317   score: 0.0   memory length: 58369   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 318   score: 1.0   memory length: 58519   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 319   score: 1.0   memory length: 58669   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 320   score: 1.0   memory length: 58838   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 321   score: 2.0   memory length: 59036   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 322   score: 0.0   memory length: 59158   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 323   score: 2.0   memory length: 59356   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 324   score: 2.0   memory length: 59554   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 325   score: 0.0   memory length: 59677   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 326   score: 2.0   memory length: 59875   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 327   score: 1.0   memory length: 60044   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 328   score: 1.0   memory length: 60213   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 329   score: 3.0   memory length: 60460   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 330   score: 0.0   memory length: 60583   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 331   score: 0.0   memory length: 60706   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 332   score: 1.0   memory length: 60877   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 333   score: 1.0   memory length: 61027   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 334   score: 2.0   memory length: 61225   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 335   score: 3.0   memory length: 61435   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 336   score: 8.0   memory length: 61876   epsilon: 1.0    steps: 441    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 337   score: 0.0   memory length: 61999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 338   score: 2.0   memory length: 62197   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 339   score: 0.0   memory length: 62320   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 340   score: 2.0   memory length: 62518   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 341   score: 3.0   memory length: 62747   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 342   score: 0.0   memory length: 62870   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 343   score: 0.0   memory length: 62993   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 344   score: 0.0   memory length: 63116   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 345   score: 1.0   memory length: 63285   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 346   score: 0.0   memory length: 63408   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 347   score: 3.0   memory length: 63633   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 348   score: 2.0   memory length: 63830   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 349   score: 2.0   memory length: 64047   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 350   score: 0.0   memory length: 64169   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 351   score: 3.0   memory length: 64418   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 352   score: 1.0   memory length: 64590   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 353   score: 0.0   memory length: 64713   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 354   score: 4.0   memory length: 64987   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 355   score: 5.0   memory length: 65313   epsilon: 1.0    steps: 326    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 356   score: 1.0   memory length: 65482   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 357   score: 0.0   memory length: 65604   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 358   score: 1.0   memory length: 65773   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 359   score: 3.0   memory length: 66039   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 360   score: 4.0   memory length: 66316   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 361   score: 1.0   memory length: 66485   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 362   score: 2.0   memory length: 66703   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 363   score: 0.0   memory length: 66825   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 364   score: 1.0   memory length: 66975   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 365   score: 1.0   memory length: 67125   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 366   score: 1.0   memory length: 67294   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 367   score: 3.0   memory length: 67539   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 368   score: 0.0   memory length: 67662   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 369   score: 1.0   memory length: 67833   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 370   score: 2.0   memory length: 68051   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 371   score: 2.0   memory length: 68266   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 372   score: 0.0   memory length: 68389   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 373   score: 1.0   memory length: 68540   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 374   score: 1.0   memory length: 68709   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 375   score: 0.0   memory length: 68831   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 376   score: 3.0   memory length: 69060   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 377   score: 0.0   memory length: 69183   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 378   score: 4.0   memory length: 69480   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 379   score: 3.0   memory length: 69745   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 380   score: 2.0   memory length: 69962   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 381   score: 5.0   memory length: 70280   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 382   score: 0.0   memory length: 70403   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 383   score: 3.0   memory length: 70649   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 384   score: 1.0   memory length: 70800   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 385   score: 4.0   memory length: 71090   epsilon: 1.0    steps: 290    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 386   score: 1.0   memory length: 71259   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 387   score: 0.0   memory length: 71382   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 388   score: 2.0   memory length: 71598   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 389   score: 1.0   memory length: 71766   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 390   score: 0.0   memory length: 71889   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 391   score: 2.0   memory length: 72087   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 392   score: 1.0   memory length: 72238   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 393   score: 3.0   memory length: 72484   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 394   score: 0.0   memory length: 72606   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 395   score: 1.0   memory length: 72757   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 396   score: 2.0   memory length: 72974   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 397   score: 1.0   memory length: 73142   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 398   score: 2.0   memory length: 73363   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 399   score: 1.0   memory length: 73531   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 400   score: 0.0   memory length: 73654   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 401   score: 2.0   memory length: 73871   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 402   score: 2.0   memory length: 74069   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 403   score: 4.0   memory length: 74365   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 404   score: 3.0   memory length: 74612   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 405   score: 0.0   memory length: 74735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 406   score: 3.0   memory length: 74998   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 407   score: 2.0   memory length: 75215   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 408   score: 0.0   memory length: 75337   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 409   score: 3.0   memory length: 75606   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 410   score: 0.0   memory length: 75728   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 411   score: 1.0   memory length: 75879   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 412   score: 0.0   memory length: 76002   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 413   score: 2.0   memory length: 76217   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 414   score: 1.0   memory length: 76369   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 415   score: 1.0   memory length: 76519   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 416   score: 1.0   memory length: 76669   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 417   score: 0.0   memory length: 76792   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 418   score: 0.0   memory length: 76915   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 419   score: 0.0   memory length: 77037   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 420   score: 2.0   memory length: 77234   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 421   score: 1.0   memory length: 77406   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 422   score: 1.0   memory length: 77575   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 423   score: 0.0   memory length: 77698   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 424   score: 2.0   memory length: 77896   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 425   score: 3.0   memory length: 78161   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 426   score: 2.0   memory length: 78361   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 427   score: 0.0   memory length: 78484   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 428   score: 2.0   memory length: 78704   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 429   score: 0.0   memory length: 78827   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 430   score: 3.0   memory length: 79073   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 431   score: 0.0   memory length: 79195   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 432   score: 0.0   memory length: 79317   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 433   score: 0.0   memory length: 79440   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 434   score: 1.0   memory length: 79611   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 435   score: 3.0   memory length: 79838   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 436   score: 3.0   memory length: 80072   epsilon: 1.0    steps: 234    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 437   score: 6.0   memory length: 80471   epsilon: 1.0    steps: 399    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 438   score: 3.0   memory length: 80719   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 439   score: 2.0   memory length: 80916   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 440   score: 0.0   memory length: 81038   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 441   score: 0.0   memory length: 81161   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 442   score: 2.0   memory length: 81378   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 443   score: 1.0   memory length: 81529   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 444   score: 1.0   memory length: 81698   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 445   score: 4.0   memory length: 81954   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 446   score: 0.0   memory length: 82077   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 447   score: 2.0   memory length: 82275   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 448   score: 1.0   memory length: 82444   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 449   score: 0.0   memory length: 82567   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 450   score: 1.0   memory length: 82718   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 451   score: 4.0   memory length: 83013   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 452   score: 0.0   memory length: 83135   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 453   score: 3.0   memory length: 83366   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 454   score: 1.0   memory length: 83517   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 455   score: 0.0   memory length: 83640   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 456   score: 0.0   memory length: 83762   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 457   score: 3.0   memory length: 84009   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 458   score: 0.0   memory length: 84131   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 459   score: 0.0   memory length: 84254   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 460   score: 0.0   memory length: 84377   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 461   score: 0.0   memory length: 84499   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 462   score: 2.0   memory length: 84697   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 463   score: 0.0   memory length: 84820   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 464   score: 0.0   memory length: 84942   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 465   score: 1.0   memory length: 85092   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 466   score: 0.0   memory length: 85215   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 467   score: 0.0   memory length: 85337   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 468   score: 1.0   memory length: 85509   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 469   score: 0.0   memory length: 85631   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 470   score: 3.0   memory length: 85897   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 471   score: 2.0   memory length: 86114   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 472   score: 3.0   memory length: 86343   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 473   score: 1.0   memory length: 86494   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 474   score: 0.0   memory length: 86617   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 475   score: 2.0   memory length: 86834   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 476   score: 0.0   memory length: 86957   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 477   score: 4.0   memory length: 87252   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 478   score: 3.0   memory length: 87478   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 479   score: 1.0   memory length: 87647   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 480   score: 0.0   memory length: 87769   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 481   score: 4.0   memory length: 88063   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 482   score: 0.0   memory length: 88186   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 483   score: 0.0   memory length: 88309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 484   score: 2.0   memory length: 88506   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 485   score: 0.0   memory length: 88629   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 486   score: 0.0   memory length: 88752   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 487   score: 2.0   memory length: 88953   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 488   score: 0.0   memory length: 89076   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 489   score: 0.0   memory length: 89198   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 490   score: 0.0   memory length: 89320   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 491   score: 2.0   memory length: 89518   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 492   score: 0.0   memory length: 89640   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 493   score: 3.0   memory length: 89885   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 494   score: 1.0   memory length: 90035   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 495   score: 0.0   memory length: 90157   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 496   score: 2.0   memory length: 90374   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 497   score: 1.0   memory length: 90543   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 498   score: 1.0   memory length: 90712   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 499   score: 2.0   memory length: 90910   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 500   score: 2.0   memory length: 91107   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 501   score: 1.0   memory length: 91277   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 502   score: 3.0   memory length: 91524   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 503   score: 1.0   memory length: 91695   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 504   score: 7.0   memory length: 92097   epsilon: 1.0    steps: 402    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 505   score: 2.0   memory length: 92297   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 506   score: 2.0   memory length: 92500   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 507   score: 1.0   memory length: 92670   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 508   score: 0.0   memory length: 92792   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 509   score: 0.0   memory length: 92915   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 510   score: 3.0   memory length: 93143   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 511   score: 0.0   memory length: 93266   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 512   score: 0.0   memory length: 93389   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 513   score: 3.0   memory length: 93634   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 514   score: 0.0   memory length: 93757   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 515   score: 0.0   memory length: 93880   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 516   score: 2.0   memory length: 94077   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 517   score: 1.0   memory length: 94245   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 518   score: 3.0   memory length: 94491   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 519   score: 1.0   memory length: 94662   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 520   score: 1.0   memory length: 94813   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 521   score: 3.0   memory length: 95038   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 522   score: 0.0   memory length: 95160   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 523   score: 0.0   memory length: 95283   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 524   score: 0.0   memory length: 95406   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 525   score: 2.0   memory length: 95606   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 526   score: 1.0   memory length: 95756   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 527   score: 0.0   memory length: 95879   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 528   score: 3.0   memory length: 96126   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 529   score: 4.0   memory length: 96379   epsilon: 1.0    steps: 253    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 530   score: 0.0   memory length: 96501   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 531   score: 2.0   memory length: 96699   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 532   score: 0.0   memory length: 96822   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 533   score: 1.0   memory length: 96991   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 534   score: 3.0   memory length: 97238   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 535   score: 0.0   memory length: 97360   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 536   score: 2.0   memory length: 97576   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 537   score: 0.0   memory length: 97699   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 538   score: 4.0   memory length: 97986   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 539   score: 1.0   memory length: 98137   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 540   score: 0.0   memory length: 98260   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 541   score: 4.0   memory length: 98576   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 542   score: 3.0   memory length: 98821   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 543   score: 0.0   memory length: 98944   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 544   score: 3.0   memory length: 99211   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 545   score: 0.0   memory length: 99334   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 546   score: 1.0   memory length: 99502   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 547   score: 0.0   memory length: 99625   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 548   score: 0.0   memory length: 99748   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 549   score: 1.0   memory length: 99918   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 550   score: 0.0   memory length: 100041   epsilon: 0.9999168400000018    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 551   score: 2.0   memory length: 100259   epsilon: 0.9994852000000112    steps: 218    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 552   score: 1.0   memory length: 100428   epsilon: 0.9991505800000184    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 553   score: 0.0   memory length: 100551   epsilon: 0.9989070400000237    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 554   score: 1.0   memory length: 100722   epsilon: 0.9985684600000311    steps: 171    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 555   score: 0.0   memory length: 100845   epsilon: 0.9983249200000364    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 556   score: 0.0   memory length: 100967   epsilon: 0.9980833600000416    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 557   score: 0.0   memory length: 101090   epsilon: 0.9978398200000469    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 558   score: 0.0   memory length: 101212   epsilon: 0.9975982600000521    steps: 122    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 559   score: 3.0   memory length: 101457   epsilon: 0.9971131600000627    steps: 245    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 560   score: 3.0   memory length: 101703   epsilon: 0.9966260800000732    steps: 246    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 561   score: 1.0   memory length: 101873   epsilon: 0.9962894800000806    steps: 170    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 562   score: 3.0   memory length: 102141   epsilon: 0.9957588400000921    steps: 268    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 563   score: 1.0   memory length: 102312   epsilon: 0.9954202600000994    steps: 171    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 564   score: 0.0   memory length: 102434   epsilon: 0.9951787000001047    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 565   score: 0.0   memory length: 102557   epsilon: 0.99493516000011    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 566   score: 0.0   memory length: 102680   epsilon: 0.9946916200001152    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 567   score: 2.0   memory length: 102879   epsilon: 0.9942976000001238    steps: 199    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 568   score: 2.0   memory length: 103077   epsilon: 0.9939055600001323    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 569   score: 0.0   memory length: 103200   epsilon: 0.9936620200001376    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 570   score: 2.0   memory length: 103380   epsilon: 0.9933056200001453    steps: 180    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 571   score: 3.0   memory length: 103630   epsilon: 0.9928106200001561    steps: 250    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 572   score: 0.0   memory length: 103753   epsilon: 0.9925670800001614    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 573   score: 2.0   memory length: 103934   epsilon: 0.9922087000001691    steps: 181    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 574   score: 2.0   memory length: 104150   epsilon: 0.9917810200001784    steps: 216    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 575   score: 2.0   memory length: 104368   epsilon: 0.9913493800001878    steps: 218    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 576   score: 3.0   memory length: 104612   epsilon: 0.9908662600001983    steps: 244    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 577   score: 1.0   memory length: 104781   epsilon: 0.9905316400002055    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 578   score: 0.0   memory length: 104904   epsilon: 0.9902881000002108    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 579   score: 5.0   memory length: 105248   epsilon: 0.9896069800002256    steps: 344    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 580   score: 2.0   memory length: 105466   epsilon: 0.989175340000235    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 581   score: 3.0   memory length: 105710   epsilon: 0.9886922200002455    steps: 244    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 582   score: 1.0   memory length: 105878   epsilon: 0.9883595800002527    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 583   score: 2.0   memory length: 106076   epsilon: 0.9879675400002612    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 584   score: 2.0   memory length: 106274   epsilon: 0.9875755000002697    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 585   score: 2.0   memory length: 106472   epsilon: 0.9871834600002782    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 586   score: 1.0   memory length: 106641   epsilon: 0.9868488400002855    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 587   score: 0.0   memory length: 106764   epsilon: 0.9866053000002908    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 588   score: 0.0   memory length: 106887   epsilon: 0.9863617600002961    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 589   score: 3.0   memory length: 107112   epsilon: 0.9859162600003057    steps: 225    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 590   score: 3.0   memory length: 107361   epsilon: 0.9854232400003164    steps: 249    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 591   score: 0.0   memory length: 107484   epsilon: 0.9851797000003217    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 592   score: 2.0   memory length: 107701   epsilon: 0.9847500400003311    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 593   score: 2.0   memory length: 107898   epsilon: 0.9843599800003395    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 594   score: 1.0   memory length: 108068   epsilon: 0.9840233800003468    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 595   score: 1.0   memory length: 108237   epsilon: 0.9836887600003541    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 596   score: 2.0   memory length: 108434   epsilon: 0.9832987000003626    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 597   score: 3.0   memory length: 108664   epsilon: 0.9828433000003725    steps: 230    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 598   score: 4.0   memory length: 108980   epsilon: 0.982217620000386    steps: 316    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 599   score: 1.0   memory length: 109131   epsilon: 0.9819186400003925    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 600   score: 0.0   memory length: 109254   epsilon: 0.9816751000003978    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 601   score: 3.0   memory length: 109498   epsilon: 0.9811919800004083    steps: 244    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 602   score: 2.0   memory length: 109696   epsilon: 0.9807999400004168    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 603   score: 0.0   memory length: 109819   epsilon: 0.9805564000004221    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 604   score: 1.0   memory length: 109988   epsilon: 0.9802217800004294    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 605   score: 3.0   memory length: 110214   epsilon: 0.9797743000004391    steps: 226    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 606   score: 1.0   memory length: 110365   epsilon: 0.9794753200004456    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 607   score: 0.0   memory length: 110488   epsilon: 0.9792317800004509    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 608   score: 0.0   memory length: 110611   epsilon: 0.9789882400004561    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 609   score: 0.0   memory length: 110734   epsilon: 0.9787447000004614    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 610   score: 1.0   memory length: 110885   epsilon: 0.9784457200004679    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 611   score: 0.0   memory length: 111008   epsilon: 0.9782021800004732    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 612   score: 3.0   memory length: 111235   epsilon: 0.977752720000483    steps: 227    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 613   score: 2.0   memory length: 111453   epsilon: 0.9773210800004923    steps: 218    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 614   score: 1.0   memory length: 111606   epsilon: 0.9770181400004989    steps: 153    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 615   score: 1.0   memory length: 111757   epsilon: 0.9767191600005054    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 616   score: 3.0   memory length: 112003   epsilon: 0.976232080000516    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 617   score: 1.0   memory length: 112153   epsilon: 0.9759350800005224    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 618   score: 1.0   memory length: 112322   epsilon: 0.9756004600005297    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 619   score: 2.0   memory length: 112540   epsilon: 0.9751688200005391    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 620   score: 3.0   memory length: 112786   epsilon: 0.9746817400005496    steps: 246    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 621   score: 0.0   memory length: 112908   epsilon: 0.9744401800005549    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 622   score: 1.0   memory length: 113076   epsilon: 0.9741075400005621    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 623   score: 5.0   memory length: 113373   epsilon: 0.9735194800005749    steps: 297    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 624   score: 0.0   memory length: 113496   epsilon: 0.9732759400005802    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 625   score: 3.0   memory length: 113764   epsilon: 0.9727453000005917    steps: 268    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 626   score: 1.0   memory length: 113933   epsilon: 0.9724106800005989    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 627   score: 0.0   memory length: 114056   epsilon: 0.9721671400006042    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 628   score: 3.0   memory length: 114327   epsilon: 0.9716305600006159    steps: 271    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 629   score: 0.0   memory length: 114450   epsilon: 0.9713870200006212    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 630   score: 2.0   memory length: 114668   epsilon: 0.9709553800006305    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 631   score: 0.0   memory length: 114791   epsilon: 0.9707118400006358    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 632   score: 2.0   memory length: 114989   epsilon: 0.9703198000006443    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 633   score: 1.0   memory length: 115159   epsilon: 0.9699832000006516    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 634   score: 1.0   memory length: 115328   epsilon: 0.9696485800006589    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 635   score: 4.0   memory length: 115624   epsilon: 0.9690625000006716    steps: 296    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 636   score: 4.0   memory length: 115882   epsilon: 0.9685516600006827    steps: 258    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 637   score: 1.0   memory length: 116053   epsilon: 0.9682130800006901    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 638   score: 2.0   memory length: 116271   epsilon: 0.9677814400006994    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 639   score: 2.0   memory length: 116469   epsilon: 0.9673894000007079    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 640   score: 2.0   memory length: 116689   epsilon: 0.9669538000007174    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 641   score: 3.0   memory length: 116954   epsilon: 0.9664291000007288    steps: 265    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 642   score: 1.0   memory length: 117104   epsilon: 0.9661321000007352    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 643   score: 2.0   memory length: 117304   epsilon: 0.9657361000007438    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 644   score: 8.0   memory length: 117623   epsilon: 0.9651044800007575    steps: 319    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 645   score: 0.0   memory length: 117746   epsilon: 0.9648609400007628    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 646   score: 5.0   memory length: 118088   epsilon: 0.9641837800007775    steps: 342    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 647   score: 1.0   memory length: 118258   epsilon: 0.9638471800007848    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 648   score: 3.0   memory length: 118526   epsilon: 0.9633165400007964    steps: 268    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 649   score: 4.0   memory length: 118819   epsilon: 0.962736400000809    steps: 293    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 650   score: 0.0   memory length: 118942   epsilon: 0.9624928600008142    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 651   score: 1.0   memory length: 119093   epsilon: 0.9621938800008207    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 652   score: 2.0   memory length: 119295   epsilon: 0.9617939200008294    steps: 202    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 653   score: 4.0   memory length: 119595   epsilon: 0.9611999200008423    steps: 300    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 654   score: 4.0   memory length: 119852   epsilon: 0.9606910600008534    steps: 257    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 655   score: 3.0   memory length: 120118   epsilon: 0.9601643800008648    steps: 266    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 656   score: 0.0   memory length: 120240   epsilon: 0.95992282000087    steps: 122    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 657   score: 0.0   memory length: 120363   epsilon: 0.9596792800008753    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 658   score: 2.0   memory length: 120581   epsilon: 0.9592476400008847    steps: 218    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 659   score: 2.0   memory length: 120779   epsilon: 0.9588556000008932    steps: 198    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 660   score: 0.0   memory length: 120902   epsilon: 0.9586120600008985    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 661   score: 3.0   memory length: 121172   epsilon: 0.9580774600009101    steps: 270    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 662   score: 1.0   memory length: 121341   epsilon: 0.9577428400009174    steps: 169    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 663   score: 2.0   memory length: 121539   epsilon: 0.9573508000009259    steps: 198    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 664   score: 1.0   memory length: 121708   epsilon: 0.9570161800009331    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 665   score: 0.0   memory length: 121830   epsilon: 0.9567746200009384    steps: 122    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 666   score: 1.0   memory length: 121998   epsilon: 0.9564419800009456    steps: 168    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 667   score: 1.0   memory length: 122168   epsilon: 0.9561053800009529    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 668   score: 0.0   memory length: 122291   epsilon: 0.9558618400009582    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 669   score: 5.0   memory length: 122620   epsilon: 0.9552104200009723    steps: 329    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 670   score: 2.0   memory length: 122818   epsilon: 0.9548183800009808    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 671   score: 0.0   memory length: 122941   epsilon: 0.9545748400009861    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 672   score: 2.0   memory length: 123161   epsilon: 0.9541392400009956    steps: 220    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 673   score: 3.0   memory length: 123430   epsilon: 0.9536066200010072    steps: 269    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 674   score: 1.0   memory length: 123598   epsilon: 0.9532739800010144    steps: 168    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 675   score: 0.0   memory length: 123721   epsilon: 0.9530304400010197    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 676   score: 0.0   memory length: 123844   epsilon: 0.952786900001025    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 677   score: 2.0   memory length: 124041   epsilon: 0.9523968400010334    steps: 197    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 678   score: 0.0   memory length: 124164   epsilon: 0.9521533000010387    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 679   score: 3.0   memory length: 124428   epsilon: 0.95163058000105    steps: 264    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 680   score: 1.0   memory length: 124597   epsilon: 0.9512959600010573    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 681   score: 3.0   memory length: 124845   epsilon: 0.950804920001068    steps: 248    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 682   score: 3.0   memory length: 125091   epsilon: 0.9503178400010786    steps: 246    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 683   score: 0.0   memory length: 125214   epsilon: 0.9500743000010838    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 684   score: 0.0   memory length: 125337   epsilon: 0.9498307600010891    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 685   score: 0.0   memory length: 125459   epsilon: 0.9495892000010944    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 686   score: 0.0   memory length: 125581   epsilon: 0.9493476400010996    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 687   score: 1.0   memory length: 125731   epsilon: 0.9490506400011061    steps: 150    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 688   score: 0.0   memory length: 125854   epsilon: 0.9488071000011113    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 689   score: 2.0   memory length: 126052   epsilon: 0.9484150600011199    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 690   score: 0.0   memory length: 126175   epsilon: 0.9481715200011251    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 691   score: 0.0   memory length: 126297   epsilon: 0.9479299600011304    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 692   score: 1.0   memory length: 126448   epsilon: 0.9476309800011369    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 693   score: 1.0   memory length: 126599   epsilon: 0.9473320000011434    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 694   score: 5.0   memory length: 126911   epsilon: 0.9467142400011568    steps: 312    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 695   score: 0.0   memory length: 127034   epsilon: 0.9464707000011621    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 696   score: 3.0   memory length: 127266   epsilon: 0.946011340001172    steps: 232    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 697   score: 0.0   memory length: 127389   epsilon: 0.9457678000011773    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 698   score: 0.0   memory length: 127511   epsilon: 0.9455262400011826    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 699   score: 2.0   memory length: 127709   epsilon: 0.9451342000011911    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 700   score: 2.0   memory length: 127906   epsilon: 0.9447441400011996    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 701   score: 1.0   memory length: 128078   epsilon: 0.9444035800012069    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 702   score: 0.0   memory length: 128200   epsilon: 0.9441620200012122    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 703   score: 1.0   memory length: 128370   epsilon: 0.9438254200012195    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 704   score: 3.0   memory length: 128618   epsilon: 0.9433343800012302    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 705   score: 0.0   memory length: 128741   epsilon: 0.9430908400012354    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 706   score: 2.0   memory length: 128939   epsilon: 0.942698800001244    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 707   score: 2.0   memory length: 129160   epsilon: 0.9422612200012535    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 708   score: 0.0   memory length: 129283   epsilon: 0.9420176800012587    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 709   score: 2.0   memory length: 129500   epsilon: 0.9415880200012681    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 710   score: 0.0   memory length: 129623   epsilon: 0.9413444800012734    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 711   score: 1.0   memory length: 129774   epsilon: 0.9410455000012798    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 712   score: 5.0   memory length: 130092   epsilon: 0.9404158600012935    steps: 318    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 713   score: 0.0   memory length: 130215   epsilon: 0.9401723200012988    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 714   score: 1.0   memory length: 130384   epsilon: 0.9398377000013061    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 715   score: 0.0   memory length: 130507   epsilon: 0.9395941600013114    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 716   score: 0.0   memory length: 130630   epsilon: 0.9393506200013166    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 717   score: 0.0   memory length: 130753   epsilon: 0.9391070800013219    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 718   score: 1.0   memory length: 130924   epsilon: 0.9387685000013293    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 719   score: 3.0   memory length: 131155   epsilon: 0.9383111200013392    steps: 231    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 720   score: 3.0   memory length: 131402   epsilon: 0.9378220600013498    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 721   score: 2.0   memory length: 131599   epsilon: 0.9374320000013583    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 722   score: 2.0   memory length: 131796   epsilon: 0.9370419400013668    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 723   score: 0.0   memory length: 131919   epsilon: 0.936798400001372    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 724   score: 1.0   memory length: 132089   epsilon: 0.9364618000013794    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 725   score: 0.0   memory length: 132211   epsilon: 0.9362202400013846    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 726   score: 2.0   memory length: 132429   epsilon: 0.935788600001394    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 727   score: 0.0   memory length: 132552   epsilon: 0.9355450600013993    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 728   score: 1.0   memory length: 132703   epsilon: 0.9352460800014057    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 729   score: 6.0   memory length: 133097   epsilon: 0.9344659600014227    steps: 394    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 730   score: 0.0   memory length: 133219   epsilon: 0.9342244000014279    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 731   score: 2.0   memory length: 133440   epsilon: 0.9337868200014374    steps: 221    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 732   score: 3.0   memory length: 133667   epsilon: 0.9333373600014472    steps: 227    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 733   score: 2.0   memory length: 133865   epsilon: 0.9329453200014557    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 734   score: 2.0   memory length: 134083   epsilon: 0.9325136800014651    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 735   score: 2.0   memory length: 134281   epsilon: 0.9321216400014736    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 736   score: 1.0   memory length: 134432   epsilon: 0.9318226600014801    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 737   score: 2.0   memory length: 134630   epsilon: 0.9314306200014886    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 738   score: 2.0   memory length: 134850   epsilon: 0.930995020001498    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 739   score: 2.0   memory length: 135048   epsilon: 0.9306029800015065    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 740   score: 0.0   memory length: 135170   epsilon: 0.9303614200015118    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 741   score: 0.0   memory length: 135293   epsilon: 0.9301178800015171    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 742   score: 2.0   memory length: 135491   epsilon: 0.9297258400015256    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 743   score: 3.0   memory length: 135734   epsilon: 0.929244700001536    steps: 243    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 744   score: 1.0   memory length: 135906   epsilon: 0.9289041400015434    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 745   score: 1.0   memory length: 136075   epsilon: 0.9285695200015507    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 746   score: 4.0   memory length: 136390   epsilon: 0.9279458200015642    steps: 315    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 747   score: 2.0   memory length: 136591   epsilon: 0.9275478400015729    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 748   score: 1.0   memory length: 136742   epsilon: 0.9272488600015794    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 749   score: 0.0   memory length: 136864   epsilon: 0.9270073000015846    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 750   score: 4.0   memory length: 137142   epsilon: 0.9264568600015965    steps: 278    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 751   score: 0.0   memory length: 137265   epsilon: 0.9262133200016018    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 752   score: 3.0   memory length: 137515   epsilon: 0.9257183200016126    steps: 250    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 753   score: 2.0   memory length: 137715   epsilon: 0.9253223200016212    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 754   score: 1.0   memory length: 137866   epsilon: 0.9250233400016277    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 755   score: 0.0   memory length: 137988   epsilon: 0.9247817800016329    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 756   score: 2.0   memory length: 138186   epsilon: 0.9243897400016414    steps: 198    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 757   score: 0.0   memory length: 138308   epsilon: 0.9241481800016467    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 758   score: 2.0   memory length: 138490   epsilon: 0.9237878200016545    steps: 182    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 759   score: 3.0   memory length: 138716   epsilon: 0.9233403400016642    steps: 226    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 760   score: 1.0   memory length: 138885   epsilon: 0.9230057200016715    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 761   score: 2.0   memory length: 139103   epsilon: 0.9225740800016808    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 762   score: 4.0   memory length: 139378   epsilon: 0.9220295800016927    steps: 275    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 763   score: 2.0   memory length: 139578   epsilon: 0.9216335800017013    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 764   score: 2.0   memory length: 139794   epsilon: 0.9212059000017105    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 765   score: 0.0   memory length: 139916   epsilon: 0.9209643400017158    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 766   score: 2.0   memory length: 140114   epsilon: 0.9205723000017243    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 767   score: 3.0   memory length: 140382   epsilon: 0.9200416600017358    steps: 268    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 768   score: 4.0   memory length: 140699   epsilon: 0.9194140000017494    steps: 317    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 769   score: 3.0   memory length: 140947   epsilon: 0.9189229600017601    steps: 248    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 770   score: 2.0   memory length: 141165   epsilon: 0.9184913200017695    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 771   score: 1.0   memory length: 141335   epsilon: 0.9181547200017768    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 772   score: 2.0   memory length: 141535   epsilon: 0.9177587200017854    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 773   score: 2.0   memory length: 141732   epsilon: 0.9173686600017938    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 774   score: 0.0   memory length: 141855   epsilon: 0.9171251200017991    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 775   score: 1.0   memory length: 142024   epsilon: 0.9167905000018064    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 776   score: 1.0   memory length: 142194   epsilon: 0.9164539000018137    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 777   score: 3.0   memory length: 142437   epsilon: 0.9159727600018241    steps: 243    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 778   score: 4.0   memory length: 142697   epsilon: 0.9154579600018353    steps: 260    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 779   score: 2.0   memory length: 142914   epsilon: 0.9150283000018447    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 780   score: 1.0   memory length: 143065   epsilon: 0.9147293200018511    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 781   score: 0.0   memory length: 143187   epsilon: 0.9144877600018564    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 782   score: 3.0   memory length: 143435   epsilon: 0.913996720001867    steps: 248    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 783   score: 2.0   memory length: 143633   epsilon: 0.9136046800018756    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 784   score: 2.0   memory length: 143831   epsilon: 0.9132126400018841    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 785   score: 2.0   memory length: 144048   epsilon: 0.9127829800018934    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 786   score: 2.0   memory length: 144246   epsilon: 0.9123909400019019    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 787   score: 3.0   memory length: 144491   epsilon: 0.9119058400019124    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 788   score: 1.0   memory length: 144662   epsilon: 0.9115672600019198    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 789   score: 4.0   memory length: 144981   epsilon: 0.9109356400019335    steps: 319    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 790   score: 6.0   memory length: 145233   epsilon: 0.9104366800019443    steps: 252    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 791   score: 0.0   memory length: 145356   epsilon: 0.9101931400019496    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 792   score: 4.0   memory length: 145632   epsilon: 0.9096466600019615    steps: 276    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 793   score: 1.0   memory length: 145801   epsilon: 0.9093120400019687    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 794   score: 4.0   memory length: 146094   epsilon: 0.9087319000019813    steps: 293    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 795   score: 0.0   memory length: 146217   epsilon: 0.9084883600019866    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 796   score: 1.0   memory length: 146388   epsilon: 0.908149780001994    steps: 171    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 797   score: 1.0   memory length: 146539   epsilon: 0.9078508000020005    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 798   score: 3.0   memory length: 146766   epsilon: 0.9074013400020102    steps: 227    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 799   score: 1.0   memory length: 146917   epsilon: 0.9071023600020167    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 800   score: 0.0   memory length: 147039   epsilon: 0.906860800002022    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 801   score: 5.0   memory length: 147362   epsilon: 0.9062212600020358    steps: 323    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 802   score: 2.0   memory length: 147560   epsilon: 0.9058292200020444    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 803   score: 2.0   memory length: 147781   epsilon: 0.9053916400020539    steps: 221    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 804   score: 3.0   memory length: 148028   epsilon: 0.9049025800020645    steps: 247    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 805   score: 2.0   memory length: 148226   epsilon: 0.904510540002073    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 806   score: 8.0   memory length: 148602   epsilon: 0.9037660600020891    steps: 376    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 807   score: 2.0   memory length: 148800   epsilon: 0.9033740200020977    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 808   score: 3.0   memory length: 149044   epsilon: 0.9028909000021081    steps: 244    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 809   score: 2.0   memory length: 149242   epsilon: 0.9024988600021167    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 810   score: 4.0   memory length: 149538   epsilon: 0.9019127800021294    steps: 296    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 811   score: 2.0   memory length: 149739   epsilon: 0.901514800002138    steps: 201    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 812   score: 1.0   memory length: 149889   epsilon: 0.9012178000021445    steps: 150    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 813   score: 0.0   memory length: 150012   epsilon: 0.9009742600021498    steps: 123    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 814   score: 1.0   memory length: 150182   epsilon: 0.9006376600021571    steps: 170    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 815   score: 2.0   memory length: 150380   epsilon: 0.9002456200021656    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 816   score: 2.0   memory length: 150599   epsilon: 0.899812000002175    steps: 219    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 817   score: 1.0   memory length: 150750   epsilon: 0.8995130200021815    steps: 151    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 818   score: 3.0   memory length: 150994   epsilon: 0.899029900002192    steps: 244    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 819   score: 2.0   memory length: 151212   epsilon: 0.8985982600022013    steps: 218    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 820   score: 0.0   memory length: 151334   epsilon: 0.8983567000022066    steps: 122    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 821   score: 2.0   memory length: 151532   epsilon: 0.8979646600022151    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 822   score: 2.0   memory length: 151750   epsilon: 0.8975330200022245    steps: 218    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 823   score: 0.0   memory length: 151873   epsilon: 0.8972894800022297    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 824   score: 3.0   memory length: 152142   epsilon: 0.8967568600022413    steps: 269    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 825   score: 3.0   memory length: 152372   epsilon: 0.8963014600022512    steps: 230    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 826   score: 1.0   memory length: 152544   epsilon: 0.8959609000022586    steps: 172    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 827   score: 1.0   memory length: 152713   epsilon: 0.8956262800022659    steps: 169    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 828   score: 0.0   memory length: 152836   epsilon: 0.8953827400022711    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 829   score: 2.0   memory length: 153034   epsilon: 0.8949907000022796    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 830   score: 0.0   memory length: 153157   epsilon: 0.8947471600022849    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 831   score: 0.0   memory length: 153279   epsilon: 0.8945056000022902    steps: 122    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 832   score: 3.0   memory length: 153527   epsilon: 0.8940145600023008    steps: 248    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 833   score: 1.0   memory length: 153677   epsilon: 0.8937175600023073    steps: 150    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 834   score: 1.0   memory length: 153828   epsilon: 0.8934185800023138    steps: 151    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 835   score: 2.0   memory length: 154008   epsilon: 0.8930621800023215    steps: 180    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 836   score: 0.0   memory length: 154131   epsilon: 0.8928186400023268    steps: 123    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 837   score: 3.0   memory length: 154357   epsilon: 0.8923711600023365    steps: 226    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 838   score: 3.0   memory length: 154627   epsilon: 0.8918365600023481    steps: 270    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 839   score: 3.0   memory length: 154853   epsilon: 0.8913890800023578    steps: 226    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 840   score: 2.0   memory length: 155071   epsilon: 0.8909574400023672    steps: 218    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 841   score: 2.0   memory length: 155269   epsilon: 0.8905654000023757    steps: 198    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 842   score: 4.0   memory length: 155544   epsilon: 0.8900209000023875    steps: 275    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 843   score: 6.0   memory length: 155928   epsilon: 0.889260580002404    steps: 384    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 844   score: 2.0   memory length: 156144   epsilon: 0.8888329000024133    steps: 216    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 845   score: 2.0   memory length: 156361   epsilon: 0.8884032400024227    steps: 217    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 846   score: 1.0   memory length: 156532   epsilon: 0.88806466000243    steps: 171    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 847   score: 2.0   memory length: 156732   epsilon: 0.8876686600024386    steps: 200    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 848   score: 2.0   memory length: 156931   epsilon: 0.8872746400024472    steps: 199    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 849   score: 5.0   memory length: 157256   epsilon: 0.8866311400024611    steps: 325    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 850   score: 1.0   memory length: 157427   epsilon: 0.8862925600024685    steps: 171    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 851   score: 3.0   memory length: 157695   epsilon: 0.88576192000248    steps: 268    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 852   score: 3.0   memory length: 157942   epsilon: 0.8852728600024906    steps: 247    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 853   score: 5.0   memory length: 158294   epsilon: 0.8845759000025057    steps: 352    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 854   score: 1.0   memory length: 158464   epsilon: 0.884239300002513    steps: 170    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 855   score: 1.0   memory length: 158633   epsilon: 0.8839046800025203    steps: 169    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 856   score: 3.0   memory length: 158897   epsilon: 0.8833819600025317    steps: 264    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 857   score: 4.0   memory length: 159193   epsilon: 0.8827958800025444    steps: 296    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 858   score: 3.0   memory length: 159418   epsilon: 0.8823503800025541    steps: 225    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 859   score: 5.0   memory length: 159724   epsilon: 0.8817445000025672    steps: 306    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 860   score: 3.0   memory length: 159969   epsilon: 0.8812594000025777    steps: 245    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 861   score: 2.0   memory length: 160185   epsilon: 0.880831720002587    steps: 216    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 862   score: 2.0   memory length: 160383   epsilon: 0.8804396800025955    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 863   score: 3.0   memory length: 160630   epsilon: 0.8799506200026062    steps: 247    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 864   score: 2.0   memory length: 160828   epsilon: 0.8795585800026147    steps: 198    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 865   score: 0.0   memory length: 160951   epsilon: 0.87931504000262    steps: 123    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 866   score: 5.0   memory length: 161231   epsilon: 0.878760640002632    steps: 280    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 867   score: 2.0   memory length: 161429   epsilon: 0.8783686000026405    steps: 198    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 868   score: 5.0   memory length: 161768   epsilon: 0.8776973800026551    steps: 339    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 869   score: 0.0   memory length: 161890   epsilon: 0.8774558200026603    steps: 122    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 870   score: 3.0   memory length: 162115   epsilon: 0.87701032000267    steps: 225    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 871   score: 0.0   memory length: 162238   epsilon: 0.8767667800026753    steps: 123    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 872   score: 4.0   memory length: 162512   epsilon: 0.876224260002687    steps: 274    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 873   score: 0.0   memory length: 162635   epsilon: 0.8759807200026923    steps: 123    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 874   score: 3.0   memory length: 162903   epsilon: 0.8754500800027039    steps: 268    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 875   score: 3.0   memory length: 163147   epsilon: 0.8749669600027143    steps: 244    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 876   score: 1.0   memory length: 163298   epsilon: 0.8746679800027208    steps: 151    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 877   score: 2.0   memory length: 163478   epsilon: 0.8743115800027286    steps: 180    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 878   score: 3.0   memory length: 163689   epsilon: 0.8738938000027376    steps: 211    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 879   score: 4.0   memory length: 164003   epsilon: 0.8732720800027511    steps: 314    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 880   score: 0.0   memory length: 164126   epsilon: 0.8730285400027564    steps: 123    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 881   score: 2.0   memory length: 164323   epsilon: 0.8726384800027649    steps: 197    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 882   score: 3.0   memory length: 164591   epsilon: 0.8721078400027764    steps: 268    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 883   score: 5.0   memory length: 164894   epsilon: 0.8715079000027894    steps: 303    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 884   score: 0.0   memory length: 165017   epsilon: 0.8712643600027947    steps: 123    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 885   score: 2.0   memory length: 165214   epsilon: 0.8708743000028032    steps: 197    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 886   score: 3.0   memory length: 165461   epsilon: 0.8703852400028138    steps: 247    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 887   score: 1.0   memory length: 165612   epsilon: 0.8700862600028203    steps: 151    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 888   score: 3.0   memory length: 165863   epsilon: 0.8695892800028311    steps: 251    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 889   score: 3.0   memory length: 166133   epsilon: 0.8690546800028427    steps: 270    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 890   score: 0.0   memory length: 166255   epsilon: 0.8688131200028479    steps: 122    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 891   score: 4.0   memory length: 166577   epsilon: 0.8681755600028618    steps: 322    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 892   score: 2.0   memory length: 166796   epsilon: 0.8677419400028712    steps: 219    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 893   score: 2.0   memory length: 166993   epsilon: 0.8673518800028797    steps: 197    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 894   score: 1.0   memory length: 167144   epsilon: 0.8670529000028862    steps: 151    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 895   score: 2.0   memory length: 167344   epsilon: 0.8666569000028947    steps: 200    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 896   score: 1.0   memory length: 167516   epsilon: 0.8663163400029021    steps: 172    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 897   score: 3.0   memory length: 167779   epsilon: 0.8657956000029134    steps: 263    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 898   score: 2.0   memory length: 167977   epsilon: 0.865403560002922    steps: 198    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 899   score: 4.0   memory length: 168252   epsilon: 0.8648590600029338    steps: 275    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 900   score: 6.0   memory length: 168623   epsilon: 0.8641244800029497    steps: 371    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 901   score: 3.0   memory length: 168872   epsilon: 0.8636314600029604    steps: 249    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 902   score: 3.0   memory length: 169098   epsilon: 0.8631839800029701    steps: 226    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 903   score: 1.0   memory length: 169249   epsilon: 0.8628850000029766    steps: 151    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 904   score: 2.0   memory length: 169465   epsilon: 0.8624573200029859    steps: 216    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 905   score: 3.0   memory length: 169712   epsilon: 0.8619682600029965    steps: 247    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 906   score: 0.0   memory length: 169835   epsilon: 0.8617247200030018    steps: 123    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 907   score: 0.0   memory length: 169958   epsilon: 0.8614811800030071    steps: 123    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 908   score: 1.0   memory length: 170130   epsilon: 0.8611406200030145    steps: 172    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 909   score: 5.0   memory length: 170453   epsilon: 0.8605010800030284    steps: 323    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 910   score: 2.0   memory length: 170651   epsilon: 0.8601090400030369    steps: 198    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 911   score: 1.0   memory length: 170822   epsilon: 0.8597704600030442    steps: 171    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 912   score: 2.0   memory length: 171020   epsilon: 0.8593784200030528    steps: 198    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 913   score: 1.0   memory length: 171189   epsilon: 0.85904380000306    steps: 169    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 914   score: 2.0   memory length: 171371   epsilon: 0.8586834400030678    steps: 182    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 915   score: 1.0   memory length: 171521   epsilon: 0.8583864400030743    steps: 150    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 916   score: 2.0   memory length: 171740   epsilon: 0.8579528200030837    steps: 219    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 917   score: 0.0   memory length: 171862   epsilon: 0.857711260003089    steps: 122    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 918   score: 6.0   memory length: 172226   epsilon: 0.8569905400031046    steps: 364    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 919   score: 4.0   memory length: 172521   epsilon: 0.8564064400031173    steps: 295    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 920   score: 1.0   memory length: 172691   epsilon: 0.8560698400031246    steps: 170    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 921   score: 2.0   memory length: 172912   epsilon: 0.8556322600031341    steps: 221    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 922   score: 2.0   memory length: 173133   epsilon: 0.8551946800031436    steps: 221    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 923   score: 3.0   memory length: 173359   epsilon: 0.8547472000031533    steps: 226    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 924   score: 4.0   memory length: 173652   epsilon: 0.8541670600031659    steps: 293    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 925   score: 3.0   memory length: 173916   epsilon: 0.8536443400031772    steps: 264    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 926   score: 3.0   memory length: 174164   epsilon: 0.8531533000031879    steps: 248    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 927   score: 0.0   memory length: 174287   epsilon: 0.8529097600031932    steps: 123    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 928   score: 0.0   memory length: 174410   epsilon: 0.8526662200031985    steps: 123    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 929   score: 3.0   memory length: 174641   epsilon: 0.8522088400032084    steps: 231    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 930   score: 4.0   memory length: 174940   epsilon: 0.8516168200032213    steps: 299    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 931   score: 2.0   memory length: 175156   epsilon: 0.8511891400032305    steps: 216    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 932   score: 3.0   memory length: 175384   epsilon: 0.8507377000032403    steps: 228    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 933   score: 3.0   memory length: 175654   epsilon: 0.8502031000032519    steps: 270    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 934   score: 3.0   memory length: 175902   epsilon: 0.8497120600032626    steps: 248    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 935   score: 1.0   memory length: 176052   epsilon: 0.849415060003269    steps: 150    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 936   score: 2.0   memory length: 176234   epsilon: 0.8490547000032769    steps: 182    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 937   score: 2.0   memory length: 176432   epsilon: 0.8486626600032854    steps: 198    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 938   score: 2.0   memory length: 176630   epsilon: 0.8482706200032939    steps: 198    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 939   score: 1.0   memory length: 176781   epsilon: 0.8479716400033004    steps: 151    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 940   score: 1.0   memory length: 176950   epsilon: 0.8476370200033077    steps: 169    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 941   score: 1.0   memory length: 177100   epsilon: 0.8473400200033141    steps: 150    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 942   score: 4.0   memory length: 177375   epsilon: 0.8467955200033259    steps: 275    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 943   score: 4.0   memory length: 177650   epsilon: 0.8462510200033377    steps: 275    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 944   score: 4.0   memory length: 177945   epsilon: 0.8456669200033504    steps: 295    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 945   score: 1.0   memory length: 178096   epsilon: 0.8453679400033569    steps: 151    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 946   score: 5.0   memory length: 178397   epsilon: 0.8447719600033698    steps: 301    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 947   score: 5.0   memory length: 178706   epsilon: 0.8441601400033831    steps: 309    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 948   score: 2.0   memory length: 178885   epsilon: 0.8438057200033908    steps: 179    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 949   score: 2.0   memory length: 179067   epsilon: 0.8434453600033986    steps: 182    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 950   score: 2.0   memory length: 179265   epsilon: 0.8430533200034072    steps: 198    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 951   score: 0.0   memory length: 179388   epsilon: 0.8428097800034124    steps: 123    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 952   score: 1.0   memory length: 179538   epsilon: 0.8425127800034189    steps: 150    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 953   score: 1.0   memory length: 179706   epsilon: 0.8421801400034261    steps: 168    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 954   score: 3.0   memory length: 179952   epsilon: 0.8416930600034367    steps: 246    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 955   score: 2.0   memory length: 180169   epsilon: 0.841263400003446    steps: 217    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 956   score: 3.0   memory length: 180397   epsilon: 0.8408119600034558    steps: 228    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 957   score: 3.0   memory length: 180625   epsilon: 0.8403605200034656    steps: 228    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 958   score: 2.0   memory length: 180807   epsilon: 0.8400001600034734    steps: 182    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 959   score: 2.0   memory length: 181005   epsilon: 0.839608120003482    steps: 198    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 960   score: 4.0   memory length: 181299   epsilon: 0.8390260000034946    steps: 294    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 961   score: 4.0   memory length: 181574   epsilon: 0.8384815000035064    steps: 275    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 962   score: 5.0   memory length: 181879   epsilon: 0.8378776000035195    steps: 305    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 963   score: 1.0   memory length: 182049   epsilon: 0.8375410000035268    steps: 170    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 964   score: 2.0   memory length: 182264   epsilon: 0.8371153000035361    steps: 215    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 965   score: 2.0   memory length: 182461   epsilon: 0.8367252400035445    steps: 197    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 966   score: 3.0   memory length: 182710   epsilon: 0.8362322200035552    steps: 249    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 967   score: 3.0   memory length: 182921   epsilon: 0.8358144400035643    steps: 211    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 968   score: 3.0   memory length: 183170   epsilon: 0.835321420003575    steps: 249    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 969   score: 0.0   memory length: 183292   epsilon: 0.8350798600035803    steps: 122    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 970   score: 1.0   memory length: 183461   epsilon: 0.8347452400035875    steps: 169    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 971   score: 3.0   memory length: 183689   epsilon: 0.8342938000035973    steps: 228    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 972   score: 2.0   memory length: 183889   epsilon: 0.8338978000036059    steps: 200    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 973   score: 4.0   memory length: 184146   epsilon: 0.833388940003617    steps: 257    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 974   score: 1.0   memory length: 184297   epsilon: 0.8330899600036235    steps: 151    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 975   score: 4.0   memory length: 184575   epsilon: 0.8325395200036354    steps: 278    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 976   score: 3.0   memory length: 184804   epsilon: 0.8320861000036452    steps: 229    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 977   score: 0.0   memory length: 184926   epsilon: 0.8318445400036505    steps: 122    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 978   score: 1.0   memory length: 185094   epsilon: 0.8315119000036577    steps: 168    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 979   score: 2.0   memory length: 185295   epsilon: 0.8311139200036664    steps: 201    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 980   score: 3.0   memory length: 185541   epsilon: 0.8306268400036769    steps: 246    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 981   score: 4.0   memory length: 185801   epsilon: 0.8301120400036881    steps: 260    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 982   score: 7.0   memory length: 186162   epsilon: 0.8293972600037036    steps: 361    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 983   score: 1.0   memory length: 186331   epsilon: 0.8290626400037109    steps: 169    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 984   score: 3.0   memory length: 186596   epsilon: 0.8285379400037223    steps: 265    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 985   score: 0.0   memory length: 186718   epsilon: 0.8282963800037275    steps: 122    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 986   score: 2.0   memory length: 186937   epsilon: 0.8278627600037369    steps: 219    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 987   score: 1.0   memory length: 187088   epsilon: 0.8275637800037434    steps: 151    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 988   score: 2.0   memory length: 187306   epsilon: 0.8271321400037528    steps: 218    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 989   score: 3.0   memory length: 187515   epsilon: 0.8267183200037618    steps: 209    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 990   score: 4.0   memory length: 187810   epsilon: 0.8261342200037745    steps: 295    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 991   score: 1.0   memory length: 187980   epsilon: 0.8257976200037818    steps: 170    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 992   score: 3.0   memory length: 188243   epsilon: 0.8252768800037931    steps: 263    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 993   score: 2.0   memory length: 188423   epsilon: 0.8249204800038008    steps: 180    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 994   score: 2.0   memory length: 188621   epsilon: 0.8245284400038093    steps: 198    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 995   score: 5.0   memory length: 188924   epsilon: 0.8239285000038223    steps: 303    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 996   score: 2.0   memory length: 189140   epsilon: 0.8235008200038316    steps: 216    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 997   score: 4.0   memory length: 189417   epsilon: 0.8229523600038435    steps: 277    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 998   score: 2.0   memory length: 189635   epsilon: 0.8225207200038529    steps: 218    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 999   score: 3.0   memory length: 189844   epsilon: 0.8221069000038619    steps: 209    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1000   score: 5.0   memory length: 190185   epsilon: 0.8214317200038765    steps: 341    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1001   score: 1.0   memory length: 190354   epsilon: 0.8210971000038838    steps: 169    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1002   score: 2.0   memory length: 190570   epsilon: 0.8206694200038931    steps: 216    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1003   score: 1.0   memory length: 190739   epsilon: 0.8203348000039004    steps: 169    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1004   score: 2.0   memory length: 190937   epsilon: 0.8199427600039089    steps: 198    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1005   score: 5.0   memory length: 191285   epsilon: 0.8192537200039238    steps: 348    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1006   score: 3.0   memory length: 191531   epsilon: 0.8187666400039344    steps: 246    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 1007   score: 5.0   memory length: 191839   epsilon: 0.8181568000039476    steps: 308    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1008   score: 4.0   memory length: 192136   epsilon: 0.8175687400039604    steps: 297    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1009   score: 4.0   memory length: 192410   epsilon: 0.8170262200039722    steps: 274    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1010   score: 3.0   memory length: 192679   epsilon: 0.8164936000039837    steps: 269    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1011   score: 3.0   memory length: 192927   epsilon: 0.8160025600039944    steps: 248    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1012   score: 4.0   memory length: 193204   epsilon: 0.8154541000040063    steps: 277    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1013   score: 2.0   memory length: 193402   epsilon: 0.8150620600040148    steps: 198    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1014   score: 3.0   memory length: 193650   epsilon: 0.8145710200040255    steps: 248    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1015   score: 1.0   memory length: 193800   epsilon: 0.8142740200040319    steps: 150    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1016   score: 3.0   memory length: 194026   epsilon: 0.8138265400040416    steps: 226    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1017   score: 2.0   memory length: 194243   epsilon: 0.813396880004051    steps: 217    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1018   score: 4.0   memory length: 194517   epsilon: 0.8128543600040627    steps: 274    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1019   score: 6.0   memory length: 194881   epsilon: 0.8121336400040784    steps: 364    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1020   score: 5.0   memory length: 195190   epsilon: 0.8115218200040917    steps: 309    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1021   score: 4.0   memory length: 195485   epsilon: 0.8109377200041044    steps: 295    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1022   score: 2.0   memory length: 195683   epsilon: 0.8105456800041129    steps: 198    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1023   score: 2.0   memory length: 195863   epsilon: 0.8101892800041206    steps: 180    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1024   score: 2.0   memory length: 196061   epsilon: 0.8097972400041291    steps: 198    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1025   score: 5.0   memory length: 196369   epsilon: 0.8091874000041424    steps: 308    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1026   score: 3.0   memory length: 196617   epsilon: 0.808696360004153    steps: 248    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1027   score: 1.0   memory length: 196768   epsilon: 0.8083973800041595    steps: 151    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1028   score: 2.0   memory length: 196987   epsilon: 0.8079637600041689    steps: 219    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 1029   score: 2.0   memory length: 197184   epsilon: 0.8075737000041774    steps: 197    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1030   score: 1.0   memory length: 197335   epsilon: 0.8072747200041839    steps: 151    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1031   score: 4.0   memory length: 197630   epsilon: 0.8066906200041966    steps: 295    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1032   score: 2.0   memory length: 197846   epsilon: 0.8062629400042058    steps: 216    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1033   score: 3.0   memory length: 198093   epsilon: 0.8057738800042165    steps: 247    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1034   score: 3.0   memory length: 198322   epsilon: 0.8053204600042263    steps: 229    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1035   score: 4.0   memory length: 198599   epsilon: 0.8047720000042382    steps: 277    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 1036   score: 4.0   memory length: 198893   epsilon: 0.8041898800042508    steps: 294    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1037   score: 5.0   memory length: 199199   epsilon: 0.803584000004264    steps: 306    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1038   score: 0.0   memory length: 199322   epsilon: 0.8033404600042693    steps: 123    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1039   score: 0.0   memory length: 199445   epsilon: 0.8030969200042746    steps: 123    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1040   score: 1.0   memory length: 199596   epsilon: 0.8027979400042811    steps: 151    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1041   score: 2.0   memory length: 199814   epsilon: 0.8023663000042904    steps: 218    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1042   score: 4.0   memory length: 200088   epsilon: 0.8018237800043022    steps: 274    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1043   score: 1.0   memory length: 200256   epsilon: 0.8014911400043094    steps: 168    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1044   score: 4.0   memory length: 200533   epsilon: 0.8009426800043213    steps: 277    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1045   score: 3.0   memory length: 200763   epsilon: 0.8004872800043312    steps: 230    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1046   score: 8.0   memory length: 201045   epsilon: 0.7999289200043433    steps: 282    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1047   score: 4.0   memory length: 201323   epsilon: 0.7993784800043553    steps: 278    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1048   score: 1.0   memory length: 201494   epsilon: 0.7990399000043626    steps: 171    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1049   score: 2.0   memory length: 201713   epsilon: 0.7986062800043721    steps: 219    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1050   score: 2.0   memory length: 201913   epsilon: 0.7982102800043807    steps: 200    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1051   score: 2.0   memory length: 202093   epsilon: 0.7978538800043884    steps: 180    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1052   score: 1.0   memory length: 202244   epsilon: 0.7975549000043949    steps: 151    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1053   score: 5.0   memory length: 202571   epsilon: 0.7969074400044089    steps: 327    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1054   score: 0.0   memory length: 202694   epsilon: 0.7966639000044142    steps: 123    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1055   score: 6.0   memory length: 203067   epsilon: 0.7959253600044303    steps: 373    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1056   score: 3.0   memory length: 203314   epsilon: 0.7954363000044409    steps: 247    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1057   score: 2.0   memory length: 203512   epsilon: 0.7950442600044494    steps: 198    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1058   score: 2.0   memory length: 203710   epsilon: 0.7946522200044579    steps: 198    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1059   score: 3.0   memory length: 203942   epsilon: 0.7941928600044679    steps: 232    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1060   score: 3.0   memory length: 204169   epsilon: 0.7937434000044776    steps: 227    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1061   score: 0.0   memory length: 204292   epsilon: 0.7934998600044829    steps: 123    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1062   score: 5.0   memory length: 204639   epsilon: 0.7928128000044978    steps: 347    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1063   score: 2.0   memory length: 204855   epsilon: 0.7923851200045071    steps: 216    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1064   score: 3.0   memory length: 205081   epsilon: 0.7919376400045168    steps: 226    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1065   score: 4.0   memory length: 205342   epsilon: 0.791420860004528    steps: 261    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1066   score: 1.0   memory length: 205493   epsilon: 0.7911218800045345    steps: 151    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1067   score: 6.0   memory length: 205883   epsilon: 0.7903496800045513    steps: 390    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1068   score: 4.0   memory length: 206159   epsilon: 0.7898032000045632    steps: 276    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1069   score: 3.0   memory length: 206426   epsilon: 0.7892745400045746    steps: 267    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1070   score: 3.0   memory length: 206653   epsilon: 0.7888250800045844    steps: 227    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1071   score: 2.0   memory length: 206873   epsilon: 0.7883894800045939    steps: 220    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1072   score: 1.0   memory length: 207044   epsilon: 0.7880509000046012    steps: 171    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1073   score: 1.0   memory length: 207195   epsilon: 0.7877519200046077    steps: 151    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1074   score: 4.0   memory length: 207472   epsilon: 0.7872034600046196    steps: 277    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1075   score: 5.0   memory length: 207817   epsilon: 0.7865203600046344    steps: 345    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1076   score: 4.0   memory length: 208094   epsilon: 0.7859719000046463    steps: 277    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1077   score: 2.0   memory length: 208294   epsilon: 0.7855759000046549    steps: 200    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1078   score: 6.0   memory length: 208679   epsilon: 0.7848136000046715    steps: 385    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1079   score: 3.0   memory length: 208909   epsilon: 0.7843582000046814    steps: 230    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1080   score: 3.0   memory length: 209156   epsilon: 0.783869140004692    steps: 247    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1081   score: 3.0   memory length: 209400   epsilon: 0.7833860200047025    steps: 244    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1082   score: 7.0   memory length: 209786   epsilon: 0.7826217400047191    steps: 386    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1083   score: 8.0   memory length: 210280   epsilon: 0.7816436200047403    steps: 494    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1084   score: 3.0   memory length: 210506   epsilon: 0.78119614000475    steps: 226    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1085   score: 0.0   memory length: 210629   epsilon: 0.7809526000047553    steps: 123    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1086   score: 2.0   memory length: 210827   epsilon: 0.7805605600047638    steps: 198    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1087   score: 2.0   memory length: 211025   epsilon: 0.7801685200047723    steps: 198    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1088   score: 6.0   memory length: 211367   epsilon: 0.779491360004787    steps: 342    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1089   score: 2.0   memory length: 211547   epsilon: 0.7791349600047948    steps: 180    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1090   score: 4.0   memory length: 211807   epsilon: 0.7786201600048059    steps: 260    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1091   score: 3.0   memory length: 212036   epsilon: 0.7781667400048158    steps: 229    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1092   score: 7.0   memory length: 212432   epsilon: 0.7773826600048328    steps: 396    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1093   score: 5.0   memory length: 212755   epsilon: 0.7767431200048467    steps: 323    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1094   score: 1.0   memory length: 212907   epsilon: 0.7764421600048532    steps: 152    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1095   score: 4.0   memory length: 213183   epsilon: 0.7758956800048651    steps: 276    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1096   score: 2.0   memory length: 213403   epsilon: 0.7754600800048745    steps: 220    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1097   score: 3.0   memory length: 213632   epsilon: 0.7750066600048844    steps: 229    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1098   score: 1.0   memory length: 213784   epsilon: 0.7747057000048909    steps: 152    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1099   score: 2.0   memory length: 213965   epsilon: 0.7743473200048987    steps: 181    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1100   score: 3.0   memory length: 214211   epsilon: 0.7738602400049093    steps: 246    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1101   score: 2.0   memory length: 214427   epsilon: 0.7734325600049186    steps: 216    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1102   score: 1.0   memory length: 214598   epsilon: 0.7730939800049259    steps: 171    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1103   score: 0.0   memory length: 214720   epsilon: 0.7728524200049312    steps: 122    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1104   score: 6.0   memory length: 215078   epsilon: 0.7721435800049465    steps: 358    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1105   score: 3.0   memory length: 215307   epsilon: 0.7716901600049564    steps: 229    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1106   score: 1.0   memory length: 215458   epsilon: 0.7713911800049629    steps: 151    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1107   score: 1.0   memory length: 215628   epsilon: 0.7710545800049702    steps: 170    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1108   score: 3.0   memory length: 215854   epsilon: 0.7706071000049799    steps: 226    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1109   score: 1.0   memory length: 216022   epsilon: 0.7702744600049871    steps: 168    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1110   score: 5.0   memory length: 216367   epsilon: 0.769591360005002    steps: 345    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1111   score: 5.0   memory length: 216675   epsilon: 0.7689815200050152    steps: 308    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1112   score: 6.0   memory length: 217006   epsilon: 0.7683261400050294    steps: 331    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1113   score: 4.0   memory length: 217303   epsilon: 0.7677380800050422    steps: 297    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1114   score: 0.0   memory length: 217426   epsilon: 0.7674945400050475    steps: 123    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1115   score: 1.0   memory length: 217595   epsilon: 0.7671599200050547    steps: 169    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1116   score: 6.0   memory length: 217939   epsilon: 0.7664788000050695    steps: 344    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1117   score: 2.0   memory length: 218155   epsilon: 0.7660511200050788    steps: 216    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1118   score: 6.0   memory length: 218468   epsilon: 0.7654313800050923    steps: 313    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1119   score: 0.0   memory length: 218591   epsilon: 0.7651878400050975    steps: 123    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1120   score: 3.0   memory length: 218817   epsilon: 0.7647403600051073    steps: 226    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1121   score: 5.0   memory length: 219127   epsilon: 0.7641265600051206    steps: 310    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1122   score: 5.0   memory length: 219454   epsilon: 0.7634791000051346    steps: 327    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1123   score: 1.0   memory length: 219624   epsilon: 0.7631425000051419    steps: 170    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1124   score: 9.0   memory length: 219931   epsilon: 0.7625346400051551    steps: 307    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1125   score: 5.0   memory length: 220275   epsilon: 0.7618535200051699    steps: 344    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1126   score: 0.0   memory length: 220398   epsilon: 0.7616099800051752    steps: 123    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1127   score: 6.0   memory length: 220757   epsilon: 0.7608991600051906    steps: 359    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1128   score: 6.0   memory length: 221081   epsilon: 0.7602576400052046    steps: 324    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1129   score: 3.0   memory length: 221348   epsilon: 0.759728980005216    steps: 267    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1130   score: 3.0   memory length: 221577   epsilon: 0.7592755600052259    steps: 229    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1131   score: 2.0   memory length: 221775   epsilon: 0.7588835200052344    steps: 198    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1132   score: 3.0   memory length: 222023   epsilon: 0.7583924800052451    steps: 248    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1133   score: 4.0   memory length: 222298   epsilon: 0.7578479800052569    steps: 275    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1134   score: 2.0   memory length: 222497   epsilon: 0.7574539600052654    steps: 199    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1135   score: 2.0   memory length: 222695   epsilon: 0.757061920005274    steps: 198    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1136   score: 2.0   memory length: 222895   epsilon: 0.7566659200052825    steps: 200    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1137   score: 2.0   memory length: 223093   epsilon: 0.756273880005291    steps: 198    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1138   score: 2.0   memory length: 223292   epsilon: 0.7558798600052996    steps: 199    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1139   score: 6.0   memory length: 223627   epsilon: 0.755216560005314    steps: 335    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1140   score: 0.0   memory length: 223750   epsilon: 0.7549730200053193    steps: 123    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1141   score: 3.0   memory length: 223976   epsilon: 0.754525540005329    steps: 226    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1142   score: 4.0   memory length: 224254   epsilon: 0.753975100005341    steps: 278    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1143   score: 1.0   memory length: 224425   epsilon: 0.7536365200053483    steps: 171    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1144   score: 3.0   memory length: 224654   epsilon: 0.7531831000053582    steps: 229    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1145   score: 4.0   memory length: 224964   epsilon: 0.7525693000053715    steps: 310    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1146   score: 6.0   memory length: 225355   epsilon: 0.7517951200053883    steps: 391    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1147   score: 7.0   memory length: 225762   epsilon: 0.7509892600054058    steps: 407    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1148   score: 7.0   memory length: 226124   epsilon: 0.7502725000054213    steps: 362    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1149   score: 2.0   memory length: 226339   epsilon: 0.7498468000054306    steps: 215    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1150   score: 5.0   memory length: 226662   epsilon: 0.7492072600054445    steps: 323    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1151   score: 3.0   memory length: 226908   epsilon: 0.748720180005455    steps: 246    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1152   score: 2.0   memory length: 227088   epsilon: 0.7483637800054628    steps: 180    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1153   score: 1.0   memory length: 227258   epsilon: 0.7480271800054701    steps: 170    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1154   score: 4.0   memory length: 227533   epsilon: 0.7474826800054819    steps: 275    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1155   score: 3.0   memory length: 227759   epsilon: 0.7470352000054916    steps: 226    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1156   score: 1.0   memory length: 227910   epsilon: 0.7467362200054981    steps: 151    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1157   score: 3.0   memory length: 228158   epsilon: 0.7462451800055088    steps: 248    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1158   score: 1.0   memory length: 228330   epsilon: 0.7459046200055162    steps: 172    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1159   score: 5.0   memory length: 228638   epsilon: 0.7452947800055294    steps: 308    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1160   score: 2.0   memory length: 228856   epsilon: 0.7448631400055388    steps: 218    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1161   score: 3.0   memory length: 229086   epsilon: 0.7444077400055487    steps: 230    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1162   score: 4.0   memory length: 229384   epsilon: 0.7438177000055615    steps: 298    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1163   score: 1.0   memory length: 229536   epsilon: 0.743516740005568    steps: 152    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1164   score: 5.0   memory length: 229876   epsilon: 0.7428435400055826    steps: 340    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1165   score: 6.0   memory length: 230199   epsilon: 0.7422040000055965    steps: 323    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1166   score: 3.0   memory length: 230425   epsilon: 0.7417565200056062    steps: 226    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1167   score: 1.0   memory length: 230575   epsilon: 0.7414595200056127    steps: 150    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1168   score: 2.0   memory length: 230776   epsilon: 0.7410615400056213    steps: 201    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1169   score: 5.0   memory length: 231101   epsilon: 0.7404180400056353    steps: 325    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1170   score: 4.0   memory length: 231381   epsilon: 0.7398636400056473    steps: 280    lr: 4e-05     evaluation reward: 3.24\n",
      "episode: 1171   score: 10.0   memory length: 231894   epsilon: 0.7388479000056694    steps: 513    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1172   score: 3.0   memory length: 232121   epsilon: 0.7383984400056791    steps: 227    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1173   score: 2.0   memory length: 232338   epsilon: 0.7379687800056884    steps: 217    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1174   score: 9.0   memory length: 232834   epsilon: 0.7369867000057098    steps: 496    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1175   score: 1.0   memory length: 232985   epsilon: 0.7366877200057163    steps: 151    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1176   score: 5.0   memory length: 233282   epsilon: 0.736099660005729    steps: 297    lr: 4e-05     evaluation reward: 3.37\n",
      "episode: 1177   score: 6.0   memory length: 233653   epsilon: 0.735365080005745    steps: 371    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1178   score: 4.0   memory length: 233921   epsilon: 0.7348344400057565    steps: 268    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1179   score: 4.0   memory length: 234179   epsilon: 0.7343236000057676    steps: 258    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1180   score: 5.0   memory length: 234503   epsilon: 0.7336820800057815    steps: 324    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1181   score: 5.0   memory length: 234808   epsilon: 0.7330781800057946    steps: 305    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1182   score: 1.0   memory length: 234959   epsilon: 0.7327792000058011    steps: 151    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1183   score: 4.0   memory length: 235252   epsilon: 0.7321990600058137    steps: 293    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1184   score: 5.0   memory length: 235555   epsilon: 0.7315991200058267    steps: 303    lr: 4e-05     evaluation reward: 3.36\n",
      "episode: 1185   score: 6.0   memory length: 235878   epsilon: 0.7309595800058406    steps: 323    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1186   score: 4.0   memory length: 236154   epsilon: 0.7304131000058525    steps: 276    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1187   score: 0.0   memory length: 236277   epsilon: 0.7301695600058578    steps: 123    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1188   score: 4.0   memory length: 236536   epsilon: 0.7296567400058689    steps: 259    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1189   score: 2.0   memory length: 236736   epsilon: 0.7292607400058775    steps: 200    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1190   score: 5.0   memory length: 237026   epsilon: 0.72868654000589    steps: 290    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1191   score: 5.0   memory length: 237358   epsilon: 0.7280291800059042    steps: 332    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1192   score: 8.0   memory length: 237689   epsilon: 0.7273738000059184    steps: 331    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1193   score: 2.0   memory length: 237908   epsilon: 0.7269401800059279    steps: 219    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1194   score: 6.0   memory length: 238303   epsilon: 0.7261580800059448    steps: 395    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1195   score: 3.0   memory length: 238530   epsilon: 0.7257086200059546    steps: 227    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1196   score: 7.0   memory length: 238958   epsilon: 0.724861180005973    steps: 428    lr: 4e-05     evaluation reward: 3.5\n",
      "episode: 1197   score: 5.0   memory length: 239247   epsilon: 0.7242889600059854    steps: 289    lr: 4e-05     evaluation reward: 3.52\n",
      "episode: 1198   score: 7.0   memory length: 239637   epsilon: 0.7235167600060022    steps: 390    lr: 4e-05     evaluation reward: 3.58\n",
      "episode: 1199   score: 4.0   memory length: 239911   epsilon: 0.722974240006014    steps: 274    lr: 4e-05     evaluation reward: 3.6\n",
      "episode: 1200   score: 2.0   memory length: 240093   epsilon: 0.7226138800060218    steps: 182    lr: 4e-05     evaluation reward: 3.59\n",
      "episode: 1201   score: 7.0   memory length: 240483   epsilon: 0.7218416800060385    steps: 390    lr: 4e-05     evaluation reward: 3.64\n",
      "episode: 1202   score: 3.0   memory length: 240730   epsilon: 0.7213526200060492    steps: 247    lr: 4e-05     evaluation reward: 3.66\n",
      "episode: 1203   score: 3.0   memory length: 240959   epsilon: 0.720899200006059    steps: 229    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1204   score: 4.0   memory length: 241225   epsilon: 0.7203725200060704    steps: 266    lr: 4e-05     evaluation reward: 3.67\n",
      "episode: 1205   score: 5.0   memory length: 241550   epsilon: 0.7197290200060844    steps: 325    lr: 4e-05     evaluation reward: 3.69\n",
      "episode: 1206   score: 2.0   memory length: 241750   epsilon: 0.719333020006093    steps: 200    lr: 4e-05     evaluation reward: 3.7\n",
      "episode: 1207   score: 9.0   memory length: 242276   epsilon: 0.7182915400061156    steps: 526    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1208   score: 4.0   memory length: 242527   epsilon: 0.7177945600061264    steps: 251    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1209   score: 3.0   memory length: 242755   epsilon: 0.7173431200061362    steps: 228    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1210   score: 2.0   memory length: 242955   epsilon: 0.7169471200061448    steps: 200    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1211   score: 4.0   memory length: 243234   epsilon: 0.7163947000061568    steps: 279    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1212   score: 1.0   memory length: 243385   epsilon: 0.7160957200061633    steps: 151    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1213   score: 4.0   memory length: 243660   epsilon: 0.7155512200061751    steps: 275    lr: 4e-05     evaluation reward: 3.72\n",
      "episode: 1214   score: 1.0   memory length: 243828   epsilon: 0.7152185800061823    steps: 168    lr: 4e-05     evaluation reward: 3.73\n",
      "episode: 1215   score: 5.0   memory length: 244171   epsilon: 0.7145394400061971    steps: 343    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1216   score: 4.0   memory length: 244445   epsilon: 0.7139969200062088    steps: 274    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1217   score: 2.0   memory length: 244644   epsilon: 0.7136029000062174    steps: 199    lr: 4e-05     evaluation reward: 3.75\n",
      "episode: 1218   score: 8.0   memory length: 245089   epsilon: 0.7127218000062365    steps: 445    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1219   score: 3.0   memory length: 245337   epsilon: 0.7122307600062472    steps: 248    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1220   score: 5.0   memory length: 245662   epsilon: 0.7115872600062612    steps: 325    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1221   score: 2.0   memory length: 245862   epsilon: 0.7111912600062698    steps: 200    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1222   score: 3.0   memory length: 246074   epsilon: 0.7107715000062789    steps: 212    lr: 4e-05     evaluation reward: 3.77\n",
      "episode: 1223   score: 4.0   memory length: 246371   epsilon: 0.7101834400062916    steps: 297    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1224   score: 8.0   memory length: 246815   epsilon: 0.7093043200063107    steps: 444    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1225   score: 5.0   memory length: 247141   epsilon: 0.7086588400063247    steps: 326    lr: 4e-05     evaluation reward: 3.79\n",
      "episode: 1226   score: 7.0   memory length: 247548   epsilon: 0.7078529800063422    steps: 407    lr: 4e-05     evaluation reward: 3.86\n",
      "episode: 1227   score: 1.0   memory length: 247717   epsilon: 0.7075183600063495    steps: 169    lr: 4e-05     evaluation reward: 3.81\n",
      "episode: 1228   score: 3.0   memory length: 247946   epsilon: 0.7070649400063593    steps: 229    lr: 4e-05     evaluation reward: 3.78\n",
      "episode: 1229   score: 5.0   memory length: 248218   epsilon: 0.706526380006371    steps: 272    lr: 4e-05     evaluation reward: 3.8\n",
      "episode: 1230   score: 5.0   memory length: 248549   epsilon: 0.7058710000063853    steps: 331    lr: 4e-05     evaluation reward: 3.82\n",
      "episode: 1231   score: 9.0   memory length: 249022   epsilon: 0.7049344600064056    steps: 473    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1232   score: 3.0   memory length: 249269   epsilon: 0.7044454000064162    steps: 247    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1233   score: 4.0   memory length: 249510   epsilon: 0.7039682200064266    steps: 241    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1234   score: 2.0   memory length: 249707   epsilon: 0.703578160006435    steps: 197    lr: 4e-05     evaluation reward: 3.89\n",
      "episode: 1235   score: 8.0   memory length: 250202   epsilon: 0.7025980600064563    steps: 495    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1236   score: 1.0   memory length: 250353   epsilon: 0.7022990800064628    steps: 151    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1237   score: 3.0   memory length: 250583   epsilon: 0.7018436800064727    steps: 230    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1238   score: 2.0   memory length: 250783   epsilon: 0.7014476800064813    steps: 200    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1239   score: 5.0   memory length: 251074   epsilon: 0.7008715000064938    steps: 291    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1240   score: 6.0   memory length: 251427   epsilon: 0.700172560006509    steps: 353    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1241   score: 5.0   memory length: 251753   epsilon: 0.699527080006523    steps: 326    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1242   score: 2.0   memory length: 251951   epsilon: 0.6991350400065315    steps: 198    lr: 4e-05     evaluation reward: 4.0\n",
      "episode: 1243   score: 3.0   memory length: 252178   epsilon: 0.6986855800065412    steps: 227    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1244   score: 5.0   memory length: 252507   epsilon: 0.6980341600065554    steps: 329    lr: 4e-05     evaluation reward: 4.04\n",
      "episode: 1245   score: 3.0   memory length: 252754   epsilon: 0.697545100006566    steps: 247    lr: 4e-05     evaluation reward: 4.03\n",
      "episode: 1246   score: 4.0   memory length: 253029   epsilon: 0.6970006000065778    steps: 275    lr: 4e-05     evaluation reward: 4.01\n",
      "episode: 1247   score: 4.0   memory length: 253329   epsilon: 0.6964066000065907    steps: 300    lr: 4e-05     evaluation reward: 3.98\n",
      "episode: 1248   score: 4.0   memory length: 253627   epsilon: 0.6958165600066035    steps: 298    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1249   score: 1.0   memory length: 253778   epsilon: 0.69551758000661    steps: 151    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1250   score: 5.0   memory length: 254123   epsilon: 0.6948344800066248    steps: 345    lr: 4e-05     evaluation reward: 3.94\n",
      "episode: 1251   score: 4.0   memory length: 254402   epsilon: 0.6942820600066368    steps: 279    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1252   score: 2.0   memory length: 254582   epsilon: 0.6939256600066446    steps: 180    lr: 4e-05     evaluation reward: 3.95\n",
      "episode: 1253   score: 8.0   memory length: 255006   epsilon: 0.6930861400066628    steps: 424    lr: 4e-05     evaluation reward: 4.02\n",
      "episode: 1254   score: 8.0   memory length: 255427   epsilon: 0.6922525600066809    steps: 421    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1255   score: 3.0   memory length: 255658   epsilon: 0.6917951800066908    steps: 231    lr: 4e-05     evaluation reward: 4.06\n",
      "episode: 1256   score: 4.0   memory length: 255954   epsilon: 0.6912091000067035    steps: 296    lr: 4e-05     evaluation reward: 4.09\n",
      "episode: 1257   score: 2.0   memory length: 256171   epsilon: 0.6907794400067129    steps: 217    lr: 4e-05     evaluation reward: 4.08\n",
      "episode: 1258   score: 4.0   memory length: 256431   epsilon: 0.690264640006724    steps: 260    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1259   score: 5.0   memory length: 256755   epsilon: 0.689623120006738    steps: 324    lr: 4e-05     evaluation reward: 4.11\n",
      "episode: 1260   score: 6.0   memory length: 257128   epsilon: 0.688884580006754    steps: 373    lr: 4e-05     evaluation reward: 4.15\n",
      "episode: 1261   score: 5.0   memory length: 257433   epsilon: 0.6882806800067671    steps: 305    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1262   score: 5.0   memory length: 257781   epsilon: 0.6875916400067821    steps: 348    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1263   score: 2.0   memory length: 257961   epsilon: 0.6872352400067898    steps: 180    lr: 4e-05     evaluation reward: 4.19\n",
      "episode: 1264   score: 3.0   memory length: 258207   epsilon: 0.6867481600068004    steps: 246    lr: 4e-05     evaluation reward: 4.17\n",
      "episode: 1265   score: 7.0   memory length: 258653   epsilon: 0.6858650800068196    steps: 446    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1266   score: 3.0   memory length: 258879   epsilon: 0.6854176000068293    steps: 226    lr: 4e-05     evaluation reward: 4.18\n",
      "episode: 1267   score: 6.0   memory length: 259242   epsilon: 0.6846988600068449    steps: 363    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1268   score: 5.0   memory length: 259545   epsilon: 0.6840989200068579    steps: 303    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1269   score: 0.0   memory length: 259668   epsilon: 0.6838553800068632    steps: 123    lr: 4e-05     evaluation reward: 4.21\n",
      "episode: 1270   score: 6.0   memory length: 260061   epsilon: 0.6830772400068801    steps: 393    lr: 4e-05     evaluation reward: 4.23\n",
      "episode: 1271   score: 13.0   memory length: 260617   epsilon: 0.681976360006904    steps: 556    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1272   score: 8.0   memory length: 261028   epsilon: 0.6811625800069216    steps: 411    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1273   score: 5.0   memory length: 261359   epsilon: 0.6805072000069359    steps: 331    lr: 4e-05     evaluation reward: 4.34\n",
      "episode: 1274   score: 1.0   memory length: 261510   epsilon: 0.6802082200069424    steps: 151    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1275   score: 1.0   memory length: 261660   epsilon: 0.6799112200069488    steps: 150    lr: 4e-05     evaluation reward: 4.26\n",
      "episode: 1276   score: 8.0   memory length: 262115   epsilon: 0.6790103200069684    steps: 455    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1277   score: 2.0   memory length: 262295   epsilon: 0.6786539200069761    steps: 180    lr: 4e-05     evaluation reward: 4.25\n",
      "episode: 1278   score: 8.0   memory length: 262700   epsilon: 0.6778520200069935    steps: 405    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1279   score: 2.0   memory length: 262897   epsilon: 0.677461960007002    steps: 197    lr: 4e-05     evaluation reward: 4.27\n",
      "episode: 1280   score: 7.0   memory length: 263305   epsilon: 0.6766541200070195    steps: 408    lr: 4e-05     evaluation reward: 4.29\n",
      "episode: 1281   score: 7.0   memory length: 263692   epsilon: 0.6758878600070362    steps: 387    lr: 4e-05     evaluation reward: 4.31\n",
      "episode: 1282   score: 5.0   memory length: 264040   epsilon: 0.6751988200070511    steps: 348    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1283   score: 4.0   memory length: 264300   epsilon: 0.6746840200070623    steps: 260    lr: 4e-05     evaluation reward: 4.35\n",
      "episode: 1284   score: 2.0   memory length: 264498   epsilon: 0.6742919800070708    steps: 198    lr: 4e-05     evaluation reward: 4.32\n",
      "episode: 1285   score: 11.0   memory length: 265033   epsilon: 0.6732326800070938    steps: 535    lr: 4e-05     evaluation reward: 4.37\n",
      "episode: 1286   score: 9.0   memory length: 265510   epsilon: 0.6722882200071143    steps: 477    lr: 4e-05     evaluation reward: 4.42\n",
      "episode: 1287   score: 6.0   memory length: 265825   epsilon: 0.6716645200071278    steps: 315    lr: 4e-05     evaluation reward: 4.48\n",
      "episode: 1288   score: 5.0   memory length: 266129   epsilon: 0.6710626000071409    steps: 304    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1289   score: 1.0   memory length: 266280   epsilon: 0.6707636200071474    steps: 151    lr: 4e-05     evaluation reward: 4.48\n",
      "episode: 1290   score: 4.0   memory length: 266566   epsilon: 0.6701973400071597    steps: 286    lr: 4e-05     evaluation reward: 4.47\n",
      "episode: 1291   score: 5.0   memory length: 266892   epsilon: 0.6695518600071737    steps: 326    lr: 4e-05     evaluation reward: 4.47\n",
      "episode: 1292   score: 4.0   memory length: 267189   epsilon: 0.6689638000071865    steps: 297    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1293   score: 8.0   memory length: 267629   epsilon: 0.6680926000072054    steps: 440    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1294   score: 3.0   memory length: 267857   epsilon: 0.6676411600072152    steps: 228    lr: 4e-05     evaluation reward: 4.46\n",
      "episode: 1295   score: 1.0   memory length: 268009   epsilon: 0.6673402000072217    steps: 152    lr: 4e-05     evaluation reward: 4.44\n",
      "episode: 1296   score: 10.0   memory length: 268483   epsilon: 0.6664016800072421    steps: 474    lr: 4e-05     evaluation reward: 4.47\n",
      "episode: 1297   score: 4.0   memory length: 268761   epsilon: 0.665851240007254    steps: 278    lr: 4e-05     evaluation reward: 4.46\n",
      "episode: 1298   score: 4.0   memory length: 269025   epsilon: 0.6653285200072654    steps: 264    lr: 4e-05     evaluation reward: 4.43\n",
      "episode: 1299   score: 5.0   memory length: 269348   epsilon: 0.6646889800072793    steps: 323    lr: 4e-05     evaluation reward: 4.44\n",
      "episode: 1300   score: 5.0   memory length: 269639   epsilon: 0.6641128000072918    steps: 291    lr: 4e-05     evaluation reward: 4.47\n",
      "episode: 1301   score: 5.0   memory length: 269946   epsilon: 0.663504940007305    steps: 307    lr: 4e-05     evaluation reward: 4.45\n",
      "episode: 1302   score: 5.0   memory length: 270268   epsilon: 0.6628673800073188    steps: 322    lr: 4e-05     evaluation reward: 4.47\n",
      "episode: 1303   score: 4.0   memory length: 270584   epsilon: 0.6622417000073324    steps: 316    lr: 4e-05     evaluation reward: 4.48\n",
      "episode: 1304   score: 7.0   memory length: 270999   epsilon: 0.6614200000073502    steps: 415    lr: 4e-05     evaluation reward: 4.51\n",
      "episode: 1305   score: 5.0   memory length: 271303   epsilon: 0.6608180800073633    steps: 304    lr: 4e-05     evaluation reward: 4.51\n",
      "episode: 1306   score: 2.0   memory length: 271500   epsilon: 0.6604280200073718    steps: 197    lr: 4e-05     evaluation reward: 4.51\n",
      "episode: 1307   score: 7.0   memory length: 271902   epsilon: 0.659632060007389    steps: 402    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1308   score: 4.0   memory length: 272198   epsilon: 0.6590459800074018    steps: 296    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1309   score: 3.0   memory length: 272447   epsilon: 0.6585529600074125    steps: 249    lr: 4e-05     evaluation reward: 4.49\n",
      "episode: 1310   score: 1.0   memory length: 272600   epsilon: 0.6582500200074191    steps: 153    lr: 4e-05     evaluation reward: 4.48\n",
      "episode: 1311   score: 9.0   memory length: 273090   epsilon: 0.6572798200074401    steps: 490    lr: 4e-05     evaluation reward: 4.53\n",
      "episode: 1312   score: 5.0   memory length: 273411   epsilon: 0.6566442400074539    steps: 321    lr: 4e-05     evaluation reward: 4.57\n",
      "episode: 1313   score: 6.0   memory length: 273790   epsilon: 0.6558938200074702    steps: 379    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1314   score: 3.0   memory length: 274036   epsilon: 0.6554067400074808    steps: 246    lr: 4e-05     evaluation reward: 4.61\n",
      "episode: 1315   score: 4.0   memory length: 274334   epsilon: 0.6548167000074936    steps: 298    lr: 4e-05     evaluation reward: 4.6\n",
      "episode: 1316   score: 3.0   memory length: 274564   epsilon: 0.6543613000075035    steps: 230    lr: 4e-05     evaluation reward: 4.59\n",
      "episode: 1317   score: 4.0   memory length: 274841   epsilon: 0.6538128400075154    steps: 277    lr: 4e-05     evaluation reward: 4.61\n",
      "episode: 1318   score: 3.0   memory length: 275070   epsilon: 0.6533594200075252    steps: 229    lr: 4e-05     evaluation reward: 4.56\n",
      "episode: 1319   score: 4.0   memory length: 275369   epsilon: 0.6527674000075381    steps: 299    lr: 4e-05     evaluation reward: 4.57\n",
      "episode: 1320   score: 3.0   memory length: 275579   epsilon: 0.6523516000075471    steps: 210    lr: 4e-05     evaluation reward: 4.55\n",
      "episode: 1321   score: 8.0   memory length: 276041   epsilon: 0.651436840007567    steps: 462    lr: 4e-05     evaluation reward: 4.61\n",
      "episode: 1322   score: 3.0   memory length: 276252   epsilon: 0.651019060007576    steps: 211    lr: 4e-05     evaluation reward: 4.61\n",
      "episode: 1323   score: 8.0   memory length: 276702   epsilon: 0.6501280600075954    steps: 450    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1324   score: 8.0   memory length: 277162   epsilon: 0.6492172600076151    steps: 460    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1325   score: 6.0   memory length: 277534   epsilon: 0.6484807000076311    steps: 372    lr: 4e-05     evaluation reward: 4.66\n",
      "episode: 1326   score: 6.0   memory length: 277875   epsilon: 0.6478055200076458    steps: 341    lr: 4e-05     evaluation reward: 4.65\n",
      "episode: 1327   score: 7.0   memory length: 278265   epsilon: 0.6470333200076626    steps: 390    lr: 4e-05     evaluation reward: 4.71\n",
      "episode: 1328   score: 4.0   memory length: 278541   epsilon: 0.6464868400076744    steps: 276    lr: 4e-05     evaluation reward: 4.72\n",
      "episode: 1329   score: 5.0   memory length: 278851   epsilon: 0.6458730400076877    steps: 310    lr: 4e-05     evaluation reward: 4.72\n",
      "episode: 1330   score: 4.0   memory length: 279125   epsilon: 0.6453305200076995    steps: 274    lr: 4e-05     evaluation reward: 4.71\n",
      "episode: 1331   score: 2.0   memory length: 279305   epsilon: 0.6449741200077073    steps: 180    lr: 4e-05     evaluation reward: 4.64\n",
      "episode: 1332   score: 6.0   memory length: 279639   epsilon: 0.6443128000077216    steps: 334    lr: 4e-05     evaluation reward: 4.67\n",
      "episode: 1333   score: 6.0   memory length: 279962   epsilon: 0.6436732600077355    steps: 323    lr: 4e-05     evaluation reward: 4.69\n",
      "episode: 1334   score: 7.0   memory length: 280359   epsilon: 0.6428872000077526    steps: 397    lr: 4e-05     evaluation reward: 4.74\n",
      "episode: 1335   score: 10.0   memory length: 280887   epsilon: 0.6418417600077753    steps: 528    lr: 4e-05     evaluation reward: 4.76\n",
      "episode: 1336   score: 8.0   memory length: 281342   epsilon: 0.6409408600077948    steps: 455    lr: 4e-05     evaluation reward: 4.83\n",
      "episode: 1337   score: 4.0   memory length: 281582   epsilon: 0.6404656600078051    steps: 240    lr: 4e-05     evaluation reward: 4.84\n",
      "episode: 1338   score: 7.0   memory length: 282006   epsilon: 0.6396261400078234    steps: 424    lr: 4e-05     evaluation reward: 4.89\n",
      "episode: 1339   score: 3.0   memory length: 282233   epsilon: 0.6391766800078331    steps: 227    lr: 4e-05     evaluation reward: 4.87\n",
      "episode: 1340   score: 9.0   memory length: 282716   epsilon: 0.6382203400078539    steps: 483    lr: 4e-05     evaluation reward: 4.9\n",
      "episode: 1341   score: 1.0   memory length: 282887   epsilon: 0.6378817600078612    steps: 171    lr: 4e-05     evaluation reward: 4.86\n",
      "episode: 1342   score: 8.0   memory length: 283332   epsilon: 0.6370006600078804    steps: 445    lr: 4e-05     evaluation reward: 4.92\n",
      "episode: 1343   score: 8.0   memory length: 283773   epsilon: 0.6361274800078993    steps: 441    lr: 4e-05     evaluation reward: 4.97\n",
      "episode: 1344   score: 10.0   memory length: 284304   epsilon: 0.6350761000079221    steps: 531    lr: 4e-05     evaluation reward: 5.02\n",
      "episode: 1345   score: 2.0   memory length: 284485   epsilon: 0.6347177200079299    steps: 181    lr: 4e-05     evaluation reward: 5.01\n",
      "episode: 1346   score: 6.0   memory length: 284824   epsilon: 0.6340465000079445    steps: 339    lr: 4e-05     evaluation reward: 5.03\n",
      "episode: 1347   score: 7.0   memory length: 285210   epsilon: 0.6332822200079611    steps: 386    lr: 4e-05     evaluation reward: 5.06\n",
      "episode: 1348   score: 4.0   memory length: 285503   epsilon: 0.6327020800079737    steps: 293    lr: 4e-05     evaluation reward: 5.06\n",
      "episode: 1349   score: 9.0   memory length: 285991   epsilon: 0.6317358400079947    steps: 488    lr: 4e-05     evaluation reward: 5.14\n",
      "episode: 1350   score: 6.0   memory length: 286346   epsilon: 0.6310329400080099    steps: 355    lr: 4e-05     evaluation reward: 5.15\n",
      "episode: 1351   score: 6.0   memory length: 286717   epsilon: 0.6302983600080259    steps: 371    lr: 4e-05     evaluation reward: 5.17\n",
      "episode: 1352   score: 7.0   memory length: 287107   epsilon: 0.6295261600080426    steps: 390    lr: 4e-05     evaluation reward: 5.22\n",
      "episode: 1353   score: 0.0   memory length: 287230   epsilon: 0.6292826200080479    steps: 123    lr: 4e-05     evaluation reward: 5.14\n",
      "episode: 1354   score: 7.0   memory length: 287635   epsilon: 0.6284807200080653    steps: 405    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1355   score: 2.0   memory length: 287815   epsilon: 0.6281243200080731    steps: 180    lr: 4e-05     evaluation reward: 5.12\n",
      "episode: 1356   score: 4.0   memory length: 288115   epsilon: 0.627530320008086    steps: 300    lr: 4e-05     evaluation reward: 5.12\n",
      "episode: 1357   score: 9.0   memory length: 288563   epsilon: 0.6266432800081052    steps: 448    lr: 4e-05     evaluation reward: 5.19\n",
      "episode: 1358   score: 9.0   memory length: 288872   epsilon: 0.6260314600081185    steps: 309    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1359   score: 7.0   memory length: 289295   epsilon: 0.6251939200081367    steps: 423    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1360   score: 7.0   memory length: 289664   epsilon: 0.6244633000081525    steps: 369    lr: 4e-05     evaluation reward: 5.27\n",
      "episode: 1361   score: 3.0   memory length: 289911   epsilon: 0.6239742400081632    steps: 247    lr: 4e-05     evaluation reward: 5.25\n",
      "episode: 1362   score: 6.0   memory length: 290283   epsilon: 0.6232376800081791    steps: 372    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1363   score: 7.0   memory length: 290659   epsilon: 0.6224932000081953    steps: 376    lr: 4e-05     evaluation reward: 5.31\n",
      "episode: 1364   score: 4.0   memory length: 290936   epsilon: 0.6219447400082072    steps: 277    lr: 4e-05     evaluation reward: 5.32\n",
      "episode: 1365   score: 4.0   memory length: 291216   epsilon: 0.6213903400082192    steps: 280    lr: 4e-05     evaluation reward: 5.29\n",
      "episode: 1366   score: 3.0   memory length: 291445   epsilon: 0.6209369200082291    steps: 229    lr: 4e-05     evaluation reward: 5.29\n",
      "episode: 1367   score: 3.0   memory length: 291691   epsilon: 0.6204498400082397    steps: 246    lr: 4e-05     evaluation reward: 5.26\n",
      "episode: 1368   score: 3.0   memory length: 291920   epsilon: 0.6199964200082495    steps: 229    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1369   score: 3.0   memory length: 292145   epsilon: 0.6195509200082592    steps: 225    lr: 4e-05     evaluation reward: 5.27\n",
      "episode: 1370   score: 3.0   memory length: 292374   epsilon: 0.619097500008269    steps: 229    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1371   score: 5.0   memory length: 292682   epsilon: 0.6184876600082823    steps: 308    lr: 4e-05     evaluation reward: 5.16\n",
      "episode: 1372   score: 8.0   memory length: 293073   epsilon: 0.6177134800082991    steps: 391    lr: 4e-05     evaluation reward: 5.16\n",
      "episode: 1373   score: 4.0   memory length: 293332   epsilon: 0.6172006600083102    steps: 259    lr: 4e-05     evaluation reward: 5.15\n",
      "episode: 1374   score: 4.0   memory length: 293592   epsilon: 0.6166858600083214    steps: 260    lr: 4e-05     evaluation reward: 5.18\n",
      "episode: 1375   score: 4.0   memory length: 293850   epsilon: 0.6161750200083325    steps: 258    lr: 4e-05     evaluation reward: 5.21\n",
      "episode: 1376   score: 5.0   memory length: 294151   epsilon: 0.6155790400083454    steps: 301    lr: 4e-05     evaluation reward: 5.18\n",
      "episode: 1377   score: 5.0   memory length: 294440   epsilon: 0.6150068200083578    steps: 289    lr: 4e-05     evaluation reward: 5.21\n",
      "episode: 1378   score: 7.0   memory length: 294854   epsilon: 0.6141871000083756    steps: 414    lr: 4e-05     evaluation reward: 5.2\n",
      "episode: 1379   score: 6.0   memory length: 295234   epsilon: 0.613434700008392    steps: 380    lr: 4e-05     evaluation reward: 5.24\n",
      "episode: 1380   score: 4.0   memory length: 295513   epsilon: 0.612882280008404    steps: 279    lr: 4e-05     evaluation reward: 5.21\n",
      "episode: 1381   score: 9.0   memory length: 295985   epsilon: 0.6119477200084242    steps: 472    lr: 4e-05     evaluation reward: 5.23\n",
      "episode: 1382   score: 3.0   memory length: 296211   epsilon: 0.611500240008434    steps: 226    lr: 4e-05     evaluation reward: 5.21\n",
      "episode: 1383   score: 2.0   memory length: 296411   epsilon: 0.6111042400084425    steps: 200    lr: 4e-05     evaluation reward: 5.19\n",
      "episode: 1384   score: 6.0   memory length: 296736   epsilon: 0.6104607400084565    steps: 325    lr: 4e-05     evaluation reward: 5.23\n",
      "episode: 1385   score: 6.0   memory length: 297092   epsilon: 0.6097558600084718    steps: 356    lr: 4e-05     evaluation reward: 5.18\n",
      "episode: 1386   score: 4.0   memory length: 297370   epsilon: 0.6092054200084838    steps: 278    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1387   score: 7.0   memory length: 297796   epsilon: 0.6083619400085021    steps: 426    lr: 4e-05     evaluation reward: 5.14\n",
      "episode: 1388   score: 3.0   memory length: 298043   epsilon: 0.6078728800085127    steps: 247    lr: 4e-05     evaluation reward: 5.12\n",
      "episode: 1389   score: 2.0   memory length: 298242   epsilon: 0.6074788600085212    steps: 199    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1390   score: 4.0   memory length: 298517   epsilon: 0.6069343600085331    steps: 275    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1391   score: 5.0   memory length: 298826   epsilon: 0.6063225400085464    steps: 309    lr: 4e-05     evaluation reward: 5.13\n",
      "episode: 1392   score: 8.0   memory length: 299276   epsilon: 0.6054315400085657    steps: 450    lr: 4e-05     evaluation reward: 5.17\n",
      "episode: 1393   score: 7.0   memory length: 299680   epsilon: 0.6046316200085831    steps: 404    lr: 4e-05     evaluation reward: 5.16\n",
      "episode: 1394   score: 4.0   memory length: 299977   epsilon: 0.6040435600085958    steps: 297    lr: 4e-05     evaluation reward: 5.17\n",
      "episode: 1395   score: 10.0   memory length: 300370   epsilon: 0.6032654200086127    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1396   score: 3.0   memory length: 300597   epsilon: 0.6028159600086225    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1397   score: 7.0   memory length: 300964   epsilon: 0.6020893000086383    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1398   score: 11.0   memory length: 301514   epsilon: 0.6010003000086619    steps: 550    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1399   score: 8.0   memory length: 301909   epsilon: 0.6002182000086789    steps: 395    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1400   score: 4.0   memory length: 302183   epsilon: 0.5996756800086906    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.31\n",
      "episode: 1401   score: 2.0   memory length: 302363   epsilon: 0.5993192800086984    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1402   score: 4.0   memory length: 302617   epsilon: 0.5988163600087093    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
      "episode: 1403   score: 1.0   memory length: 302767   epsilon: 0.5985193600087158    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
      "episode: 1404   score: 4.0   memory length: 303046   epsilon: 0.5979669400087277    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1405   score: 6.0   memory length: 303402   epsilon: 0.597262060008743    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1406   score: 3.0   memory length: 303628   epsilon: 0.5968145800087528    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1407   score: 4.0   memory length: 303887   epsilon: 0.5963017600087639    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1408   score: 5.0   memory length: 304175   epsilon: 0.5957315200087763    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1409   score: 2.0   memory length: 304357   epsilon: 0.5953711600087841    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1410   score: 4.0   memory length: 304658   epsilon: 0.594775180008797    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1411   score: 6.0   memory length: 304994   epsilon: 0.5941099000088115    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1412   score: 5.0   memory length: 305320   epsilon: 0.5934644200088255    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1413   score: 8.0   memory length: 305794   epsilon: 0.5925259000088459    steps: 474    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1414   score: 1.0   memory length: 305945   epsilon: 0.5922269200088524    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1415   score: 4.0   memory length: 306187   epsilon: 0.5917477600088628    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1416   score: 4.0   memory length: 306465   epsilon: 0.5911973200088747    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.21\n",
      "episode: 1417   score: 3.0   memory length: 306674   epsilon: 0.5907835000088837    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.2\n",
      "episode: 1418   score: 5.0   memory length: 306978   epsilon: 0.5901815800088968    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 5.22\n",
      "episode: 1419   score: 8.0   memory length: 307383   epsilon: 0.5893796800089142    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1420   score: 6.0   memory length: 307708   epsilon: 0.5887361800089281    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1421   score: 5.0   memory length: 308035   epsilon: 0.5880887200089422    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.26\n",
      "episode: 1422   score: 12.0   memory length: 308618   epsilon: 0.5869343800089672    steps: 583    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1423   score: 10.0   memory length: 309115   epsilon: 0.5859503200089886    steps: 497    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1424   score: 6.0   memory length: 309453   epsilon: 0.5852810800090031    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1425   score: 7.0   memory length: 309818   epsilon: 0.5845583800090188    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1426   score: 7.0   memory length: 310226   epsilon: 0.5837505400090364    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 5.37\n",
      "episode: 1427   score: 8.0   memory length: 310679   epsilon: 0.5828536000090558    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 5.38\n",
      "episode: 1428   score: 8.0   memory length: 311152   epsilon: 0.5819170600090762    steps: 473    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1429   score: 11.0   memory length: 311727   epsilon: 0.5807785600091009    steps: 575    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1430   score: 4.0   memory length: 312005   epsilon: 0.5802281200091128    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 5.48\n",
      "episode: 1431   score: 4.0   memory length: 312245   epsilon: 0.5797529200091232    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 5.5\n",
      "episode: 1432   score: 10.0   memory length: 312795   epsilon: 0.5786639200091468    steps: 550    lr: 1.6000000000000003e-05     evaluation reward: 5.54\n",
      "episode: 1433   score: 4.0   memory length: 313088   epsilon: 0.5780837800091594    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 5.52\n",
      "episode: 1434   score: 6.0   memory length: 313464   epsilon: 0.5773393000091755    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 5.51\n",
      "episode: 1435   score: 5.0   memory length: 313808   epsilon: 0.5766581800091903    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.46\n",
      "episode: 1436   score: 4.0   memory length: 314083   epsilon: 0.5761136800092022    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1437   score: 3.0   memory length: 314294   epsilon: 0.5756959000092112    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1438   score: 7.0   memory length: 314703   epsilon: 0.5748860800092288    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 5.41\n",
      "episode: 1439   score: 6.0   memory length: 315049   epsilon: 0.5742010000092437    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1440   score: 4.0   memory length: 315342   epsilon: 0.5736208600092563    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1441   score: 4.0   memory length: 315602   epsilon: 0.5731060600092674    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1442   score: 5.0   memory length: 315928   epsilon: 0.5724605800092815    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1443   score: 5.0   memory length: 316227   epsilon: 0.5718685600092943    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1444   score: 6.0   memory length: 316584   epsilon: 0.5711617000093097    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1445   score: 10.0   memory length: 317046   epsilon: 0.5702469400093295    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 5.4\n",
      "episode: 1446   score: 10.0   memory length: 317533   epsilon: 0.5692826800093505    steps: 487    lr: 1.6000000000000003e-05     evaluation reward: 5.44\n",
      "episode: 1447   score: 5.0   memory length: 317841   epsilon: 0.5686728400093637    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.42\n",
      "episode: 1448   score: 7.0   memory length: 318246   epsilon: 0.5678709400093811    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1449   score: 3.0   memory length: 318479   epsilon: 0.5674096000093911    steps: 233    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1450   score: 3.0   memory length: 318725   epsilon: 0.5669225200094017    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1451   score: 2.0   memory length: 318906   epsilon: 0.5665641400094095    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1452   score: 3.0   memory length: 319137   epsilon: 0.5661067600094194    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.28\n",
      "episode: 1453   score: 7.0   memory length: 319563   epsilon: 0.5652632800094377    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.35\n",
      "episode: 1454   score: 2.0   memory length: 319763   epsilon: 0.5648672800094463    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1455   score: 4.0   memory length: 320044   epsilon: 0.5643109000094584    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 5.32\n",
      "episode: 1456   score: 8.0   memory length: 320510   epsilon: 0.5633882200094784    steps: 466    lr: 1.6000000000000003e-05     evaluation reward: 5.36\n",
      "episode: 1457   score: 3.0   memory length: 320737   epsilon: 0.5629387600094882    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 5.3\n",
      "episode: 1458   score: 2.0   memory length: 320935   epsilon: 0.5625467200094967    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 5.23\n",
      "episode: 1459   score: 2.0   memory length: 321155   epsilon: 0.5621111200095061    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1460   score: 4.0   memory length: 321429   epsilon: 0.5615686000095179    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 5.15\n",
      "episode: 1461   score: 7.0   memory length: 321806   epsilon: 0.5608221400095341    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 5.19\n",
      "episode: 1462   score: 5.0   memory length: 322117   epsilon: 0.5602063600095475    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 5.18\n",
      "episode: 1463   score: 13.0   memory length: 322664   epsilon: 0.559123300009571    steps: 547    lr: 1.6000000000000003e-05     evaluation reward: 5.24\n",
      "episode: 1464   score: 7.0   memory length: 323070   epsilon: 0.5583194200095885    steps: 406    lr: 1.6000000000000003e-05     evaluation reward: 5.27\n",
      "episode: 1465   score: 6.0   memory length: 323428   epsilon: 0.5576105800096038    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 5.29\n",
      "episode: 1466   score: 13.0   memory length: 323919   epsilon: 0.556638400009625    steps: 491    lr: 1.6000000000000003e-05     evaluation reward: 5.39\n",
      "episode: 1467   score: 7.0   memory length: 324369   epsilon: 0.5557474000096443    steps: 450    lr: 1.6000000000000003e-05     evaluation reward: 5.43\n",
      "episode: 1468   score: 5.0   memory length: 324691   epsilon: 0.5551098400096581    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 5.45\n",
      "episode: 1469   score: 11.0   memory length: 325151   epsilon: 0.5541990400096779    steps: 460    lr: 1.6000000000000003e-05     evaluation reward: 5.53\n",
      "episode: 1470   score: 7.0   memory length: 325555   epsilon: 0.5533991200096953    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1471   score: 5.0   memory length: 325897   epsilon: 0.55272196000971    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 5.57\n",
      "episode: 1472   score: 9.0   memory length: 326404   epsilon: 0.5517181000097318    steps: 507    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1473   score: 4.0   memory length: 326700   epsilon: 0.5511320200097445    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 5.58\n",
      "episode: 1474   score: 7.0   memory length: 327104   epsilon: 0.5503321000097618    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
      "episode: 1475   score: 4.0   memory length: 327363   epsilon: 0.549819280009773    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.61\n",
      "episode: 1476   score: 4.0   memory length: 327628   epsilon: 0.5492945800097844    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 5.6\n",
      "episode: 1477   score: 8.0   memory length: 328060   epsilon: 0.5484392200098029    steps: 432    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1478   score: 9.0   memory length: 328531   epsilon: 0.5475066400098232    steps: 471    lr: 1.6000000000000003e-05     evaluation reward: 5.65\n",
      "episode: 1479   score: 4.0   memory length: 328790   epsilon: 0.5469938200098343    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1480   score: 4.0   memory length: 329049   epsilon: 0.5464810000098455    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 5.63\n",
      "episode: 1481   score: 12.0   memory length: 329526   epsilon: 0.545536540009866    steps: 477    lr: 1.6000000000000003e-05     evaluation reward: 5.66\n",
      "episode: 1482   score: 20.0   memory length: 330114   epsilon: 0.5443723000098912    steps: 588    lr: 1.6000000000000003e-05     evaluation reward: 5.83\n",
      "episode: 1483   score: 7.0   memory length: 330518   epsilon: 0.5435723800099086    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1484   score: 3.0   memory length: 330764   epsilon: 0.5430853000099192    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1485   score: 3.0   memory length: 330976   epsilon: 0.5426655400099283    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1486   score: 8.0   memory length: 331380   epsilon: 0.5418656200099456    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1487   score: 6.0   memory length: 331731   epsilon: 0.5411706400099607    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1488   score: 7.0   memory length: 332087   epsilon: 0.540465760009976    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1489   score: 6.0   memory length: 332402   epsilon: 0.5398420600099896    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1490   score: 3.0   memory length: 332652   epsilon: 0.5393470600100003    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 5.92\n",
      "episode: 1491   score: 6.0   memory length: 332971   epsilon: 0.538715440010014    steps: 319    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1492   score: 6.0   memory length: 333343   epsilon: 0.53797888001003    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1493   score: 7.0   memory length: 333769   epsilon: 0.5371354000100483    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1494   score: 3.0   memory length: 333998   epsilon: 0.5366819800100582    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
      "episode: 1495   score: 6.0   memory length: 334349   epsilon: 0.5359870000100733    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1496   score: 6.0   memory length: 334745   epsilon: 0.5352029200100903    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1497   score: 7.0   memory length: 335118   epsilon: 0.5344643800101063    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1498   score: 7.0   memory length: 335531   epsilon: 0.5336466400101241    steps: 413    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1499   score: 3.0   memory length: 335760   epsilon: 0.5331932200101339    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1500   score: 4.0   memory length: 336018   epsilon: 0.532682380010145    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1501   score: 9.0   memory length: 336517   epsilon: 0.5316943600101665    steps: 499    lr: 1.6000000000000003e-05     evaluation reward: 5.87\n",
      "episode: 1502   score: 7.0   memory length: 336906   epsilon: 0.5309241400101832    steps: 389    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
      "episode: 1503   score: 6.0   memory length: 337278   epsilon: 0.5301875800101992    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1504   score: 3.0   memory length: 337509   epsilon: 0.5297302000102091    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.94\n",
      "episode: 1505   score: 3.0   memory length: 337738   epsilon: 0.5292767800102189    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1506   score: 1.0   memory length: 337889   epsilon: 0.5289778000102254    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1507   score: 6.0   memory length: 338248   epsilon: 0.5282669800102409    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 5.91\n",
      "episode: 1508   score: 6.0   memory length: 338620   epsilon: 0.5275304200102569    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 5.92\n",
      "episode: 1509   score: 5.0   memory length: 338947   epsilon: 0.5268829600102709    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1510   score: 6.0   memory length: 339291   epsilon: 0.5262018400102857    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
      "episode: 1511   score: 6.0   memory length: 339641   epsilon: 0.5255088400103007    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
      "episode: 1512   score: 4.0   memory length: 339916   epsilon: 0.5249643400103126    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 5.96\n",
      "episode: 1513   score: 4.0   memory length: 340176   epsilon: 0.5244495400103237    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 5.92\n",
      "episode: 1514   score: 11.0   memory length: 340734   epsilon: 0.5233447000103477    steps: 558    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1515   score: 4.0   memory length: 340991   epsilon: 0.5228358400103588    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1516   score: 4.0   memory length: 341273   epsilon: 0.5222774800103709    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1517   score: 4.0   memory length: 341532   epsilon: 0.521764660010382    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
      "episode: 1518   score: 8.0   memory length: 341988   epsilon: 0.5208617800104016    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1519   score: 5.0   memory length: 342318   epsilon: 0.5202083800104158    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 6.03\n",
      "episode: 1520   score: 5.0   memory length: 342648   epsilon: 0.51955498001043    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1521   score: 5.0   memory length: 342967   epsilon: 0.5189233600104437    steps: 319    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1522   score: 3.0   memory length: 343176   epsilon: 0.5185095400104527    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1523   score: 7.0   memory length: 343554   epsilon: 0.5177611000104689    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 5.9\n",
      "episode: 1524   score: 5.0   memory length: 343860   epsilon: 0.5171552200104821    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1525   score: 3.0   memory length: 344069   epsilon: 0.5167414000104911    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 5.85\n",
      "episode: 1526   score: 11.0   memory length: 344619   epsilon: 0.5156524000105147    steps: 550    lr: 1.6000000000000003e-05     evaluation reward: 5.89\n",
      "episode: 1527   score: 7.0   memory length: 345029   epsilon: 0.5148406000105323    steps: 410    lr: 1.6000000000000003e-05     evaluation reward: 5.88\n",
      "episode: 1528   score: 2.0   memory length: 345231   epsilon: 0.514440640010541    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1529   score: 5.0   memory length: 345539   epsilon: 0.5138308000105543    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 5.76\n",
      "episode: 1530   score: 8.0   memory length: 345956   epsilon: 0.5130051400105722    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1531   score: 5.0   memory length: 346284   epsilon: 0.5123557000105863    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1532   score: 9.0   memory length: 346710   epsilon: 0.5115122200106046    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 5.8\n",
      "episode: 1533   score: 5.0   memory length: 347077   epsilon: 0.5107855600106204    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 5.81\n",
      "episode: 1534   score: 7.0   memory length: 347444   epsilon: 0.5100589000106361    steps: 367    lr: 1.6000000000000003e-05     evaluation reward: 5.82\n",
      "episode: 1535   score: 9.0   memory length: 347900   epsilon: 0.5091560200106557    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 5.86\n",
      "episode: 1536   score: 16.0   memory length: 348564   epsilon: 0.5078413000106843    steps: 664    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1537   score: 3.0   memory length: 348777   epsilon: 0.5074195600106934    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1538   score: 6.0   memory length: 349082   epsilon: 0.5068156600107065    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 5.97\n",
      "episode: 1539   score: 9.0   memory length: 349509   epsilon: 0.5059702000107249    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 6.0\n",
      "episode: 1540   score: 6.0   memory length: 349871   epsilon: 0.5052534400107405    steps: 362    lr: 1.6000000000000003e-05     evaluation reward: 6.02\n",
      "episode: 1541   score: 7.0   memory length: 350228   epsilon: 0.5045465800107558    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 6.05\n",
      "episode: 1542   score: 5.0   memory length: 350554   epsilon: 0.5039011000107698    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.05\n",
      "episode: 1543   score: 6.0   memory length: 350909   epsilon: 0.5031982000107851    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1544   score: 6.0   memory length: 351251   epsilon: 0.5025210400107998    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 6.06\n",
      "episode: 1545   score: 3.0   memory length: 351482   epsilon: 0.5020636600108097    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 5.99\n",
      "episode: 1546   score: 4.0   memory length: 351763   epsilon: 0.5015072800108218    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 5.93\n",
      "episode: 1547   score: 10.0   memory length: 352156   epsilon: 0.5007291400108387    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1548   score: 4.0   memory length: 352419   epsilon: 0.50020840001085    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 5.95\n",
      "episode: 1549   score: 6.0   memory length: 352798   epsilon: 0.49945798001085107    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1550   score: 3.0   memory length: 353026   epsilon: 0.4990065400108482    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 5.98\n",
      "episode: 1551   score: 8.0   memory length: 353464   epsilon: 0.4981393000108427    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 6.04\n",
      "episode: 1552   score: 3.0   memory length: 353694   epsilon: 0.49768390001083984    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 6.04\n",
      "episode: 1553   score: 4.0   memory length: 353973   epsilon: 0.49713148001083635    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 6.01\n",
      "episode: 1554   score: 9.0   memory length: 354422   epsilon: 0.4962424600108307    steps: 449    lr: 1.6000000000000003e-05     evaluation reward: 6.08\n",
      "episode: 1555   score: 7.0   memory length: 354837   epsilon: 0.4954207600108255    steps: 415    lr: 1.6000000000000003e-05     evaluation reward: 6.11\n",
      "episode: 1556   score: 7.0   memory length: 355245   epsilon: 0.4946129200108204    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 6.1\n",
      "episode: 1557   score: 10.0   memory length: 355815   epsilon: 0.49348432001081327    steps: 570    lr: 1.6000000000000003e-05     evaluation reward: 6.17\n",
      "episode: 1558   score: 4.0   memory length: 356074   epsilon: 0.49297150001081    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 6.19\n",
      "episode: 1559   score: 5.0   memory length: 356378   epsilon: 0.4923695800108062    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 6.22\n",
      "episode: 1560   score: 8.0   memory length: 356786   epsilon: 0.4915617400108011    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1561   score: 7.0   memory length: 357190   epsilon: 0.49076182001079605    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 6.26\n",
      "episode: 1562   score: 13.0   memory length: 357642   epsilon: 0.4898668600107904    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 6.34\n",
      "episode: 1563   score: 3.0   memory length: 357871   epsilon: 0.4894134400107875    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 6.24\n",
      "episode: 1564   score: 8.0   memory length: 358295   epsilon: 0.4885739200107822    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1565   score: 12.0   memory length: 358868   epsilon: 0.487439380010775    steps: 573    lr: 1.6000000000000003e-05     evaluation reward: 6.31\n",
      "episode: 1566   score: 7.0   memory length: 359298   epsilon: 0.48658798001076964    steps: 430    lr: 1.6000000000000003e-05     evaluation reward: 6.25\n",
      "episode: 1567   score: 10.0   memory length: 359816   epsilon: 0.48556234001076315    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 6.28\n",
      "episode: 1568   score: 9.0   memory length: 360281   epsilon: 0.4846416400107573    steps: 465    lr: 1.6000000000000003e-05     evaluation reward: 6.32\n",
      "episode: 1569   score: 12.0   memory length: 360702   epsilon: 0.48380806001075205    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 6.33\n",
      "episode: 1570   score: 9.0   memory length: 361150   epsilon: 0.48292102001074644    steps: 448    lr: 1.6000000000000003e-05     evaluation reward: 6.35\n",
      "episode: 1571   score: 8.0   memory length: 361567   epsilon: 0.4820953600107412    steps: 417    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n",
      "episode: 1572   score: 9.0   memory length: 362013   epsilon: 0.4812122800107356    steps: 446    lr: 1.6000000000000003e-05     evaluation reward: 6.38\n",
      "episode: 1573   score: 10.0   memory length: 362515   epsilon: 0.48021832001072934    steps: 502    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1574   score: 7.0   memory length: 362875   epsilon: 0.47950552001072483    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 6.44\n",
      "episode: 1575   score: 8.0   memory length: 363268   epsilon: 0.4787273800107199    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1576   score: 11.0   memory length: 363805   epsilon: 0.4776641200107132    steps: 537    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1577   score: 7.0   memory length: 364192   epsilon: 0.47689786001070833    steps: 387    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1578   score: 8.0   memory length: 364570   epsilon: 0.4761494200107036    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1579   score: 7.0   memory length: 364953   epsilon: 0.4753910800106988    steps: 383    lr: 1.6000000000000003e-05     evaluation reward: 6.56\n",
      "episode: 1580   score: 5.0   memory length: 365275   epsilon: 0.47475352001069476    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
      "episode: 1581   score: 9.0   memory length: 365766   epsilon: 0.4737813400106886    steps: 491    lr: 1.6000000000000003e-05     evaluation reward: 6.54\n",
      "episode: 1582   score: 5.0   memory length: 366092   epsilon: 0.47313586001068453    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 6.39\n",
      "episode: 1583   score: 11.0   memory length: 366575   epsilon: 0.4721795200106785    steps: 483    lr: 1.6000000000000003e-05     evaluation reward: 6.43\n",
      "episode: 1584   score: 8.0   memory length: 366966   epsilon: 0.4714053400106736    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 6.48\n",
      "episode: 1585   score: 6.0   memory length: 367288   epsilon: 0.47076778001066955    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1586   score: 6.0   memory length: 367645   epsilon: 0.4700609200106651    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 6.49\n",
      "episode: 1587   score: 10.0   memory length: 368054   epsilon: 0.46925110001065995    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1588   score: 6.0   memory length: 368391   epsilon: 0.46858384001065573    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 6.52\n",
      "episode: 1589   score: 5.0   memory length: 368703   epsilon: 0.4679660800106518    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1590   score: 3.0   memory length: 368933   epsilon: 0.46751068001064894    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 6.51\n",
      "episode: 1591   score: 8.0   memory length: 369374   epsilon: 0.4666375000106434    steps: 441    lr: 1.6000000000000003e-05     evaluation reward: 6.53\n",
      "episode: 1592   score: 8.0   memory length: 369854   epsilon: 0.4656871000106374    steps: 480    lr: 1.6000000000000003e-05     evaluation reward: 6.55\n",
      "episode: 1593   score: 9.0   memory length: 370326   epsilon: 0.4647525400106315    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 6.57\n",
      "episode: 1594   score: 7.0   memory length: 370731   epsilon: 0.4639506400106264    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
      "episode: 1595   score: 7.0   memory length: 371124   epsilon: 0.4631725000106215    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 6.62\n",
      "episode: 1596   score: 5.0   memory length: 371431   epsilon: 0.46256464001061764    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 6.61\n",
      "episode: 1597   score: 5.0   memory length: 371755   epsilon: 0.4619231200106136    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 6.59\n",
      "episode: 1598   score: 6.0   memory length: 372079   epsilon: 0.4612816000106095    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 6.58\n",
      "episode: 1599   score: 11.0   memory length: 372592   epsilon: 0.4602658600106031    steps: 513    lr: 1.6000000000000003e-05     evaluation reward: 6.66\n",
      "episode: 1600   score: 11.0   memory length: 373139   epsilon: 0.45918280001059625    steps: 547    lr: 1.6000000000000003e-05     evaluation reward: 6.73\n",
      "episode: 1601   score: 7.0   memory length: 373513   epsilon: 0.45844228001059156    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 6.71\n",
      "episode: 1602   score: 14.0   memory length: 374037   epsilon: 0.457404760010585    steps: 524    lr: 1.6000000000000003e-05     evaluation reward: 6.78\n",
      "episode: 1603   score: 11.0   memory length: 374509   epsilon: 0.4564702000105791    steps: 472    lr: 1.6000000000000003e-05     evaluation reward: 6.83\n",
      "episode: 1604   score: 3.0   memory length: 374719   epsilon: 0.45605440001057646    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 6.83\n",
      "episode: 1605   score: 8.0   memory length: 375172   epsilon: 0.4551574600105708    steps: 453    lr: 1.6000000000000003e-05     evaluation reward: 6.88\n",
      "episode: 1606   score: 6.0   memory length: 375533   epsilon: 0.45444268001056626    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 6.93\n",
      "episode: 1607   score: 7.0   memory length: 375936   epsilon: 0.4536447400105612    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 6.94\n",
      "episode: 1608   score: 6.0   memory length: 376292   epsilon: 0.45293986001055675    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 6.94\n",
      "episode: 1609   score: 8.0   memory length: 376696   epsilon: 0.4521399400105517    steps: 404    lr: 1.6000000000000003e-05     evaluation reward: 6.97\n",
      "episode: 1610   score: 10.0   memory length: 377226   epsilon: 0.45109054001054505    steps: 530    lr: 1.6000000000000003e-05     evaluation reward: 7.01\n",
      "episode: 1611   score: 6.0   memory length: 377603   epsilon: 0.4503440800105403    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 7.01\n",
      "episode: 1612   score: 6.0   memory length: 377994   epsilon: 0.44956990001053543    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 7.03\n",
      "episode: 1613   score: 15.0   memory length: 378557   epsilon: 0.4484551600105284    steps: 563    lr: 1.6000000000000003e-05     evaluation reward: 7.14\n",
      "episode: 1614   score: 5.0   memory length: 378867   epsilon: 0.4478413600105245    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 7.08\n",
      "episode: 1615   score: 7.0   memory length: 379307   epsilon: 0.446970160010519    steps: 440    lr: 1.6000000000000003e-05     evaluation reward: 7.11\n",
      "episode: 1616   score: 10.0   memory length: 379761   epsilon: 0.4460712400105133    steps: 454    lr: 1.6000000000000003e-05     evaluation reward: 7.17\n",
      "episode: 1617   score: 9.0   memory length: 380219   epsilon: 0.44516440001050755    steps: 458    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1618   score: 7.0   memory length: 380654   epsilon: 0.4443031000105021    steps: 435    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
      "episode: 1619   score: 6.0   memory length: 381009   epsilon: 0.44360020001049766    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 7.22\n",
      "episode: 1620   score: 8.0   memory length: 381487   epsilon: 0.44265376001049167    steps: 478    lr: 1.6000000000000003e-05     evaluation reward: 7.25\n",
      "episode: 1621   score: 3.0   memory length: 381717   epsilon: 0.4421983600104888    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 7.23\n",
      "episode: 1622   score: 8.0   memory length: 382137   epsilon: 0.4413667600104835    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1623   score: 7.0   memory length: 382521   epsilon: 0.4406064400104787    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1624   score: 5.0   memory length: 382834   epsilon: 0.4399867000104748    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1625   score: 2.0   memory length: 383036   epsilon: 0.43958674001047227    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1626   score: 5.0   memory length: 383345   epsilon: 0.4389749200104684    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 7.21\n",
      "episode: 1627   score: 6.0   memory length: 383702   epsilon: 0.4382680600104639    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 7.2\n",
      "episode: 1628   score: 12.0   memory length: 384154   epsilon: 0.43737310001045826    steps: 452    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
      "episode: 1629   score: 8.0   memory length: 384550   epsilon: 0.4365890200104533    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
      "episode: 1630   score: 5.0   memory length: 384873   epsilon: 0.43594948001044925    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
      "episode: 1631   score: 8.0   memory length: 385311   epsilon: 0.43508224001044377    steps: 438    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
      "episode: 1632   score: 10.0   memory length: 385834   epsilon: 0.4340467000104372    steps: 523    lr: 1.6000000000000003e-05     evaluation reward: 7.34\n",
      "episode: 1633   score: 2.0   memory length: 386053   epsilon: 0.43361308001043447    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 7.31\n",
      "episode: 1634   score: 3.0   memory length: 386264   epsilon: 0.4331953000104318    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1635   score: 9.0   memory length: 386725   epsilon: 0.43228252001042605    steps: 461    lr: 1.6000000000000003e-05     evaluation reward: 7.27\n",
      "episode: 1636   score: 7.0   memory length: 387099   epsilon: 0.43154200001042137    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
      "episode: 1637   score: 3.0   memory length: 387311   epsilon: 0.4311222400104187    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 7.18\n",
      "episode: 1638   score: 7.0   memory length: 387699   epsilon: 0.43035400001041385    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1639   score: 5.0   memory length: 388004   epsilon: 0.42975010001041003    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 7.15\n",
      "episode: 1640   score: 10.0   memory length: 388522   epsilon: 0.42872446001040354    steps: 518    lr: 1.6000000000000003e-05     evaluation reward: 7.19\n",
      "episode: 1641   score: 17.0   memory length: 389190   epsilon: 0.4274018200103952    steps: 668    lr: 1.6000000000000003e-05     evaluation reward: 7.29\n",
      "episode: 1642   score: 4.0   memory length: 389468   epsilon: 0.4268513800103917    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1643   score: 6.0   memory length: 389823   epsilon: 0.42614848001038724    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1644   score: 6.0   memory length: 390180   epsilon: 0.42544162001038277    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 7.28\n",
      "episode: 1645   score: 7.0   memory length: 390565   epsilon: 0.42467932001037795    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 7.32\n",
      "episode: 1646   score: 7.0   memory length: 390933   epsilon: 0.42395068001037334    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 7.35\n",
      "episode: 1647   score: 5.0   memory length: 391255   epsilon: 0.4233131200103693    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 7.3\n",
      "episode: 1648   score: 7.0   memory length: 391658   epsilon: 0.42251518001036426    steps: 403    lr: 1.6000000000000003e-05     evaluation reward: 7.33\n",
      "episode: 1649   score: 16.0   memory length: 392274   epsilon: 0.42129550001035654    steps: 616    lr: 1.6000000000000003e-05     evaluation reward: 7.43\n",
      "episode: 1650   score: 7.0   memory length: 392696   epsilon: 0.42045994001035125    steps: 422    lr: 1.6000000000000003e-05     evaluation reward: 7.47\n",
      "episode: 1651   score: 6.0   memory length: 393051   epsilon: 0.4197570400103468    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 7.45\n",
      "episode: 1652   score: 9.0   memory length: 393488   epsilon: 0.41889178001034133    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 7.51\n",
      "episode: 1653   score: 7.0   memory length: 393872   epsilon: 0.4181314600103365    steps: 384    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
      "episode: 1654   score: 8.0   memory length: 394299   epsilon: 0.41728600001033117    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 7.53\n",
      "episode: 1655   score: 8.0   memory length: 394717   epsilon: 0.41645836001032593    steps: 418    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
      "episode: 1656   score: 10.0   memory length: 395237   epsilon: 0.4154287600103194    steps: 520    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
      "episode: 1657   score: 7.0   memory length: 395645   epsilon: 0.4146209200103143    steps: 408    lr: 1.6000000000000003e-05     evaluation reward: 7.54\n",
      "episode: 1658   score: 6.0   memory length: 396015   epsilon: 0.4138883200103097    steps: 370    lr: 1.6000000000000003e-05     evaluation reward: 7.56\n",
      "episode: 1659   score: 9.0   memory length: 396518   epsilon: 0.4128923800103034    steps: 503    lr: 1.6000000000000003e-05     evaluation reward: 7.6\n",
      "episode: 1660   score: 10.0   memory length: 397040   epsilon: 0.41185882001029683    steps: 522    lr: 1.6000000000000003e-05     evaluation reward: 7.62\n",
      "episode: 1661   score: 8.0   memory length: 397533   epsilon: 0.41088268001029066    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 7.63\n",
      "episode: 1662   score: 7.0   memory length: 397908   epsilon: 0.41014018001028596    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 7.57\n",
      "episode: 1663   score: 5.0   memory length: 398173   epsilon: 0.40961548001028264    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 7.59\n",
      "episode: 1664   score: 8.0   memory length: 398597   epsilon: 0.40877596001027733    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 7.59\n",
      "episode: 1665   score: 8.0   memory length: 399009   epsilon: 0.40796020001027217    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 7.55\n",
      "episode: 1666   score: 17.0   memory length: 399610   epsilon: 0.40677022001026464    steps: 601    lr: 1.6000000000000003e-05     evaluation reward: 7.65\n",
      "episode: 1667   score: 7.0   memory length: 399998   epsilon: 0.4060019800102598    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 7.62\n",
      "episode: 1668   score: 10.0   memory length: 400498   epsilon: 0.4050119800102535    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1669   score: 11.0   memory length: 401046   epsilon: 0.40392694001024665    steps: 548    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1670   score: 8.0   memory length: 401466   epsilon: 0.4030953400102414    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 7.61\n",
      "episode: 1671   score: 11.0   memory length: 402020   epsilon: 0.40199842001023445    steps: 554    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1672   score: 5.0   memory length: 402308   epsilon: 0.40142818001023084    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 7.6\n",
      "episode: 1673   score: 15.0   memory length: 402919   epsilon: 0.4002184000102232    steps: 611    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1674   score: 5.0   memory length: 403212   epsilon: 0.3996382600102195    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1675   score: 9.0   memory length: 403662   epsilon: 0.3987472600102139    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1676   score: 10.0   memory length: 404027   epsilon: 0.3980245600102093    steps: 365    lr: 6.400000000000001e-06     evaluation reward: 7.63\n",
      "episode: 1677   score: 13.0   memory length: 404602   epsilon: 0.3968860600102021    steps: 575    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1678   score: 9.0   memory length: 405092   epsilon: 0.39591586001019596    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1679   score: 6.0   memory length: 405440   epsilon: 0.3952268200101916    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1680   score: 7.0   memory length: 405862   epsilon: 0.3943912600101863    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1681   score: 6.0   memory length: 406206   epsilon: 0.393710140010182    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 7.68\n",
      "episode: 1682   score: 9.0   memory length: 406657   epsilon: 0.39281716001017636    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1683   score: 6.0   memory length: 407012   epsilon: 0.3921142600101719    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 7.67\n",
      "episode: 1684   score: 6.0   memory length: 407390   epsilon: 0.3913658200101672    steps: 378    lr: 6.400000000000001e-06     evaluation reward: 7.65\n",
      "episode: 1685   score: 3.0   memory length: 407620   epsilon: 0.3909104200101643    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 7.62\n",
      "episode: 1686   score: 8.0   memory length: 408094   epsilon: 0.38997190001015836    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 7.64\n",
      "episode: 1687   score: 18.0   memory length: 408799   epsilon: 0.3885760000101495    steps: 705    lr: 6.400000000000001e-06     evaluation reward: 7.72\n",
      "episode: 1688   score: 4.0   memory length: 409055   epsilon: 0.3880691200101463    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1689   score: 6.0   memory length: 409390   epsilon: 0.3874058200101421    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1690   score: 6.0   memory length: 409745   epsilon: 0.3867029200101377    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 7.74\n",
      "episode: 1691   score: 5.0   memory length: 410073   epsilon: 0.38605348001013357    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1692   score: 7.0   memory length: 410494   epsilon: 0.3852199000101283    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 7.7\n",
      "episode: 1693   score: 8.0   memory length: 410930   epsilon: 0.38435662001012283    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 7.69\n",
      "episode: 1694   score: 9.0   memory length: 411367   epsilon: 0.38349136001011735    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 7.71\n",
      "episode: 1695   score: 11.0   memory length: 411943   epsilon: 0.38235088001011014    steps: 576    lr: 6.400000000000001e-06     evaluation reward: 7.75\n",
      "episode: 1696   score: 12.0   memory length: 412519   epsilon: 0.3812104000101029    steps: 576    lr: 6.400000000000001e-06     evaluation reward: 7.82\n",
      "episode: 1697   score: 9.0   memory length: 412978   epsilon: 0.3803015800100972    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 7.86\n",
      "episode: 1698   score: 12.0   memory length: 413571   epsilon: 0.37912744001008974    steps: 593    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1699   score: 10.0   memory length: 414067   epsilon: 0.37814536001008353    steps: 496    lr: 6.400000000000001e-06     evaluation reward: 7.91\n",
      "episode: 1700   score: 10.0   memory length: 414569   epsilon: 0.37715140001007724    steps: 502    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
      "episode: 1701   score: 9.0   memory length: 415000   epsilon: 0.37629802001007184    steps: 431    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1702   score: 9.0   memory length: 415437   epsilon: 0.37543276001006637    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 7.87\n",
      "episode: 1703   score: 12.0   memory length: 415956   epsilon: 0.37440514001005987    steps: 519    lr: 6.400000000000001e-06     evaluation reward: 7.88\n",
      "episode: 1704   score: 7.0   memory length: 416343   epsilon: 0.373638880010055    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1705   score: 8.0   memory length: 416770   epsilon: 0.37279342001004967    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1706   score: 8.0   memory length: 417223   epsilon: 0.371896480010044    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 7.94\n",
      "episode: 1707   score: 13.0   memory length: 417709   epsilon: 0.3709342000100379    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
      "episode: 1708   score: 6.0   memory length: 418047   epsilon: 0.3702649600100337    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
      "episode: 1709   score: 4.0   memory length: 418304   epsilon: 0.36975610001003045    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 7.96\n",
      "episode: 1710   score: 9.0   memory length: 418808   epsilon: 0.36875818001002414    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 7.95\n",
      "episode: 1711   score: 7.0   memory length: 419159   epsilon: 0.36806320001001974    steps: 351    lr: 6.400000000000001e-06     evaluation reward: 7.96\n",
      "episode: 1712   score: 10.0   memory length: 419640   epsilon: 0.3671108200100137    steps: 481    lr: 6.400000000000001e-06     evaluation reward: 8.0\n",
      "episode: 1713   score: 7.0   memory length: 420033   epsilon: 0.3663326800100088    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1714   score: 3.0   memory length: 420244   epsilon: 0.36591490001000615    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 7.9\n",
      "episode: 1715   score: 6.0   memory length: 420587   epsilon: 0.36523576001000185    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 7.89\n",
      "episode: 1716   score: 13.0   memory length: 421227   epsilon: 0.36396856000999384    steps: 640    lr: 6.400000000000001e-06     evaluation reward: 7.92\n",
      "episode: 1717   score: 6.0   memory length: 421627   epsilon: 0.3631765600099888    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 7.89\n",
      "episode: 1718   score: 9.0   memory length: 422063   epsilon: 0.36231328000998336    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 7.91\n",
      "episode: 1719   score: 13.0   memory length: 422649   epsilon: 0.361153000009976    steps: 586    lr: 6.400000000000001e-06     evaluation reward: 7.98\n",
      "episode: 1720   score: 6.0   memory length: 422981   epsilon: 0.36049564000997186    steps: 332    lr: 6.400000000000001e-06     evaluation reward: 7.96\n",
      "episode: 1721   score: 12.0   memory length: 423521   epsilon: 0.3594264400099651    steps: 540    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1722   score: 10.0   memory length: 424007   epsilon: 0.358464160009959    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 8.07\n",
      "episode: 1723   score: 4.0   memory length: 424289   epsilon: 0.3579058000099555    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 8.04\n",
      "episode: 1724   score: 6.0   memory length: 424618   epsilon: 0.35725438000995136    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1725   score: 5.0   memory length: 424943   epsilon: 0.3566108800099473    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
      "episode: 1726   score: 2.0   memory length: 425125   epsilon: 0.356250520009945    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1727   score: 9.0   memory length: 425637   epsilon: 0.3552367600099386    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 8.08\n",
      "episode: 1728   score: 6.0   memory length: 425996   epsilon: 0.3545259400099341    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 8.02\n",
      "episode: 1729   score: 11.0   memory length: 426549   epsilon: 0.35343100000992717    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1730   score: 6.0   memory length: 426925   epsilon: 0.35268652000992246    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 8.06\n",
      "episode: 1731   score: 7.0   memory length: 427282   epsilon: 0.351979660009918    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 8.05\n",
      "episode: 1732   score: 6.0   memory length: 427619   epsilon: 0.35131240000991376    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 8.01\n",
      "episode: 1733   score: 15.0   memory length: 428341   epsilon: 0.3498828400099047    steps: 722    lr: 6.400000000000001e-06     evaluation reward: 8.14\n",
      "episode: 1734   score: 6.0   memory length: 428696   epsilon: 0.34917994000990027    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 8.17\n",
      "episode: 1735   score: 11.0   memory length: 429263   epsilon: 0.34805728000989317    steps: 567    lr: 6.400000000000001e-06     evaluation reward: 8.19\n",
      "episode: 1736   score: 14.0   memory length: 429903   epsilon: 0.34679008000988515    steps: 640    lr: 6.400000000000001e-06     evaluation reward: 8.26\n",
      "episode: 1737   score: 9.0   memory length: 430350   epsilon: 0.34590502000987955    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1738   score: 5.0   memory length: 430660   epsilon: 0.34529122000987567    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 8.3\n",
      "episode: 1739   score: 11.0   memory length: 431185   epsilon: 0.3442517200098691    steps: 525    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1740   score: 8.0   memory length: 431638   epsilon: 0.3433547800098634    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 8.34\n",
      "episode: 1741   score: 4.0   memory length: 431934   epsilon: 0.3427687000098597    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 8.21\n",
      "episode: 1742   score: 3.0   memory length: 432147   epsilon: 0.34234696000985704    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 8.2\n",
      "episode: 1743   score: 14.0   memory length: 432545   epsilon: 0.34155892000985205    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 8.28\n",
      "episode: 1744   score: 8.0   memory length: 432960   epsilon: 0.34073722000984685    steps: 415    lr: 6.400000000000001e-06     evaluation reward: 8.3\n",
      "episode: 1745   score: 9.0   memory length: 433425   epsilon: 0.339816520009841    steps: 465    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1746   score: 8.0   memory length: 433826   epsilon: 0.339022540009836    steps: 401    lr: 6.400000000000001e-06     evaluation reward: 8.33\n",
      "episode: 1747   score: 8.0   memory length: 434261   epsilon: 0.33816124000983055    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1748   score: 8.0   memory length: 434717   epsilon: 0.33725836000982484    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1749   score: 11.0   memory length: 435283   epsilon: 0.33613768000981775    steps: 566    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1750   score: 7.0   memory length: 435655   epsilon: 0.3354011200098131    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 8.32\n",
      "episode: 1751   score: 9.0   memory length: 435997   epsilon: 0.3347239600098088    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 8.35\n",
      "episode: 1752   score: 10.0   memory length: 436501   epsilon: 0.3337260400098025    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1753   score: 7.0   memory length: 436887   epsilon: 0.33296176000979766    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1754   score: 8.0   memory length: 437342   epsilon: 0.33206086000979196    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1755   score: 6.0   memory length: 437663   epsilon: 0.33142528000978794    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 8.34\n",
      "episode: 1756   score: 13.0   memory length: 438259   epsilon: 0.33024520000978047    steps: 596    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1757   score: 8.0   memory length: 438674   epsilon: 0.32942350000977527    steps: 415    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
      "episode: 1758   score: 9.0   memory length: 439017   epsilon: 0.328744360009771    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1759   score: 8.0   memory length: 439450   epsilon: 0.32788702000976555    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 8.4\n",
      "episode: 1760   score: 6.0   memory length: 439803   epsilon: 0.32718808000976113    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 8.36\n",
      "episode: 1761   score: 13.0   memory length: 440472   epsilon: 0.32586346000975275    steps: 669    lr: 6.400000000000001e-06     evaluation reward: 8.41\n",
      "episode: 1762   score: 9.0   memory length: 440943   epsilon: 0.32493088000974685    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1763   score: 10.0   memory length: 441448   epsilon: 0.3239309800097405    steps: 505    lr: 6.400000000000001e-06     evaluation reward: 8.48\n",
      "episode: 1764   score: 5.0   memory length: 441772   epsilon: 0.32328946000973646    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 8.45\n",
      "episode: 1765   score: 6.0   memory length: 442147   epsilon: 0.32254696000973176    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1766   score: 12.0   memory length: 442676   epsilon: 0.32149954000972514    steps: 529    lr: 6.400000000000001e-06     evaluation reward: 8.38\n",
      "episode: 1767   score: 6.0   memory length: 443051   epsilon: 0.32075704000972044    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 8.37\n",
      "episode: 1768   score: 8.0   memory length: 443480   epsilon: 0.31990762000971507    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 8.35\n",
      "episode: 1769   score: 9.0   memory length: 443900   epsilon: 0.3190760200097098    steps: 420    lr: 6.400000000000001e-06     evaluation reward: 8.33\n",
      "episode: 1770   score: 14.0   memory length: 444507   epsilon: 0.3178741600097022    steps: 607    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1771   score: 11.0   memory length: 445027   epsilon: 0.3168445600096957    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1772   score: 9.0   memory length: 445474   epsilon: 0.3159595000096901    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1773   score: 11.0   memory length: 445896   epsilon: 0.3151239400096848    steps: 422    lr: 6.400000000000001e-06     evaluation reward: 8.39\n",
      "episode: 1774   score: 8.0   memory length: 446324   epsilon: 0.31427650000967944    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 8.42\n",
      "episode: 1775   score: 9.0   memory length: 446811   epsilon: 0.31331224000967334    steps: 487    lr: 6.400000000000001e-06     evaluation reward: 8.42\n",
      "episode: 1776   score: 11.0   memory length: 447313   epsilon: 0.31231828000966705    steps: 502    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1777   score: 16.0   memory length: 448003   epsilon: 0.3109520800096584    steps: 690    lr: 6.400000000000001e-06     evaluation reward: 8.46\n",
      "episode: 1778   score: 6.0   memory length: 448342   epsilon: 0.31028086000965416    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 8.43\n",
      "episode: 1779   score: 12.0   memory length: 448832   epsilon: 0.309310660009648    steps: 490    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1780   score: 7.0   memory length: 449255   epsilon: 0.3084731200096427    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1781   score: 6.0   memory length: 449611   epsilon: 0.30776824000963826    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1782   score: 9.0   memory length: 450123   epsilon: 0.30675448000963185    steps: 512    lr: 6.400000000000001e-06     evaluation reward: 8.49\n",
      "episode: 1783   score: 7.0   memory length: 450497   epsilon: 0.30601396000962716    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 8.5\n",
      "episode: 1784   score: 8.0   memory length: 450949   epsilon: 0.3051190000096215    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 8.52\n",
      "episode: 1785   score: 10.0   memory length: 451400   epsilon: 0.30422602000961585    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 8.59\n",
      "episode: 1786   score: 11.0   memory length: 451953   epsilon: 0.3031310800096089    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 8.62\n",
      "episode: 1787   score: 11.0   memory length: 452504   epsilon: 0.302040100009602    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 8.55\n",
      "episode: 1788   score: 19.0   memory length: 453128   epsilon: 0.3008045800095942    steps: 624    lr: 6.400000000000001e-06     evaluation reward: 8.7\n",
      "episode: 1789   score: 9.0   memory length: 453603   epsilon: 0.29986408000958825    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 8.73\n",
      "episode: 1790   score: 7.0   memory length: 453984   epsilon: 0.2991097000095835    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 8.74\n",
      "episode: 1791   score: 14.0   memory length: 454545   epsilon: 0.29799892000957645    steps: 561    lr: 6.400000000000001e-06     evaluation reward: 8.83\n",
      "episode: 1792   score: 14.0   memory length: 455188   epsilon: 0.2967257800095684    steps: 643    lr: 6.400000000000001e-06     evaluation reward: 8.9\n",
      "episode: 1793   score: 7.0   memory length: 455564   epsilon: 0.2959813000095637    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 8.89\n",
      "episode: 1794   score: 4.0   memory length: 455843   epsilon: 0.2954288800095602    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 8.84\n",
      "episode: 1795   score: 7.0   memory length: 456191   epsilon: 0.29473984000955583    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1796   score: 11.0   memory length: 456742   epsilon: 0.29364886000954893    steps: 551    lr: 6.400000000000001e-06     evaluation reward: 8.79\n",
      "episode: 1797   score: 7.0   memory length: 457128   epsilon: 0.2928845800095441    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 8.77\n",
      "episode: 1798   score: 10.0   memory length: 457616   epsilon: 0.291918340009538    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 8.75\n",
      "episode: 1799   score: 9.0   memory length: 458122   epsilon: 0.29091646000953164    steps: 506    lr: 6.400000000000001e-06     evaluation reward: 8.74\n",
      "episode: 1800   score: 8.0   memory length: 458543   epsilon: 0.29008288000952637    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 8.72\n",
      "episode: 1801   score: 17.0   memory length: 459187   epsilon: 0.2888077600095183    steps: 644    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1802   score: 9.0   memory length: 459658   epsilon: 0.2878751800095124    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 8.8\n",
      "episode: 1803   score: 15.0   memory length: 460340   epsilon: 0.28652482000950386    steps: 682    lr: 6.400000000000001e-06     evaluation reward: 8.83\n",
      "episode: 1804   score: 11.0   memory length: 460894   epsilon: 0.2854279000094969    steps: 554    lr: 6.400000000000001e-06     evaluation reward: 8.87\n",
      "episode: 1805   score: 9.0   memory length: 461377   epsilon: 0.28447156000949086    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 8.88\n",
      "episode: 1806   score: 14.0   memory length: 461983   epsilon: 0.2832716800094833    steps: 606    lr: 6.400000000000001e-06     evaluation reward: 8.94\n",
      "episode: 1807   score: 13.0   memory length: 462566   epsilon: 0.28211734000947597    steps: 583    lr: 6.400000000000001e-06     evaluation reward: 8.94\n",
      "episode: 1808   score: 8.0   memory length: 462999   epsilon: 0.28126000000947055    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 8.96\n",
      "episode: 1809   score: 8.0   memory length: 463455   epsilon: 0.28035712000946483    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 9.0\n",
      "episode: 1810   score: 8.0   memory length: 463911   epsilon: 0.2794542400094591    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 8.99\n",
      "episode: 1811   score: 9.0   memory length: 464341   epsilon: 0.27860284000945373    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 9.01\n",
      "episode: 1812   score: 10.0   memory length: 464841   epsilon: 0.27761284000944747    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 9.01\n",
      "episode: 1813   score: 8.0   memory length: 465310   epsilon: 0.2766842200094416    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 9.02\n",
      "episode: 1814   score: 14.0   memory length: 465970   epsilon: 0.2753774200094333    steps: 660    lr: 6.400000000000001e-06     evaluation reward: 9.13\n",
      "episode: 1815   score: 9.0   memory length: 466451   epsilon: 0.2744250400094273    steps: 481    lr: 6.400000000000001e-06     evaluation reward: 9.16\n",
      "episode: 1816   score: 9.0   memory length: 466888   epsilon: 0.2735597800094218    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 9.12\n",
      "episode: 1817   score: 7.0   memory length: 467275   epsilon: 0.272793520009417    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 9.13\n",
      "episode: 1818   score: 14.0   memory length: 467976   epsilon: 0.2714055400094082    steps: 701    lr: 6.400000000000001e-06     evaluation reward: 9.18\n",
      "episode: 1819   score: 11.0   memory length: 468526   epsilon: 0.2703165400094013    steps: 550    lr: 6.400000000000001e-06     evaluation reward: 9.16\n",
      "episode: 1820   score: 9.0   memory length: 468964   epsilon: 0.2694493000093958    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 9.19\n",
      "episode: 1821   score: 11.0   memory length: 469492   epsilon: 0.2684038600093892    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 9.18\n",
      "episode: 1822   score: 6.0   memory length: 469829   epsilon: 0.267736600009385    steps: 337    lr: 6.400000000000001e-06     evaluation reward: 9.14\n",
      "episode: 1823   score: 5.0   memory length: 470138   epsilon: 0.2671247800093811    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 9.15\n",
      "episode: 1824   score: 14.0   memory length: 470794   epsilon: 0.2658259000093729    steps: 656    lr: 6.400000000000001e-06     evaluation reward: 9.23\n",
      "episode: 1825   score: 14.0   memory length: 471467   epsilon: 0.26449336000936446    steps: 673    lr: 6.400000000000001e-06     evaluation reward: 9.32\n",
      "episode: 1826   score: 7.0   memory length: 471794   epsilon: 0.26384590000936037    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 9.37\n",
      "episode: 1827   score: 7.0   memory length: 472199   epsilon: 0.2630440000093553    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 9.35\n",
      "episode: 1828   score: 5.0   memory length: 472508   epsilon: 0.2624321800093514    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 9.34\n",
      "episode: 1829   score: 10.0   memory length: 473052   epsilon: 0.2613550600093446    steps: 544    lr: 6.400000000000001e-06     evaluation reward: 9.33\n",
      "episode: 1830   score: 18.0   memory length: 473687   epsilon: 0.26009776000933665    steps: 635    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
      "episode: 1831   score: 8.0   memory length: 474095   epsilon: 0.25928992000933154    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 9.46\n",
      "episode: 1832   score: 6.0   memory length: 474451   epsilon: 0.2585850400093271    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 9.46\n",
      "episode: 1833   score: 12.0   memory length: 474915   epsilon: 0.25766632000932127    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 9.43\n",
      "episode: 1834   score: 12.0   memory length: 475484   epsilon: 0.25653970000931414    steps: 569    lr: 6.400000000000001e-06     evaluation reward: 9.49\n",
      "episode: 1835   score: 10.0   memory length: 476037   epsilon: 0.2554447600093072    steps: 553    lr: 6.400000000000001e-06     evaluation reward: 9.48\n",
      "episode: 1836   score: 9.0   memory length: 476491   epsilon: 0.2545458400093015    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 9.43\n",
      "episode: 1837   score: 7.0   memory length: 476912   epsilon: 0.25371226000929625    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
      "episode: 1838   score: 9.0   memory length: 477391   epsilon: 0.25276384000929025    steps: 479    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
      "episode: 1839   score: 7.0   memory length: 477777   epsilon: 0.2519995600092854    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 9.41\n",
      "episode: 1840   score: 7.0   memory length: 478138   epsilon: 0.2512847800092809    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 9.4\n",
      "episode: 1841   score: 9.0   memory length: 478641   epsilon: 0.2502888400092746    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 9.45\n",
      "episode: 1842   score: 9.0   memory length: 479113   epsilon: 0.24935428000926868    steps: 472    lr: 6.400000000000001e-06     evaluation reward: 9.51\n",
      "episode: 1843   score: 9.0   memory length: 479569   epsilon: 0.24845140000926297    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 9.46\n",
      "episode: 1844   score: 9.0   memory length: 480024   epsilon: 0.24755050000925727    steps: 455    lr: 6.400000000000001e-06     evaluation reward: 9.47\n",
      "episode: 1845   score: 6.0   memory length: 480381   epsilon: 0.2468436400092528    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 9.44\n",
      "episode: 1846   score: 10.0   memory length: 480902   epsilon: 0.24581206000924627    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 9.46\n",
      "episode: 1847   score: 12.0   memory length: 481353   epsilon: 0.24491908000924062    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 9.5\n",
      "episode: 1848   score: 6.0   memory length: 481708   epsilon: 0.24421618000923617    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 9.48\n",
      "episode: 1849   score: 11.0   memory length: 482229   epsilon: 0.24318460000922965    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 9.48\n",
      "episode: 1850   score: 6.0   memory length: 482565   epsilon: 0.24251932000922544    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 9.47\n",
      "episode: 1851   score: 13.0   memory length: 483032   epsilon: 0.2415946600092196    steps: 467    lr: 6.400000000000001e-06     evaluation reward: 9.51\n",
      "episode: 1852   score: 8.0   memory length: 483468   epsilon: 0.24073138000921412    steps: 436    lr: 6.400000000000001e-06     evaluation reward: 9.49\n",
      "episode: 1853   score: 15.0   memory length: 484165   epsilon: 0.2393513200092054    steps: 697    lr: 6.400000000000001e-06     evaluation reward: 9.57\n",
      "episode: 1854   score: 12.0   memory length: 484701   epsilon: 0.23829004000919868    steps: 536    lr: 6.400000000000001e-06     evaluation reward: 9.61\n",
      "episode: 1855   score: 6.0   memory length: 485060   epsilon: 0.23757922000919418    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 9.61\n",
      "episode: 1856   score: 9.0   memory length: 485470   epsilon: 0.23676742000918904    steps: 410    lr: 6.400000000000001e-06     evaluation reward: 9.57\n",
      "episode: 1857   score: 8.0   memory length: 485866   epsilon: 0.23598334000918408    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 9.57\n",
      "episode: 1858   score: 4.0   memory length: 486108   epsilon: 0.23550418000918105    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 9.52\n",
      "episode: 1859   score: 7.0   memory length: 486513   epsilon: 0.23470228000917598    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 9.51\n",
      "episode: 1860   score: 15.0   memory length: 487033   epsilon: 0.23367268000916946    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1861   score: 10.0   memory length: 487528   epsilon: 0.23269258000916326    steps: 495    lr: 6.400000000000001e-06     evaluation reward: 9.57\n",
      "episode: 1862   score: 8.0   memory length: 487933   epsilon: 0.2318906800091582    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 9.56\n",
      "episode: 1863   score: 3.0   memory length: 488165   epsilon: 0.23143132000915528    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 9.49\n",
      "episode: 1864   score: 14.0   memory length: 488840   epsilon: 0.23009482000914683    steps: 675    lr: 6.400000000000001e-06     evaluation reward: 9.58\n",
      "episode: 1865   score: 14.0   memory length: 489483   epsilon: 0.22882168000913877    steps: 643    lr: 6.400000000000001e-06     evaluation reward: 9.66\n",
      "episode: 1866   score: 11.0   memory length: 490069   epsilon: 0.22766140000913143    steps: 586    lr: 6.400000000000001e-06     evaluation reward: 9.65\n",
      "episode: 1867   score: 8.0   memory length: 490525   epsilon: 0.22675852000912572    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 9.67\n",
      "episode: 1868   score: 10.0   memory length: 491002   epsilon: 0.22581406000911974    steps: 477    lr: 6.400000000000001e-06     evaluation reward: 9.69\n",
      "episode: 1869   score: 9.0   memory length: 491490   epsilon: 0.22484782000911363    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 9.69\n",
      "episode: 1870   score: 7.0   memory length: 491831   epsilon: 0.22417264000910936    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 9.62\n",
      "episode: 1871   score: 9.0   memory length: 492293   epsilon: 0.22325788000910357    steps: 462    lr: 6.400000000000001e-06     evaluation reward: 9.6\n",
      "episode: 1872   score: 8.0   memory length: 492743   epsilon: 0.22236688000909793    steps: 450    lr: 6.400000000000001e-06     evaluation reward: 9.59\n",
      "episode: 1873   score: 7.0   memory length: 493149   epsilon: 0.22156300000909285    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 9.55\n",
      "episode: 1874   score: 10.0   memory length: 493690   epsilon: 0.22049182000908607    steps: 541    lr: 6.400000000000001e-06     evaluation reward: 9.57\n",
      "episode: 1875   score: 20.0   memory length: 494332   epsilon: 0.21922066000907803    steps: 642    lr: 6.400000000000001e-06     evaluation reward: 9.68\n",
      "episode: 1876   score: 13.0   memory length: 494836   epsilon: 0.21822274000907171    steps: 504    lr: 6.400000000000001e-06     evaluation reward: 9.7\n",
      "episode: 1877   score: 13.0   memory length: 495495   epsilon: 0.21691792000906346    steps: 659    lr: 6.400000000000001e-06     evaluation reward: 9.67\n",
      "episode: 1878   score: 9.0   memory length: 495951   epsilon: 0.21601504000905775    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 9.7\n",
      "episode: 1879   score: 14.0   memory length: 496488   epsilon: 0.21495178000905102    steps: 537    lr: 6.400000000000001e-06     evaluation reward: 9.72\n",
      "episode: 1880   score: 9.0   memory length: 496988   epsilon: 0.21396178000904476    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 9.74\n",
      "episode: 1881   score: 11.0   memory length: 497521   epsilon: 0.21290644000903808    steps: 533    lr: 6.400000000000001e-06     evaluation reward: 9.79\n",
      "episode: 1882   score: 11.0   memory length: 498061   epsilon: 0.2118372400090313    steps: 540    lr: 6.400000000000001e-06     evaluation reward: 9.81\n",
      "episode: 1883   score: 10.0   memory length: 498615   epsilon: 0.21074032000902437    steps: 554    lr: 6.400000000000001e-06     evaluation reward: 9.84\n",
      "episode: 1884   score: 11.0   memory length: 499167   epsilon: 0.20964736000901746    steps: 552    lr: 6.400000000000001e-06     evaluation reward: 9.87\n",
      "episode: 1885   score: 6.0   memory length: 499478   epsilon: 0.20903158000901356    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 9.83\n",
      "episode: 1886   score: 13.0   memory length: 500040   epsilon: 0.20791882000900652    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 9.85\n",
      "episode: 1887   score: 13.0   memory length: 500679   epsilon: 0.20665360000899852    steps: 639    lr: 2.560000000000001e-06     evaluation reward: 9.87\n",
      "episode: 1888   score: 7.0   memory length: 501091   epsilon: 0.20583784000899336    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 1889   score: 9.0   memory length: 501501   epsilon: 0.20502604000898822    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 1890   score: 8.0   memory length: 501930   epsilon: 0.20417662000898285    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
      "episode: 1891   score: 9.0   memory length: 502406   epsilon: 0.20323414000897688    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 9.71\n",
      "episode: 1892   score: 18.0   memory length: 503058   epsilon: 0.20194318000896871    steps: 652    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 1893   score: 5.0   memory length: 503366   epsilon: 0.20133334000896486    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 9.73\n",
      "episode: 1894   score: 6.0   memory length: 503719   epsilon: 0.20063440000896043    steps: 353    lr: 2.560000000000001e-06     evaluation reward: 9.75\n",
      "episode: 1895   score: 10.0   memory length: 504222   epsilon: 0.19963846000895413    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 9.78\n",
      "episode: 1896   score: 11.0   memory length: 504770   epsilon: 0.19855342000894727    steps: 548    lr: 2.560000000000001e-06     evaluation reward: 9.78\n",
      "episode: 1897   score: 10.0   memory length: 505242   epsilon: 0.19761886000894135    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 9.81\n",
      "episode: 1898   score: 6.0   memory length: 505600   epsilon: 0.19691002000893687    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 9.77\n",
      "episode: 1899   score: 6.0   memory length: 505957   epsilon: 0.1962031600089324    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
      "episode: 1900   score: 17.0   memory length: 506588   epsilon: 0.1949537800089245    steps: 631    lr: 2.560000000000001e-06     evaluation reward: 9.83\n",
      "episode: 1901   score: 13.0   memory length: 507101   epsilon: 0.19393804000891807    steps: 513    lr: 2.560000000000001e-06     evaluation reward: 9.79\n",
      "episode: 1902   score: 9.0   memory length: 507583   epsilon: 0.19298368000891203    steps: 482    lr: 2.560000000000001e-06     evaluation reward: 9.79\n",
      "episode: 1903   score: 9.0   memory length: 508029   epsilon: 0.19210060000890644    steps: 446    lr: 2.560000000000001e-06     evaluation reward: 9.73\n",
      "episode: 1904   score: 12.0   memory length: 508644   epsilon: 0.19088290000889874    steps: 615    lr: 2.560000000000001e-06     evaluation reward: 9.74\n",
      "episode: 1905   score: 14.0   memory length: 509320   epsilon: 0.18954442000889027    steps: 676    lr: 2.560000000000001e-06     evaluation reward: 9.79\n",
      "episode: 1906   score: 11.0   memory length: 509896   epsilon: 0.18840394000888305    steps: 576    lr: 2.560000000000001e-06     evaluation reward: 9.76\n",
      "episode: 1907   score: 28.0   memory length: 510445   epsilon: 0.18731692000887618    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 9.91\n",
      "episode: 1908   score: 13.0   memory length: 511077   epsilon: 0.18606556000886826    steps: 632    lr: 2.560000000000001e-06     evaluation reward: 9.96\n",
      "episode: 1909   score: 7.0   memory length: 511463   epsilon: 0.18530128000886342    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 9.95\n",
      "episode: 1910   score: 12.0   memory length: 512080   epsilon: 0.1840796200088557    steps: 617    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 1911   score: 9.0   memory length: 512558   epsilon: 0.1831331800088497    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 9.99\n",
      "episode: 1912   score: 11.0   memory length: 513109   epsilon: 0.1820422000088428    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 10.0\n",
      "episode: 1913   score: 15.0   memory length: 513695   epsilon: 0.18088192000883546    steps: 586    lr: 2.560000000000001e-06     evaluation reward: 10.07\n",
      "episode: 1914   score: 17.0   memory length: 514448   epsilon: 0.17939098000882603    steps: 753    lr: 2.560000000000001e-06     evaluation reward: 10.1\n",
      "episode: 1915   score: 6.0   memory length: 514771   epsilon: 0.17875144000882198    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 10.07\n",
      "episode: 1916   score: 15.0   memory length: 515323   epsilon: 0.17765848000881507    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 10.13\n",
      "episode: 1917   score: 9.0   memory length: 515801   epsilon: 0.17671204000880908    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 10.15\n",
      "episode: 1918   score: 8.0   memory length: 516186   epsilon: 0.17594974000880426    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 10.09\n",
      "episode: 1919   score: 14.0   memory length: 516836   epsilon: 0.1746627400087961    steps: 650    lr: 2.560000000000001e-06     evaluation reward: 10.12\n",
      "episode: 1920   score: 14.0   memory length: 517398   epsilon: 0.17354998000878907    steps: 562    lr: 2.560000000000001e-06     evaluation reward: 10.17\n",
      "episode: 1921   score: 11.0   memory length: 517895   epsilon: 0.17256592000878285    steps: 497    lr: 2.560000000000001e-06     evaluation reward: 10.17\n",
      "episode: 1922   score: 13.0   memory length: 518375   epsilon: 0.17161552000877683    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 10.24\n",
      "episode: 1923   score: 8.0   memory length: 518762   epsilon: 0.17084926000877199    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 10.27\n",
      "episode: 1924   score: 10.0   memory length: 519212   epsilon: 0.16995826000876635    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 10.23\n",
      "episode: 1925   score: 8.0   memory length: 519632   epsilon: 0.1691266600087611    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 10.17\n",
      "episode: 1926   score: 19.0   memory length: 520236   epsilon: 0.16793074000875352    steps: 604    lr: 2.560000000000001e-06     evaluation reward: 10.29\n",
      "episode: 1927   score: 10.0   memory length: 520725   epsilon: 0.1669625200087474    steps: 489    lr: 2.560000000000001e-06     evaluation reward: 10.32\n",
      "episode: 1928   score: 10.0   memory length: 521256   epsilon: 0.16591114000874074    steps: 531    lr: 2.560000000000001e-06     evaluation reward: 10.37\n",
      "episode: 1929   score: 9.0   memory length: 521717   epsilon: 0.16499836000873497    steps: 461    lr: 2.560000000000001e-06     evaluation reward: 10.36\n",
      "episode: 1930   score: 8.0   memory length: 522123   epsilon: 0.16419448000872988    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 10.26\n",
      "episode: 1931   score: 16.0   memory length: 522739   epsilon: 0.16297480000872216    steps: 616    lr: 2.560000000000001e-06     evaluation reward: 10.34\n",
      "episode: 1932   score: 5.0   memory length: 523025   epsilon: 0.16240852000871858    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 10.33\n",
      "episode: 1933   score: 7.0   memory length: 523390   epsilon: 0.161685820008714    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 10.28\n",
      "episode: 1934   score: 5.0   memory length: 523680   epsilon: 0.16111162000871038    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 10.21\n",
      "episode: 1935   score: 7.0   memory length: 524055   epsilon: 0.16036912000870568    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 10.18\n",
      "episode: 1936   score: 12.0   memory length: 524527   epsilon: 0.15943456000869977    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 10.21\n",
      "episode: 1937   score: 12.0   memory length: 525074   epsilon: 0.1583515000086929    steps: 547    lr: 2.560000000000001e-06     evaluation reward: 10.26\n",
      "episode: 1938   score: 15.0   memory length: 525766   epsilon: 0.15698134000868424    steps: 692    lr: 2.560000000000001e-06     evaluation reward: 10.32\n",
      "episode: 1939   score: 7.0   memory length: 526118   epsilon: 0.15628438000867984    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 10.32\n",
      "episode: 1940   score: 5.0   memory length: 526407   epsilon: 0.15571216000867621    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 10.3\n",
      "episode: 1941   score: 12.0   memory length: 526944   epsilon: 0.1546489000086695    steps: 537    lr: 2.560000000000001e-06     evaluation reward: 10.33\n",
      "episode: 1942   score: 9.0   memory length: 527408   epsilon: 0.15373018000866367    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 10.33\n",
      "episode: 1943   score: 14.0   memory length: 527972   epsilon: 0.1526134600086566    steps: 564    lr: 2.560000000000001e-06     evaluation reward: 10.38\n",
      "episode: 1944   score: 8.0   memory length: 528368   epsilon: 0.15182938000865165    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 10.37\n",
      "episode: 1945   score: 11.0   memory length: 528913   epsilon: 0.15075028000864482    steps: 545    lr: 2.560000000000001e-06     evaluation reward: 10.42\n",
      "episode: 1946   score: 14.0   memory length: 529547   epsilon: 0.14949496000863688    steps: 634    lr: 2.560000000000001e-06     evaluation reward: 10.46\n",
      "episode: 1947   score: 10.0   memory length: 530060   epsilon: 0.14847922000863045    steps: 513    lr: 2.560000000000001e-06     evaluation reward: 10.44\n",
      "episode: 1948   score: 11.0   memory length: 530640   epsilon: 0.1473308200086232    steps: 580    lr: 2.560000000000001e-06     evaluation reward: 10.49\n",
      "episode: 1949   score: 12.0   memory length: 531232   epsilon: 0.14615866000861577    steps: 592    lr: 2.560000000000001e-06     evaluation reward: 10.5\n",
      "episode: 1950   score: 9.0   memory length: 531708   epsilon: 0.1452161800086098    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 10.53\n",
      "episode: 1951   score: 11.0   memory length: 532232   epsilon: 0.14417866000860324    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 10.51\n",
      "episode: 1952   score: 9.0   memory length: 532675   epsilon: 0.1433015200085977    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 1953   score: 11.0   memory length: 533161   epsilon: 0.1423392400085916    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 10.48\n",
      "episode: 1954   score: 12.0   memory length: 533681   epsilon: 0.1413096400085851    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 10.48\n",
      "episode: 1955   score: 10.0   memory length: 534188   epsilon: 0.14030578000857874    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 10.52\n",
      "episode: 1956   score: 12.0   memory length: 534789   epsilon: 0.1391158000085712    steps: 601    lr: 2.560000000000001e-06     evaluation reward: 10.55\n",
      "episode: 1957   score: 13.0   memory length: 535396   epsilon: 0.1379139400085636    steps: 607    lr: 2.560000000000001e-06     evaluation reward: 10.6\n",
      "episode: 1958   score: 11.0   memory length: 535940   epsilon: 0.1368368200085568    steps: 544    lr: 2.560000000000001e-06     evaluation reward: 10.67\n",
      "episode: 1959   score: 13.0   memory length: 536458   epsilon: 0.1358111800085503    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 10.73\n",
      "episode: 1960   score: 7.0   memory length: 536831   epsilon: 0.13507264000854563    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 10.65\n",
      "episode: 1961   score: 9.0   memory length: 537285   epsilon: 0.13417372000853994    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 10.64\n",
      "episode: 1962   score: 12.0   memory length: 537862   epsilon: 0.13303126000853271    steps: 577    lr: 2.560000000000001e-06     evaluation reward: 10.68\n",
      "episode: 1963   score: 12.0   memory length: 538432   epsilon: 0.13190266000852557    steps: 570    lr: 2.560000000000001e-06     evaluation reward: 10.77\n",
      "episode: 1964   score: 10.0   memory length: 538981   epsilon: 0.1308156400085187    steps: 549    lr: 2.560000000000001e-06     evaluation reward: 10.73\n",
      "episode: 1965   score: 10.0   memory length: 539486   epsilon: 0.12981574000851237    steps: 505    lr: 2.560000000000001e-06     evaluation reward: 10.69\n",
      "episode: 1966   score: 19.0   memory length: 540177   epsilon: 0.12844756000850371    steps: 691    lr: 2.560000000000001e-06     evaluation reward: 10.77\n",
      "episode: 1967   score: 12.0   memory length: 540754   epsilon: 0.12730510000849649    steps: 577    lr: 2.560000000000001e-06     evaluation reward: 10.81\n",
      "episode: 1968   score: 10.0   memory length: 541216   epsilon: 0.1263903400084907    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 10.81\n",
      "episode: 1969   score: 12.0   memory length: 541807   epsilon: 0.1252201600084833    steps: 591    lr: 2.560000000000001e-06     evaluation reward: 10.84\n",
      "episode: 1970   score: 12.0   memory length: 542393   epsilon: 0.12405988000848255    steps: 586    lr: 2.560000000000001e-06     evaluation reward: 10.89\n",
      "episode: 1971   score: 9.0   memory length: 542902   epsilon: 0.12305206000848323    steps: 509    lr: 2.560000000000001e-06     evaluation reward: 10.89\n",
      "episode: 1972   score: 9.0   memory length: 543405   epsilon: 0.12205612000848391    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 10.9\n",
      "episode: 1973   score: 10.0   memory length: 543935   epsilon: 0.12100672000848463    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 10.93\n",
      "episode: 1974   score: 13.0   memory length: 544513   epsilon: 0.11986228000848541    steps: 578    lr: 2.560000000000001e-06     evaluation reward: 10.96\n",
      "episode: 1975   score: 21.0   memory length: 545199   epsilon: 0.11850400000848633    steps: 686    lr: 2.560000000000001e-06     evaluation reward: 10.97\n",
      "episode: 1976   score: 9.0   memory length: 545663   epsilon: 0.11758528000848696    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 10.93\n",
      "episode: 1977   score: 13.0   memory length: 546268   epsilon: 0.11638738000848778    steps: 605    lr: 2.560000000000001e-06     evaluation reward: 10.93\n",
      "episode: 1978   score: 16.0   memory length: 546856   epsilon: 0.11522314000848857    steps: 588    lr: 2.560000000000001e-06     evaluation reward: 11.0\n",
      "episode: 1979   score: 13.0   memory length: 547446   epsilon: 0.11405494000848937    steps: 590    lr: 2.560000000000001e-06     evaluation reward: 10.99\n",
      "episode: 1980   score: 14.0   memory length: 548091   epsilon: 0.11277784000849024    steps: 645    lr: 2.560000000000001e-06     evaluation reward: 11.04\n",
      "episode: 1981   score: 16.0   memory length: 548689   epsilon: 0.11159380000849105    steps: 598    lr: 2.560000000000001e-06     evaluation reward: 11.09\n",
      "episode: 1982   score: 16.0   memory length: 549286   epsilon: 0.11041174000849185    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 11.14\n",
      "episode: 1983   score: 14.0   memory length: 549869   epsilon: 0.10925740000849264    steps: 583    lr: 2.560000000000001e-06     evaluation reward: 11.18\n",
      "episode: 1984   score: 7.0   memory length: 550275   epsilon: 0.10845352000849319    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 11.14\n",
      "episode: 1985   score: 11.0   memory length: 550846   epsilon: 0.10732294000849396    steps: 571    lr: 2.560000000000001e-06     evaluation reward: 11.19\n",
      "episode: 1986   score: 14.0   memory length: 551397   epsilon: 0.1062319600084947    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 11.2\n",
      "episode: 1987   score: 13.0   memory length: 552022   epsilon: 0.10499446000849555    steps: 625    lr: 2.560000000000001e-06     evaluation reward: 11.2\n",
      "episode: 1988   score: 10.0   memory length: 552513   epsilon: 0.10402228000849621    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 11.23\n",
      "episode: 1989   score: 5.0   memory length: 552807   epsilon: 0.10344016000849661    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 11.19\n",
      "episode: 1990   score: 14.0   memory length: 553310   epsilon: 0.10244422000849729    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 11.25\n",
      "episode: 1991   score: 14.0   memory length: 553965   epsilon: 0.10114732000849817    steps: 655    lr: 2.560000000000001e-06     evaluation reward: 11.3\n",
      "episode: 1992   score: 15.0   memory length: 554647   epsilon: 0.0997969600084991    steps: 682    lr: 2.560000000000001e-06     evaluation reward: 11.27\n",
      "episode: 1993   score: 15.0   memory length: 555248   epsilon: 0.0986069800084999    steps: 601    lr: 2.560000000000001e-06     evaluation reward: 11.37\n",
      "episode: 1994   score: 13.0   memory length: 555780   epsilon: 0.09755362000850062    steps: 532    lr: 2.560000000000001e-06     evaluation reward: 11.44\n",
      "episode: 1995   score: 7.0   memory length: 556209   epsilon: 0.0967042000085012    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 11.41\n",
      "episode: 1996   score: 14.0   memory length: 556859   epsilon: 0.09541720000850208    steps: 650    lr: 2.560000000000001e-06     evaluation reward: 11.44\n",
      "episode: 1997   score: 27.0   memory length: 557760   epsilon: 0.0936332200085033    steps: 901    lr: 2.560000000000001e-06     evaluation reward: 11.61\n",
      "episode: 1998   score: 13.0   memory length: 558205   epsilon: 0.0927521200085039    steps: 445    lr: 2.560000000000001e-06     evaluation reward: 11.68\n",
      "episode: 1999   score: 11.0   memory length: 558721   epsilon: 0.0917304400085046    steps: 516    lr: 2.560000000000001e-06     evaluation reward: 11.73\n",
      "episode: 2000   score: 20.0   memory length: 559317   epsilon: 0.0905503600085054    steps: 596    lr: 2.560000000000001e-06     evaluation reward: 11.76\n",
      "episode: 2001   score: 9.0   memory length: 559743   epsilon: 0.08970688000850598    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 11.72\n",
      "episode: 2002   score: 7.0   memory length: 560145   epsilon: 0.08891092000850652    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 11.7\n",
      "episode: 2003   score: 8.0   memory length: 560599   epsilon: 0.08801200000850713    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 11.69\n",
      "episode: 2004   score: 8.0   memory length: 561032   epsilon: 0.08715466000850772    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 11.65\n",
      "episode: 2005   score: 12.0   memory length: 561602   epsilon: 0.08602606000850849    steps: 570    lr: 2.560000000000001e-06     evaluation reward: 11.63\n",
      "episode: 2006   score: 10.0   memory length: 562094   epsilon: 0.08505190000850915    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 11.62\n",
      "episode: 2007   score: 14.0   memory length: 562596   epsilon: 0.08405794000850983    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 11.48\n",
      "episode: 2008   score: 17.0   memory length: 563252   epsilon: 0.08275906000851072    steps: 656    lr: 2.560000000000001e-06     evaluation reward: 11.52\n",
      "episode: 2009   score: 12.0   memory length: 563837   epsilon: 0.0816007600085115    steps: 585    lr: 2.560000000000001e-06     evaluation reward: 11.57\n",
      "episode: 2010   score: 11.0   memory length: 564371   epsilon: 0.08054344000851223    steps: 534    lr: 2.560000000000001e-06     evaluation reward: 11.56\n",
      "episode: 2011   score: 8.0   memory length: 564782   epsilon: 0.07972966000851278    steps: 411    lr: 2.560000000000001e-06     evaluation reward: 11.55\n",
      "episode: 2012   score: 6.0   memory length: 565092   epsilon: 0.0791158600085132    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 11.5\n",
      "episode: 2013   score: 21.0   memory length: 565869   epsilon: 0.07757740000851425    steps: 777    lr: 2.560000000000001e-06     evaluation reward: 11.56\n",
      "episode: 2014   score: 10.0   memory length: 566404   epsilon: 0.07651810000851497    steps: 535    lr: 2.560000000000001e-06     evaluation reward: 11.49\n",
      "episode: 2015   score: 13.0   memory length: 567041   epsilon: 0.07525684000851583    steps: 637    lr: 2.560000000000001e-06     evaluation reward: 11.56\n",
      "episode: 2016   score: 6.0   memory length: 567398   epsilon: 0.07454998000851631    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 11.47\n",
      "episode: 2017   score: 12.0   memory length: 568064   epsilon: 0.07323130000851721    steps: 666    lr: 2.560000000000001e-06     evaluation reward: 11.5\n",
      "episode: 2018   score: 14.0   memory length: 568565   epsilon: 0.07223932000851789    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 11.56\n",
      "episode: 2019   score: 7.0   memory length: 568959   epsilon: 0.07145920000851842    steps: 394    lr: 2.560000000000001e-06     evaluation reward: 11.49\n",
      "episode: 2020   score: 17.0   memory length: 569708   epsilon: 0.06997618000851943    steps: 749    lr: 2.560000000000001e-06     evaluation reward: 11.52\n",
      "episode: 2021   score: 16.0   memory length: 570321   epsilon: 0.06876244000852026    steps: 613    lr: 2.560000000000001e-06     evaluation reward: 11.57\n",
      "episode: 2022   score: 11.0   memory length: 570838   epsilon: 0.06773878000852096    steps: 517    lr: 2.560000000000001e-06     evaluation reward: 11.55\n",
      "episode: 2023   score: 10.0   memory length: 571342   epsilon: 0.06674086000852164    steps: 504    lr: 2.560000000000001e-06     evaluation reward: 11.57\n",
      "episode: 2024   score: 12.0   memory length: 571928   epsilon: 0.06558058000852243    steps: 586    lr: 2.560000000000001e-06     evaluation reward: 11.59\n",
      "episode: 2025   score: 8.0   memory length: 572341   epsilon: 0.06476284000852299    steps: 413    lr: 2.560000000000001e-06     evaluation reward: 11.59\n",
      "episode: 2026   score: 11.0   memory length: 572878   epsilon: 0.06369958000852372    steps: 537    lr: 2.560000000000001e-06     evaluation reward: 11.51\n",
      "episode: 2027   score: 21.0   memory length: 573454   epsilon: 0.06255910000852449    steps: 576    lr: 2.560000000000001e-06     evaluation reward: 11.62\n",
      "episode: 2028   score: 14.0   memory length: 574102   epsilon: 0.06127606000852537    steps: 648    lr: 2.560000000000001e-06     evaluation reward: 11.66\n",
      "episode: 2029   score: 9.0   memory length: 574542   epsilon: 0.06040486000852596    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 11.66\n",
      "episode: 2030   score: 15.0   memory length: 575224   epsilon: 0.05905450000852688    steps: 682    lr: 2.560000000000001e-06     evaluation reward: 11.73\n",
      "episode: 2031   score: 6.0   memory length: 575544   epsilon: 0.058420900008527316    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 11.63\n",
      "episode: 2032   score: 14.0   memory length: 576165   epsilon: 0.057191320008528154    steps: 621    lr: 2.560000000000001e-06     evaluation reward: 11.72\n",
      "episode: 2033   score: 18.0   memory length: 576870   epsilon: 0.055795420008529106    steps: 705    lr: 2.560000000000001e-06     evaluation reward: 11.83\n",
      "episode: 2034   score: 13.0   memory length: 577369   epsilon: 0.05480740000852978    steps: 499    lr: 2.560000000000001e-06     evaluation reward: 11.91\n",
      "episode: 2035   score: 10.0   memory length: 577836   epsilon: 0.05388274000853041    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 11.94\n",
      "episode: 2036   score: 14.0   memory length: 578516   epsilon: 0.05253634000853133    steps: 680    lr: 2.560000000000001e-06     evaluation reward: 11.96\n",
      "episode: 2037   score: 8.0   memory length: 578971   epsilon: 0.051635440008531944    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 11.92\n",
      "episode: 2038   score: 13.0   memory length: 579545   epsilon: 0.05049892000853272    steps: 574    lr: 2.560000000000001e-06     evaluation reward: 11.9\n",
      "episode: 2039   score: 10.0   memory length: 580075   epsilon: 0.049449520008533435    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 11.93\n",
      "episode: 2040   score: 10.0   memory length: 580579   epsilon: 0.048451600008534115    steps: 504    lr: 2.560000000000001e-06     evaluation reward: 11.98\n",
      "episode: 2041   score: 10.0   memory length: 581109   epsilon: 0.04740220000853483    steps: 530    lr: 2.560000000000001e-06     evaluation reward: 11.96\n",
      "episode: 2042   score: 10.0   memory length: 581631   epsilon: 0.046368640008535536    steps: 522    lr: 2.560000000000001e-06     evaluation reward: 11.97\n",
      "episode: 2043   score: 23.0   memory length: 582422   epsilon: 0.044802460008536604    steps: 791    lr: 2.560000000000001e-06     evaluation reward: 12.06\n",
      "episode: 2044   score: 14.0   memory length: 582925   epsilon: 0.043806520008537284    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 12.12\n",
      "episode: 2045   score: 15.0   memory length: 583508   epsilon: 0.04265218000853807    steps: 583    lr: 2.560000000000001e-06     evaluation reward: 12.16\n",
      "episode: 2046   score: 6.0   memory length: 583831   epsilon: 0.04201264000853851    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 12.08\n",
      "episode: 2047   score: 9.0   memory length: 584332   epsilon: 0.041020660008539184    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 12.07\n",
      "episode: 2048   score: 13.0   memory length: 584969   epsilon: 0.039759400008540044    steps: 637    lr: 2.560000000000001e-06     evaluation reward: 12.09\n",
      "episode: 2049   score: 9.0   memory length: 585433   epsilon: 0.03884068000854067    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 12.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Start training after random sample generation\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(frame \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_frame): \u001b[38;5;66;03m# You can set train_frame to a lower value while testing your starts training earlier\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Update the target network only for Double DQN only\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m double_dqn \u001b[38;5;129;01mand\u001b[39;00m (frame \u001b[38;5;241m%\u001b[39m update_target_network_frequency)\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/private/UIUC-CS444-DL-for-cv/assignment5_materials/agent_double.py:88\u001b[0m, in \u001b[0;36mAgent.train_policy_net\u001b[0;34m(self, frame, device)\u001b[0m\n\u001b[1;32m     86\u001b[0m history \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(mini_batch[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(history[:, :\u001b[38;5;241m4\u001b[39m, :, :]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[0;32m---> 88\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mini_batch[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     90\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(actions)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAv0lEQVR4nO3deXQUZd728asTSBMSkhCWECBAQBZZjAvCYQmiRBBxwXFBRAcYlyPiKOAGMyPgjBpXBnUUdOYZwUdHERX0dQRFZBEFREEURARlCUvYk0CAkKTv9488adJJJ3RCp6ur+/s5p07SVdXVv+5KUlfu+64qhzHGCAAAwIYirC4AAACgpggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyQAiZOnWqHA5HQF9z+/btcjgcmjVrVkBfF2fP4XBo6tSpVpcBnBWCDGCRWbNmyeFwVDqtWrXK6hLDVvl9U6dOHbVo0UKjRo3S7t27rS4PQBl1rC4ACHd//etflZqaWmH+OeecU+1t/eUvf9HEiRP9URZ0et+cPHlSq1at0qxZs7RixQpt2LBB9erVs7o8ACLIAJYbPHiwunfv7pdt1alTR3Xq8GvtL2X3zR133KHGjRvr6aef1kcffaSbbrrJ4urOLD8/XzExMVaXAdQqupaAIFc6BuW5557T3//+d7Vu3VrR0dG65JJLtGHDBo91vY2RWbRokfr27auEhATFxsaqY8eO+tOf/uSxzv79+3X77bcrKSlJ9erVU1pammbPnl2hlpycHI0aNUrx8fFKSEjQyJEjlZOT47Xun3/+WTfccIMSExNVr149de/eXR999JHHOoWFhXrsscfUvn171atXT40aNVLfvn21aNGiSj+Pb7/9Vg6Hw2t9n376qRwOhz7++GNJ0tGjRzVu3Di1adNGTqdTTZs21eWXX661a9dWuv2qpKenS5J+/fXXar3XnJwcRUZG6sUXX3TPO3jwoCIiItSoUSMZY9zzx4wZo2bNmrkff/nll7rxxhvVqlUrOZ1OpaSkaPz48Tpx4oRHDaNGjVJsbKx+/fVXXXnllWrQoIFGjBghSSooKND48ePVpEkTNWjQQNdcc4127dpVo88ACDb86wZYLDc3VwcPHvSY53A41KhRI495b7zxho4ePaqxY8fq5MmTeuGFF3TZZZfpxx9/VFJSktdtb9y4UVdddZXOO+88/fWvf5XT6dTWrVv11Vdfudc5ceKE+vfvr61bt+ree+9Vamqq5s6dq1GjRiknJ0f333+/JMkYo2uvvVYrVqzQ3XffrXPPPVfz5s3TyJEjvb5unz591KJFC02cOFExMTF69913NXToUL3//vu67rrrJJUEr8zMTN1xxx3q0aOH8vLy9O2332rt2rW6/PLLvb6n7t27q23btnr33XcrvPacOXPUsGFDDRo0SJJ0991367333tO9996rzp0769ChQ1qxYoU2bdqkCy+8sKrd4tX27dslSQ0bNqzWe01ISFDXrl21fPly3XfffZKkFStWyOFw6PDhw/rpp5/UpUsXSSXBpTQwSdLcuXN1/PhxjRkzRo0aNdI333yjl156Sbt27dLcuXM96isqKtKgQYPUt29fPffcc6pfv76kktakN998U7fccot69+6tL774QkOGDKn2+weCkgFgiddff91I8jo5nU73etu2bTOSTHR0tNm1a5d7/urVq40kM378ePe8KVOmmLK/1n//+9+NJHPgwIFK65g+fbqRZN588033vFOnTplevXqZ2NhYk5eXZ4wxZv78+UaSeeaZZ9zrFRUVmfT0dCPJvP766+75AwYMMN26dTMnT550z3O5XKZ3796mffv27nlpaWlmyJAhvn5kbpMmTTJ169Y1hw8fds8rKCgwCQkJ5g9/+IN7Xnx8vBk7dmy1t1+6bz7//HNz4MABk5WVZd577z3TpEkT43Q6TVZWlntdX9/r2LFjTVJSkvvxhAkTTL9+/UzTpk3NjBkzjDHGHDp0yDgcDvPCCy+41zt+/HiF+jIzM43D4TA7duxwzxs5cqSRZCZOnOix7vfff28kmXvuucdj/i233GIkmSlTplTz0wGCC11LgMVefvllLVq0yGNasGBBhfWGDh2qFi1auB/36NFDPXv21CeffFLpthMSEiRJH374oVwul9d1PvnkEzVr1kzDhw93z6tbt67uu+8+HTt2TMuWLXOvV6dOHY0ZM8a9XmRkpP74xz96bO/w4cP64osvdNNNN+no0aM6ePCgDh48qEOHDmnQoEHasmWL+8yfhIQEbdy4UVu2bDnDp+Rp2LBhKiws1AcffOCe99lnnyknJ0fDhg3zeP+rV6/Wnj17qrX9UhkZGWrSpIlSUlJ0ww03KCYmRh999JFatmxZ7feanp6uffv2afPmzZJKWl769eun9PR0ffnll5JKWmmMMR4tMtHR0e7v8/PzdfDgQfXu3VvGGK1bt65CzWX3jyT3z0dpS1CpcePG1egzAYINQQawWI8ePZSRkeExXXrppRXWa9++fYV5HTp0cHd3eDNs2DD16dNHd9xxh5KSknTzzTfr3Xff9Qg1O3bsUPv27RUR4fnn4Nxzz3UvL/2anJys2NhYj/U6duzo8Xjr1q0yxujRRx9VkyZNPKYpU6ZIKhmTI5WcFZSTk6MOHTqoW7dueuihh/TDDz9U+n5KpaWlqVOnTpozZ4573pw5c9S4cWNddtll7nnPPPOMNmzYoJSUFPXo0UNTp07Vb7/9dsbtlyoNme+9956uvPJKHTx4UE6ns0bvtTScfPnll8rPz9e6deuUnp6ufv36uYPMl19+qbi4OKWlpblfY+fOnRo1apQSExMVGxurJk2a6JJLLpFU0i1ZVp06ddwhq9SOHTsUERGhdu3aecwvv98Au2KMDBDCoqOjtXz5ci1ZskT//e9/tXDhQs2ZM0eXXXaZPvvsM0VGRvr9NUtD0oMPPugeq1Je6anl/fr106+//qoPP/xQn332mf71r3/p73//u2bOnKk77rijytcZNmyYnnjiCR08eFANGjTQRx99pOHDh3uctXXTTTcpPT1d8+bN02effaZnn31WTz/9tD744AMNHjz4jO+lR48e7rOWhg4dqr59++qWW27R5s2bFRsbW6332rx5c6Wmpmr58uVq06aNjDHq1auXmjRpovvvv187duzQl19+qd69e7tDZXFxsS6//HIdPnxYjzzyiDp16qSYmBjt3r1bo0aNqtDK5nQ6KwRSINQRZACb8Nb98ssvv6hNmzZVPi8iIkIDBgzQgAEDNG3aND355JP685//rCVLligjI0OtW7fWDz/8IJfL5XEQ/PnnnyVJrVu3dn9dvHixjh075tEqU9pVUqpt27aSSrqnMjIyzvi+EhMTNXr0aI0ePVrHjh1Tv379NHXqVJ+CzGOPPab3339fSUlJysvL080331xhveTkZN1zzz265557tH//fl144YV64oknfAoyZUVGRiozM1OXXnqp/vGPf2jixInVfq/p6elavny5UlNTdf7556tBgwZKS0tTfHy8Fi5cqLVr1+qxxx5zr//jjz/ql19+0ezZs/X73//ePb+qs7rKa926tVwul3799VePVpjy+w2wK6I7YBPz58/3uKrsN998o9WrV1d5QD58+HCFeeeff76kklNyJenKK69Udna2RzdNUVGRXnrpJcXGxrq7Ma688koVFRVpxowZ7vWKi4v10ksveWy/adOm6t+/v1599VXt3bu3wusfOHDA/f2hQ4c8lsXGxuqcc85x11aVc889V926ddOcOXM0Z84cJScnq1+/fh61le96adq0qZo3b+7T9r3p37+/evTooenTp+vkyZPVeq9SSZDZvn275syZ4+5qioiIUO/evTVt2jQVFhZ6jI8pbTEzZU7PNsbohRde8Lnm0p+Psqd+S9L06dN93gYQzGiRASy2YMECd+tHWb1793b/xy+VdFH07dtXY8aMUUFBgaZPn65GjRrp4YcfrnTbf/3rX7V8+XINGTJErVu31v79+/XKK6+oZcuW6tu3ryTprrvu0quvvqpRo0bpu+++U5s2bfTee+/pq6++0vTp09WgQQNJ0tVXX60+ffpo4sSJ2r59uzp37qwPPvigQliQSsaW9O3bV926ddOdd96ptm3bat++fVq5cqV27dql9evXS5I6d+6s/v3766KLLlJiYqK+/fZb9+nSvhg2bJgmT56sevXq6fbbb/doUTp69KhatmypG264QWlpaYqNjdXnn3+uNWvW6Pnnn/dp+9489NBDuvHGGzVr1izdfffdPr9X6fQ4mc2bN+vJJ590z+/Xr58WLFggp9Opiy++2D2/U6dOateunR588EHt3r1bcXFxev/993XkyBGf6z3//PM1fPhwvfLKK8rNzVXv3r21ePFibd26tcafARBULDxjCghrVZ1+rTKnM5eefv3ss8+a559/3qSkpBin02nS09PN+vXrPbZZ/vTrxYsXm2uvvdY0b97cREVFmebNm5vhw4ebX375xeN5+/btM6NHjzaNGzc2UVFRplu3bh6nU5c6dOiQue2220xcXJyJj483t912m1m3bl2F06+NMebXX381v//9702zZs1M3bp1TYsWLcxVV11l3nvvPfc6jz/+uOnRo4dJSEgw0dHRplOnTuaJJ54wp06d8ukz3LJli/vzWrFihceygoIC89BDD5m0tDTToEEDExMTY9LS0swrr7xyxu2W7ps1a9ZUWFZcXGzatWtn2rVrZ4qKinx+r6WaNm1qJJl9+/a5561YscJIMunp6RXW/+mnn0xGRoaJjY01jRs3NnfeeadZv359hc985MiRJiYmxuv7OXHihLnvvvtMo0aNTExMjLn66qtNVlYWp18jJDiMKdNmCSDobN++XampqXr22Wf14IMPWl0OAAQVxsgAAADbIsgAAADbIsgAAADbYowMAACwLVpkAACAbRFkAACAbYX8BfFcLpf27NmjBg0ayOFwWF0OAADwgTFGR48eVfPmzau8h1jIB5k9e/YoJSXF6jIAAEANZGVlVbire1khH2RKL6+elZWluLg4i6sBAAC+yMvLU0pKivs4XpmQDzKl3UlxcXEEGQAAbOZMw0IY7AsAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAGyLIAMAAKpt927ps88kY6ytI+Tvfg0AAPyvSxcpN1dq1Eg6eNC6OixtkVm+fLmuvvpqNW/eXA6HQ/Pnz3cvKyws1COPPKJu3bopJiZGzZs31+9//3vt2bPHuoIBAICkkhAjSYcOWVuHpUEmPz9faWlpevnllyssO378uNauXatHH31Ua9eu1QcffKDNmzfrmmuusaBSAABQmdmzrXttS7uWBg8erMGDB3tdFh8fr0WLFnnM+8c//qEePXpo586datWqVSBKBAAA5RQXez7evNmaOiSbjZHJzc2Vw+FQQkJCpesUFBSooKDA/TgvLy8AlQEAED6ee+7095GRUq9e1tVimyBz8uRJPfLIIxo+fLji4uIqXS8zM1OPPfZYACsDACB8XHSRtHbt6cfHjkn16llXjy1Ovy4sLNRNN90kY4xmzJhR5bqTJk1Sbm6ue8rKygpQlQAAhL6yIUaSTp2ypo5SQd8iUxpiduzYoS+++KLK1hhJcjqdcjqdAaoOAIDwkZPj+bhePekMh+VaF9RBpjTEbNmyRUuWLFGjRo2sLgkAgLA1ZYrn440bramjLEuDzLFjx7R161b3423btun7779XYmKikpOTdcMNN2jt2rX6+OOPVVxcrOzsbElSYmKioqKirCobAICw9NZbp7//73+ltm2tq6WUwxjrLi68dOlSXXrppRXmjxw5UlOnTlVqaqrX5y1ZskT9+/f36TXy8vIUHx+v3NzcM3ZLAQAQ7nJzpSNHpDZtKi6LiDh9S4LaTg++Hr8tbZHp37+/qspRFmYsAADCUtkrnJQ/DAfjYdkWZy0BAAB4Q5ABAABeORynv9+yxbo6qkKQAQAAkqruOurQIXB1VAdBBgAASCoZzFvewYOBr6M6gvo6MgAAwFpNmlScd8EFga+jMrTIAACAail/mwIrEWQAAIBtEWQAAIDPrLzTtTeMkQEAAD7hgngAAMAWjCmZDh2SioqCM8RItMgAAABJLpf3+YmJga2jumiRAQAAiow8/X2wjYOpCkEGAAB4OHHC6gp8R5ABAAAegnU8jDcEGQAAwlhxsefNIUvn2QVBBgCAMFbHy2k/3uYFK4IMAABwGzPG6gqqhyADAAAkSdHR0iuvWF1F9RBkAACAJOn4casrqD6CDAAAYcpOp1lXhiADAECYql/f6grOHkEGAIAwVP6U6z/8wZo6zhZBBgAA6H/+x+oKaoYgAwBAmOne3eoK/IcgAwBAmPnuO8/HdrolQXkEGQAAwlhhodUVnB2CDAAAYaT8IF873Y7AG4IMAABhonyICQUEGQAAYFsEGQAAwpTLZXUFZ48gAwBAmLL7QF+JIAMAQFh66ikpKsrqKs4eQQYAgBCXk1NxoO8jj1hSit8RZAAACHENG3o+Li62po7aQJABACCE5eZWnBcRQkf/EHorAACgvIQEqyuoXQQZAABgWwQZAABCiMNxenr5Zc9lBQX2vkGkNwQZAABC1L33ej4OhdOtyyPIAAAQIoqKKl/20kuBqyOQCDIAAISIunUrX1a+dSZUEGQAAAhxTz5pdQW1hyADAECImzTJ6gpqD0EGAIAQUP4WBOHC0iCzfPlyXX311WrevLkcDofmz5/vsdwYo8mTJys5OVnR0dHKyMjQli1brCkWAIAgdeSI1RVYx9Igk5+fr7S0NL1c/kT3//PMM8/oxRdf1MyZM7V69WrFxMRo0KBBOnnyZIArBQAgeCUmej42xnMKZXWsfPHBgwdr8ODBXpcZYzR9+nT95S9/0bXXXitJeuONN5SUlKT58+fr5ptvDmSpAAAgCAXtGJlt27YpOztbGRkZ7nnx8fHq2bOnVq5caWFlAAAEj/KdFPv2WVOHVSxtkalKdna2JCkpKcljflJSknuZNwUFBSooKHA/zsvLq50CAQAIAtHRno+bNrWmDqsEbYtMTWVmZio+Pt49paSkWF0SAACoJUEbZJo1ayZJ2leujWzfvn3uZd5MmjRJubm57ikrK6tW6wQAIFCKi6Xjxytf7nIFrpZgEbRBJjU1Vc2aNdPixYvd8/Ly8rR69Wr16tWr0uc5nU7FxcV5TAAAhII6daSYmJJrxngLLeF4LRlLx8gcO3ZMW7dudT/etm2bvv/+eyUmJqpVq1YaN26cHn/8cbVv316pqal69NFH1bx5cw0dOtS6ogEAsED5kBIZaU0dwcbSIPPtt9/q0ksvdT+eMGGCJGnkyJGaNWuWHn74YeXn5+uuu+5STk6O+vbtq4ULF6pevXpWlQwAAIKIw5jQvlROXl6e4uPjlZubSzcTAMC2ztRtFGpHc1+P30E7RgYAgHBXWCidOOHb2JdwvdpI0F5HBgCAcFZcLEVF+b5+gwa1V0swo0UGAIAgVMeHpoYrryz56nKF5xlLEi0yAAAElXAbC3O2aJEBAMAmyt9XCbTIAABgOV9bWcLxyr1nQpABAMBC1RnbUv4GkSDIAABgGV9DTJs20nnn1WoptkWQAQAgyG3bZnUFwYsgAwCABY4cqXo5Zyf5hrOWAACwQGKi9/kuFyGmOggyAAAEiXC+sF1NEWQAAAiw4mLPxz/+WNIKQ4ipPoIMAAABVv72A127WlNHKCDIAABgodatra7A3ggyAAAEUPnuo+3bLSkjZBBkAAAIEG9jYLjtwNkhyAAAYCEG+J4dggwAABYiyJwdruwLAIAFuOidf9AiAwBAANDyUjsIMgAA1LLyF8CD/xBkAACoBYWFJa0wxlS8AB5nKvkPQQYAgFoQFVXyNcLLkZZuJv8hyAAAEEAM8vUvggwAALAtggwAALAtggwAALAtggwAAH5W2WBexsf4H0EGAADYFkEGAIBaVFgobdpEa0xtIcgAAFCL6tSROnWyuorQRZABAAC2RZABAAC2RZABAOAsOBynp/x8bj8QaAQZAAD8JDbW83FBgTV1hBOCDAAANXSm1pfSG0ei9tQ58yoAAECqXrcRp1sHBi0yAAD4oDrdRCdO1F4d8ESQAQDAB/Xq1c66ODsEGQAA/Kiw0OoKwgtBBgCAGjp2rGQsjDHSqVMlX+sw+jSg+LgBAKiB8oN569a1po5wR4sMAABn4K276PDhwNeBiggyAACcgbfrwSQmBr4OVBTUQaa4uFiPPvqoUlNTFR0drXbt2ulvf/ubDCfnAwACoPTWA2WVjolBcAjqMTJPP/20ZsyYodmzZ6tLly769ttvNXr0aMXHx+u+++6zujwAAGCxoA4yX3/9ta699loNGTJEktSmTRu9/fbb+uabbyyuDAAQqrjpo70EdddS7969tXjxYv3yyy+SpPXr12vFihUaPHhwpc8pKChQXl6exwQAAEJTULfITJw4UXl5eerUqZMiIyNVXFysJ554QiNGjKj0OZmZmXrssccCWCUAIBScPClFR1tdBaorqFtk3n33Xb311lv6z3/+o7Vr12r27Nl67rnnNHv27EqfM2nSJOXm5rqnrKysAFYMALCrqkLMb78xyDdYOUwQnwKUkpKiiRMnauzYse55jz/+uN588039/PPPPm0jLy9P8fHxys3NVVxcXG2VCgCwsTONiwneI2Xo8vX4HdQtMsePH1dEhGeJkZGRcrlcFlUEAACCSVCPkbn66qv1xBNPqFWrVurSpYvWrVunadOm6Q9/+IPVpQEAQlhOjrRjh9SlixQZaXU1qEpQdy0dPXpUjz76qObNm6f9+/erefPmGj58uCZPnqwob5dZ9IKuJQBAVQ4dkho3Pv04eI+K4cXX43dQBxl/IMgAALypbFxMaB8V7SMkxsgAAFAbuOhd6CDIAADCQlaW93snwd6CerAvAAD+4Gt4oVvJfmiRAQCENF/DCSHGnggyAICQFuHDke7AgdqvA7WDriUAQMiqrJXl1Cmpbt3A1oLaQYsMACBkVdYa40srDeyBFhkAQNhgHEzoIZMCAMJCQYHVFaA2EGQAACHlt99Kvh4+7DnfxzvbwGboWgIAhASXixs8hiOCDADA9rhab/iiawkAANgWQQYAYGu+tMYUF9d+HbAGXUsAgJDGKdehjRYZAIBtMTYGBBkAAGBbBBkAQEgoLi7pRirblXTwoHX1IDAYIwMAsKXyY1/K3j+JcTHhgxYZAIAtceNHSH4KMnl5eZo/f742bdrkj80BAAD4pEZB5qabbtI//vEPSdKJEyfUvXt33XTTTTrvvPP0/vvv+7VAAADKK3+2kstlTR2wXo2CzPLly5Weni5JmjdvnowxysnJ0YsvvqjHH3/crwUCAHAmnIYdvmoUZHJzc5WYmChJWrhwoa6//nrVr19fQ4YM0ZYtW/xaIAAAVTl0yOoKYKUaBZmUlBStXLlS+fn5WrhwoQYOHChJOnLkiOrVq+fXAgEAqMr//V+NMFWj06/HjRunESNGKDY2Vq1bt1b//v0llXQ5devWzZ/1AQAAVKpGQeaee+5Rjx49lJWVpcsvv1wR/3cOXNu2bRkjAwCoVYyHQVkOY0L7skF5eXmKj49Xbm6u4uLirC4HAHCWygeZ0D6KhS9fj98+t8hMmDDB5xefNm2az+sCAFBTRUVWVwCr+Rxk1q1b5/F47dq1KioqUseOHSVJv/zyiyIjI3XRRRf5t0IAAP5P+evF0BoDn4PMkiVL3N9PmzZNDRo00OzZs9WwYUNJJWcsjR492n19GQAA/MHlKulOcjikyEjPZXW4Y2DYq9EYmRYtWuizzz5Tly5dPOZv2LBBAwcO1J49e/xW4NlijAwA2NeOHVKbNpUvp0UmdPl6/K7RdWTy8vJ04MCBCvMPHDigo0eP1mSTAABUUFWIAaQaBpnrrrtOo0eP1gcffKBdu3Zp165dev/993X77bfrd7/7nb9rBACgAlpjINXwOjIzZ87Ugw8+qFtuuUWFhYUlG6pTR7fffrueffZZvxYIAAhPVV0vhhCDUtUeI1NcXKyvvvpK3bp1U1RUlH799VdJUrt27RQTE1MrRZ4NxsgAgD1VFmSOH5eiowNbCwLP79eRKRUZGamBAwdq06ZNSk1N1XnnnXdWhQIA4CtaYlBejcbIdO3aVb/99pu/awEAwH2qNeCLGgWZxx9/XA8++KA+/vhj7d27V3l5eR4TAABAINToOjKlN4mUJEeZ2GyMkcPhUHFxsX+q8wPGyACAfRQUSPXqec6jOyk81doYGcnzKr8AAPhL+RADnEmNgswll1zi7zoAAACq7azuUnH8+HHt3LlTp06d8pjPmUwAAH84ftzqChDsahRkDhw4oNGjR2vBggVelwfTGBkAgD2UP1OJsTHwRY3OWho3bpxycnK0evVqRUdHa+HChZo9e7bat2+vjz76yK8F7t69W7feeqsaNWqk6OhodevWTd9++61fXwMAANhTjVpkvvjiC3344Yfq3r27IiIi1Lp1a11++eWKi4tTZmamhgwZ4pfijhw5oj59+ujSSy/VggUL1KRJE23ZskUNGzb0y/YBAIFTtsWltLWlsuvF7NxZ+/UgNNQoyOTn56tp06aSpIYNG+rAgQPq0KGDunXrprVr1/qtuKefflopKSl6/fXX3fNSU1P9tn0AQGCUDyx790rJyZWvn5JSu/UgdNSoa6ljx47avHmzJCktLU2vvvqqdu/erZkzZyq5qp/Mavroo4/UvXt33XjjjWratKkuuOAC/fOf/6zyOQUFBVygDwCCXPPmXL0X/lGjIHP//fdr7969kqQpU6ZowYIFatWqlV588UU9+eSTfivut99+04wZM9S+fXt9+umnGjNmjO677z7Nnj270udkZmYqPj7ePaUQ6wEgoI4ckTZuLPme2w2gttXoyr7lHT9+XD///LNatWqlxo0b+6MuSVJUVJS6d++ur7/+2j3vvvvu05o1a7Ry5UqvzykoKFBBQYH7cV5enlJSUriyLwAEyNkGF85WguT7lX1r1CJT/oaR9evX14UXXujXECNJycnJ6ty5s8e8c889VzurGAXmdDoVFxfnMQEAap8/Wl+ysvxTC8JHjQb7nnPOOWrZsqUuueQS9e/fX5dcconOOeccf9emPn36uMfilPrll1/UunVrv78WAMBaBQVSVJTVVcBuatQik5WVpczMTEVHR+uZZ55Rhw4d1LJlS40YMUL/+te//Fbc+PHjtWrVKj355JPaunWr/vOf/+i1117T2LFj/fYaAIDAqKrLyBhCDGrGL2NktmzZoieeeEJvvfWWXC6XX6/s+/HHH2vSpEnasmWLUlNTNWHCBN15550+P5+7XwNAYFTVrVT2SONtPcbFoLxavfv18ePHtWLFCi1dulRLly7VunXr1KlTJ917773q379/TWv26qqrrtJVV13l120CAGqXMb6PlyHE4GzUKMgkJCSoYcOGGjFihCZOnKj09HSutgsA8OBLQOGmkDhbNQoyV155pVasWKF33nlH2dnZys7OVv/+/dWhQwd/1wcAsIGTJ31flxYY+FONBvvOnz9fBw8e1MKFC9WrVy999tlnSk9PV4sWLTRixAh/1wgACGLGSNHRVleBcFWjFplS3bp1U1FRkU6dOqWTJ0/q008/1Zw5c/TWW2/5qz4AQJCLqNG/xIB/1OjHb9q0abrmmmvUqFEj9ezZU2+//bY6dOig999/XwcOHPB3jQAAi+zZU/WF7o4erTiPwwACqUYtMm+//bYuueQS3XXXXUpPT1d8fLy/6wIABIEWLapeXv6sWMa/INBqFGTWrFnj7zoAAEGm/CXBHA6CCoJPjXs2v/zyS916663q1auXdu/eLUn63//9X61YscJvxQEArFPHy7+6pd1MXNQOwaJGQeb999/XoEGDFB0drXXr1rnvNp2bm6snn3zSrwUCAABUpkZB5vHHH9fMmTP1z3/+U3Xr1nXP79Onj9auXeu34gAA1vDlqrxne6drwB9qFGQ2b96sfv36VZgfHx+vnJycs60JAOBnDofkclW+jFACu6pRkGnWrJm2bt1aYf6KFSvUtm3bsy4KAOA/pSElMrLqWwIQZmBHNQoyd955p+6//36tXr1aDodDe/bs0VtvvaUHHnhAY8aM8XeNAIAaKh9OYmKkU6cqX+7tZo/GnHkgL43xsEqNTr+eOHGiXC6XBgwYoOPHj6tfv35yOp166KGHdMcdd/i7RgBADVTWwuJ0Vv6cqq7Se/iwlJjofRmXE4NVatQi43A49Oc//1mHDx/Whg0btGrVKh04cEDx8fFKTU31d40AgGryVzfRzz+f/r5hQ06xRvCpVpApKCjQpEmT1L17d/Xp00effPKJOnfurI0bN6pjx4564YUXNH78+NqqFQAQYB07VpxHmEEwqVbX0uTJk/Xqq68qIyNDX3/9tW688UaNHj1aq1at0vPPP68bb7xRkZGRtVUrACCACCywg2oFmblz5+qNN97QNddcow0bNui8885TUVGR1q9fLwfD3QEgaBUVeb9Sb6mzCS0EHlipWkFm165duuiiiyRJXbt2ldPp1Pjx4wkxABBEvP1JjowsCRwuV8mA3rP9s014QbCoVpApLi5WVFTU6SfXqaPY2Fi/FwUA8J+yoaP0rCSCCEJFtYKMMUajRo2S8//O3Tt58qTuvvtuxcTEeKz3wQcf+K9CAECNEVgQ6qoVZEaOHOnx+NZbb/VrMQCAs0NPP8JNtYLM66+/Xlt1AADOUvl7KdEag3BQowviAQCCD1e/QDgiyAAAANsiyAAAANsiyABACGJ8DMJFje5+DQAIHpyphHBGiwwAALAtggwA2Ji31hi6lRBO6FoCABuiOwkoQYsMAACwLYIMANhIVd1GxtCthPBD1xIABDGXiyv2AlWhRQYAgpivIYaWGIQrggwA2Fh+PiEG4Y0gAwA2Vr++1RUA1iLIAECQycsrOb26qlOsGdgLlGCwLwAEmfh47/MJLkBFtMgAQBAhrADVQ5ABgCASUclf5f37A1sHYBcEGQAIEi6X9/l79khNmgS2FsAuCDIAECQqu2ZMcnJg6wDshMG+ABAEuIs1UDO2apF56qmn5HA4NG7cOKtLAYBadeyY1RUA9mCbFpk1a9bo1Vdf1XnnnWd1KQDgFy5XyeDe8q0xtMQAvrNFi8yxY8c0YsQI/fOf/1TDhg2tLgcAztqhQyVjYqq66B2AM7NFkBk7dqyGDBmijIyMM65bUFCgvLw8j6k2lF51kz9CAGqicWPv82mNAaon6LuW3nnnHa1du1Zr1qzxaf3MzEw99thjtVwVANQc/wAB/hPULTJZWVm6//779dZbb6levXo+PWfSpEnKzc11T1lZWbVcJQAAsIrDmOBtyJw/f76uu+46RZa5uEJxcbEcDociIiJUUFDgscybvLw8xcfHKzc3V3FxcX6rrex/VMH7CQIINlW1xhQXV35lXyDc+Hr8DuqupQEDBujHH3/0mDd69Gh16tRJjzzyyBlDDAAEE28hJje35CaRubmEGKAmgjrINGjQQF27dvWYFxMTo0aNGlWYDwDB5kxjYQoKpKgoWnWBs0H+BwCLREVZXQFgf0HdIuPN0qVLrS4BAM7oTK0xtMIA/kGLDAAAsC2CDAAESFZWSUsMrTGA/9iuawkAgp23bqXsbCkpKfC1AKGOIAMAtYwWGKD20LUEALWIEAPULoIMAPgR91ECAosg4wePP251BQCC0eHDVlcAhD6CjB88+qjVFQCwksNxeiqrYUNr6gHCCUEGAGqosJCuJMBqBBkAqCFuMQBYjyADALVgzx6rKwDCA9eRAYAacLmqXkaXExAYtMgAgI+2bpUKCkq+j4z0vo4xhBggkGiRAQAfcDdrIDjRIgMAAGyLIAMAlShtZaE1BghedC0BgBeMcwHsgSADICyVBhVvZxhVFWJofQGCC11LAMJO2aASEVHyuDSgnDxpTU0AaoYgAwA6HWiio70vX7uW1hggGBFkAISVmo59ueAC/9YBwD8YIwMgbNQkxNAKAwQ3WmQAoBJV3YYAQHAgyAAIC95aY4ypvMXl4EFOwQbsgK4lACHPW1ipLMBww0fAXggyAEKaL6GEcTCAfdG1BCDkHD5cEmAqCzEEFyB0EGQAhJxGjSpfRogBQgtdSwBsjzEtQPiiRQaALRlTdfdRZc8BEFoIMgBsKaIaf7327iXEAKGKriUAtsOZSABK0SIDwFaqCjEFBVVf5A5A6KFFBoDtEVyA8EWLDADbqOw2AwDCF0EGgG0RYgAQZAAETOnp0mWnI0eqfk5BgffTrAkxACTGyAAIkMoG6SYmeoaSoiKpbt3A1ATA/ggyACxX3Svz0hoDoBRdSwD8pmyXkcvlOR8AagNBxk/4Q41wVH68S1mRkSVhxt+/GwUF/t0eAHsjyAColvx8KSvLt4ASGVlxnjHSsWOeLTaV2bixZL3Si9wZI0VFVb9mAKGLMTIAfOKPlpWiopKvMTElX0tv/FgqK0tq0ULavVtq2fLsXw9A6CPI+JHDwSBEoCqVtdCUR4gB4Kug7lrKzMzUxRdfrAYNGqhp06YaOnSoNm/ebHVZQNjxR2sMIR9AbQjqILNs2TKNHTtWq1at0qJFi1RYWKiBAwcqPz/f6tKAsLZqlfTNN55jV8pOZf3yCyEGQO1xGGOfPzEHDhxQ06ZNtWzZMvXr18+n5+Tl5Sk+Pl65ubmKi4vzWy2V/Ydqn08T8E35n/UjR6SEBEtKARBGfD1+B3WLTHm5ubmSpMTERIsrAcKDt8BOiAEQTGwz2NflcmncuHHq06ePunbtWul6BQUFKihzoYm8vLxAlAfYmq9jYGhxBBBsbNMiM3bsWG3YsEHvvPNOletlZmYqPj7ePaWkpASoQsB+8vK4mCMAe7PFGJl7771XH374oZYvX67U1NQq1/XWIpOSksIYGcCL6oQYfrYBBJKvY2SCumvJGKM//vGPmjdvnpYuXXrGECNJTqdTTqczANUB9nb8uO/rEmIABKugDjJjx47Vf/7zH3344Ydq0KCBsrOzJUnx8fGKjo62uDrvyl+pFAhWpVfXLcvbzy8hBkAwC+quJUclieD111/XqFGjfNpGoE+/lvjDj5opLJTq1g3c61UVWMou4+cZgBVCpmsJwaXsAe7YMe//1ePMTp2SKusBDcSP/ZlaXfjVA2AXtjlrKVQ5HKenYLRxY0lg8VZjbGzw1h3sqhrGVZufaTD/rAFATQR1i4xdlB9XUPp9Vf/VejuYlM5zuaw/2FTn9X15v6ieQN6AtMxJfgBgO7TIBEB1/wuOiKi4fiBabs72NawOX3bh6+fkcEh799ZuLZIUFVX7rwEAtYUgU4vKh4LqBgWXK3CtM9V9jZ07K98O3RfV43KVtL7s2VNxWfPmJZ/l4cOen21xcclyh6Pk3ke+8LZPaEUDYHcEmSAWGVkyledweB70Sg9qta30gGuMFKgLJm/ZElpdH+XDRNkr6yYnVx4sGjXyfFynzunnJSbWLDgSYgCEAoJMEDCm5OJk1TmwtGhx+r/z0oNaYWHNXv9MB8HS8OLtTBdjpKNHK9/url0lX0+dqlldHTpI9eqdboUoLpb27z+93NtkJw0aVJx37FjNtlXZe/f2uRBiAIQKBvv6SU0vhOdylXz1x/X9Ssc6nG13VHWfHxtb+fsvbbkpPUvH1wOot23VsflP67Ztno8r+yxiYmr+81R2kLDdQh0A1AQtMhYp7aYpf7ApDTaVPccXEVXs1dKWkexs7/+pFxXV/ABY2kJTlfKtJ+VbV/xxZWQrDuCVtQhlZ5d85g6H1LZt9bZZ01aToqLa2S4ABCOCjB8ZUzIos7IxK5s3lxxkqjpYlx7Md+0qGRtSGg5Kn+NrF83u3d7nO50l20lO9r7c25ic6jpwwPd1k5I8P4uqQlgw+u0372eYlX5NTq76mjFnUnb/l52qUrcurTEAwofNDhvBr2FD7wdjl6tkvIevQaFFC++nxdat69t/1C1bVn/8SH6+b7WdSePG/tlOqdKD92+/lTz+6qszPycQ42aOHZPatav89atyplaTM6lOsCn/HAAIJTYfdRC8avuAUb5Vx+U6u9aM3Fypfv2zr6tU+fpOniwZtFsdxcWe7yk11fvn6suYkKouMJeXJ8XFlXyGpUGzqKjq0GmM94G6vqiNn42q9j/hBUAoo0XGxsqeDn02V4I1puRA7m9lWwyczor1eWuVWLbs9HOqCmbeWiNKx9tU9RzJ85Rnh0OKjy/5Wja4lJ4JVnZckjGnb9dQ09BYW6GishBHiAEQ6miRsTFvB6/c3JIDc1WsPLjV5s0JmzSpevxR+fDhS5dTZKRvA5CNkbKypFatvC8LhPJ1EmIAhANaZEJMXFzlB7BwGSORkyNt3eq/9+pLiJFKTjU3puS2AtUdv+IvVr0uAFiFFpkQFc4Hsvj4061S/jiduyrePudmzWrv9QAAnmiRQcjzJdQZ4/t1egAAwYMWGYSF0jBTejaSt1YabwOmubQ/AAQ3WmQQVsrebNEXZYPL1q3+rwcAcHZokQHOgFYYAAhetMgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbIsgAAADbskWQefnll9WmTRvVq1dPPXv21DfffGN1SQAAIAgEfZCZM2eOJkyYoClTpmjt2rVKS0vToEGDtH//fqtLAwAAFgv6IDNt2jTdeeedGj16tDp37qyZM2eqfv36+ve//211aQAAwGJBHWROnTql7777ThkZGe55ERERysjI0MqVK70+p6CgQHl5eR4TAAAITUEdZA4ePKji4mIlJSV5zE9KSlJ2drbX52RmZio+Pt49paSkBKJUAABggaAOMjUxadIk5ebmuqesrKxaeR1jTk8AAMAadawuoCqNGzdWZGSk9u3b5zF/3759atasmdfnOJ1OOZ3OQJQHAAAsFtQtMlFRUbrooou0ePFi9zyXy6XFixerV69eFlYGAACCQVC3yEjShAkTNHLkSHXv3l09evTQ9OnTlZ+fr9GjR1tdGgAAsFjQB5lhw4bpwIEDmjx5srKzs3X++edr4cKFFQYAAwCA8OMwJrSHq+bl5Sk+Pl65ubmKi4uzuhwAAOADX4/fQT1GBgAAoCoEGQAAYFsEGQAAYFsEGQAAYFsEGQAAYFsEGQAAYFsEGQAAYFsEGQAAYFsEGQAAYFtBf4uCs1V64eK8vDyLKwEAAL4qPW6f6QYEIR9kjh49KklKSUmxuBIAAFBdR48eVXx8fKXLQ/5eSy6XS3v27FGDBg3kcDj8tt28vDylpKQoKyuLezjZCPvNvth39sR+s6dg2G/GGB09elTNmzdXRETlI2FCvkUmIiJCLVu2rLXtx8XF8ctpQ+w3+2Lf2RP7zZ6s3m9VtcSUYrAvAACwLYIMAACwLYJMDTmdTk2ZMkVOp9PqUlAN7Df7Yt/ZE/vNnuy030J+sC8AAAhdtMgAAADbIsgAAADbIsgAAADbIsgAAADbIsjU0Msvv6w2bdqoXr166tmzp7755hurSwpbU6dOlcPh8Jg6derkXn7y5EmNHTtWjRo1UmxsrK6//nrt27fPYxs7d+7UkCFDVL9+fTVt2lQPPfSQioqKAv1WQt7y5ct19dVXq3nz5nI4HJo/f77HcmOMJk+erOTkZEVHRysjI0NbtmzxWOfw4cMaMWKE4uLilJCQoNtvv13Hjh3zWOeHH35Qenq66tWrp5SUFD3zzDO1/dZC2pn226hRoyr8Dl5xxRUe67DfAiszM1MXX3yxGjRooKZNm2ro0KHavHmzxzr++tu4dOlSXXjhhXI6nTrnnHM0a9as2n57HggyNTBnzhxNmDBBU6ZM0dq1a5WWlqZBgwZp//79VpcWtrp06aK9e/e6pxUrVriXjR8/Xv/v//0/zZ07V8uWLdOePXv0u9/9zr28uLhYQ4YM0alTp/T1119r9uzZmjVrliZPnmzFWwlp+fn5SktL08svv+x1+TPPPKMXX3xRM2fO1OrVqxUTE6NBgwbp5MmT7nVGjBihjRs3atGiRfr444+1fPly3XXXXe7leXl5GjhwoFq3bq3vvvtOzz77rKZOnarXXnut1t9fqDrTfpOkK664wuN38O233/ZYzn4LrGXLlmns2LFatWqVFi1apMLCQg0cOFD5+fnudfzxt3Hbtm0aMmSILr30Un3//fcaN26c7rjjDn366aeBe7MG1dajRw8zduxY9+Pi4mLTvHlzk5mZaWFV4WvKlCkmLS3N67KcnBxTt25dM3fuXPe8TZs2GUlm5cqVxhhjPvnkExMREWGys7Pd68yYMcPExcWZgoKCWq09nEky8+bNcz92uVymWbNm5tlnn3XPy8nJMU6n07z99tvGGGN++uknI8msWbPGvc6CBQuMw+Ewu3fvNsYY88orr5iGDRt67LtHHnnEdOzYsZbfUXgov9+MMWbkyJHm2muvrfQ57Dfr7d+/30gyy5YtM8b472/jww8/bLp06eLxWsOGDTODBg2q7bfkRotMNZ06dUrfffedMjIy3PMiIiKUkZGhlStXWlhZeNuyZYuaN2+utm3basSIEdq5c6ck6bvvvlNhYaHH/urUqZNatWrl3l8rV65Ut27dlJSU5F5n0KBBysvL08aNGwP7RsLYtm3blJ2d7bGv4uPj1bNnT499lZCQoO7du7vXycjIUEREhFavXu1ep1+/foqKinKvM2jQIG3evFlHjhwJ0LsJP0uXLlXTpk3VsWNHjRkzRocOHXIvY79ZLzc3V5KUmJgoyX9/G1euXOmxjdJ1Ank8JMhU08GDB1VcXOyxYyUpKSlJ2dnZFlUV3nr27KlZs2Zp4cKFmjFjhrZt26b09HQdPXpU2dnZioqKUkJCgsdzyu6v7Oxsr/uzdBkCo/Szrup3Kzs7W02bNvVYXqdOHSUmJrI/LXTFFVfojTfe0OLFi/X0009r2bJlGjx4sIqLiyWx36zmcrk0btw49enTR127dpUkv/1trGydvLw8nThxojbeTgUhf/drhL7Bgwe7vz/vvPPUs2dPtW7dWu+++66io6MtrAwIDzfffLP7+27duum8885Tu3bttHTpUg0YMMDCyiBJY8eO1YYNGzzGDoYSWmSqqXHjxoqMjKwwsnvfvn1q1qyZRVWhrISEBHXo0EFbt25Vs2bNdOrUKeXk5HisU3Z/NWvWzOv+LF2GwCj9rKv63WrWrFmFQfVFRUU6fPgw+zOItG3bVo0bN9bWrVslsd+sdO+99+rjjz/WkiVL1LJlS/d8f/1trGyduLi4gP0jSZCppqioKF100UVavHixe57L5dLixYvVq1cvCytDqWPHjunXX39VcnKyLrroItWtW9djf23evFk7d+50769evXrpxx9/9PhDu2jRIsXFxalz584Brz9cpaamqlmzZh77Ki8vT6tXr/bYVzk5Ofruu+/c63zxxRdyuVzq2bOne53ly5ersLDQvc6iRYvUsWNHNWzYMEDvJrzt2rVLhw4dUnJysiT2mxWMMbr33ns1b948ffHFF0pNTfVY7q+/jb169fLYRuk6AT0eBmxYcQh55513jNPpNLNmzTI//fSTueuuu0xCQoLHyG4EzgMPPGCWLl1qtm3bZr766iuTkZFhGjdubPbv32+MMebuu+82rVq1Ml988YX59ttvTa9evUyvXr3czy8qKjJdu3Y1AwcONN9//71ZuHChadKkiZk0aZJVbylkHT161Kxbt86sW7fOSDLTpk0z69atMzt27DDGGPPUU0+ZhIQE8+GHH5offvjBXHvttSY1NdWcOHHCvY0rrrjCXHDBBWb16tVmxYoVpn379mb48OHu5Tk5OSYpKcncdtttZsOGDeadd94x9evXN6+++mrA32+oqGq/HT161Dz44INm5cqVZtu2bebzzz83F154oWnfvr05efKkexvst8AaM2aMiY+PN0uXLjV79+51T8ePH3ev44+/jb/99pupX7++eeihh8ymTZvMyy+/bCIjI83ChQsD9l4JMjX00ksvmVatWpmoqCjTo0cPs2rVKqtLClvDhg0zycnJJioqyrRo0cIMGzbMbN261b38xIkT5p577jENGzY09evXN9ddd53Zu3evxza2b99uBg8ebKKjo03jxo3NAw88YAoLCwP9VkLekiVLjKQK08iRI40xJadgP/rooyYpKck4nU4zYMAAs3nzZo9tHDp0yAwfPtzExsaauLg4M3r0aHP06FGPddavX2/69u1rnE6nadGihXnqqacC9RZDUlX77fjx42bgwIGmSZMmpm7duqZ169bmzjvvrPCPHfstsLztL0nm9ddfd6/jr7+NS5YsMeeff76Jiooybdu29XiNQHAYY0zg2n8AAAD8hzEyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAADAtggyAILC9u3b5XA49P3339faa4waNUpDhw6tte0DCDyCDAC/GDVqlBwOR4Xpiiuu8On5KSkp2rt3r7p27VrLlQIIJXWsLgBA6Ljiiiv0+uuve8xzOp0+PTcyMpK7HAOoNlpkAPiN0+lUs2bNPKbSOxc7HA7NmDFDgwcPVnR0tNq2bav33nvP/dzyXUtHjhzRiBEj1KRJE0VHR6t9+/YeIenHH3/UZZddpujoaDVq1Eh33XWXjh075l5eXFysCRMmKCEhQY0aNdLDDz+s8ndkcblcyszMVGpqqqKjo5WWluZR05lqAGA9ggyAgHn00Ud1/fXXa/369RoxYoRuvvlmbdq0qdJ1f/rpJy1YsECbNm3SjBkz1LhxY0lSfn6+Bg0apIYNG2rNmjWaO3euPv/8c917773u5z///POaNWuW/v3vf2vFihU6fPiw5s2b5/EamZmZeuONNzRz5kxt3LhR48eP16233qply5adsQYAQSKgt6gEELJGjhxpIiMjTUxMjMf0xBNPGGNK7sZ79913ezynZ8+eZsyYMcYYY7Zt22YkmXXr1hljjLn66qvN6NGjvb7Wa6+9Zho2bGiOHTvmnvff//7XREREuO+6nJycbJ555hn38sLCQtOyZUtz7bXXGmOMOXnypKlfv775+uuvPbZ9++23m+HDh5+xBgDBgTEyAPzm0ksv1YwZMzzmJSYmur/v1auXx7JevXpVepbSmDFjdP3112vt2rUaOHCghg4dqt69e0uSNm3apLS0NMXExLjX79Onj1wulzZv3qx69epp79696tmzp3t5nTp11L17d3f30tatW3X8+HFdfvnlHq976tQpXXDBBWesAUBwIMgA8JuYmBidc845ftnW4MGDtWPHDn3yySdatGiRBgwYoLFjx+q5557zy/ZLx9P897//VYsWLTyWlQ5Qru0aAJw9xsgACJhVq1ZVeHzuuedWun6TJk00cuRIvfnmm5o+fbpee+01SdK5556r9evXKz8/373uV199pYiICHXs2FHx8fFKTk7W6tWr3cuLior03XffuR937txZTqdTO3fu1DnnnOMxpaSknLEGAMGBFhkAflNQUKDs7GyPeXXq1HEPkJ07d666d++uvn376q233tI333yj//mf//G6rcmTJ+uiiy5Sly5dVFBQoI8//tgdekaMGKEpU6Zo5MiRmjp1qg4cOKA//vGPuu2225SUlCRJuv/++/XUU0+pffv26tSpk6ZNm6acnBz39hs0aKAHH3xQ48ePl8vlUt++fZWbm6uvvvpKcXFxGjlyZJU1AAgOBBkAfrNw4UIlJyd7zOvYsaN+/vlnSdJjjz2md955R/fcc4+Sk5P19ttvq3Pnzl63FRUVpUmTJmn79u2Kjo5Wenq63nnnHUlS/fr19emnn+r+++/XxRdfrPr16+v666/XtGnT3M9/4IEHtHfvXo0cOVIRERH6wx/+oOuuu065ubnudf72t7+pSZMmyszM1G+//aaEhARdeOGF+tOf/nTGGgAEB4cx5S6sAAC1wOFwaN68edwiAIBfMUYGAADYFkEGAADYFmNkAAQEvdgAagMtMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLYIMgAAwLb+P5CTBa6QZhbyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255., device)\n",
    "        state = next_state\n",
    "        next_state, reward, done, truncated, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "        # history = torch.from_numpy(history).to(device)\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame, device)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"save_model/breakout_double_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aff4f8f0310>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "# env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "state = env.reset()\n",
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "# # Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
    "        loop controls style=\"height: 400px;\">\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "        </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "# from agent_double import Agent\n",
    "action_size = 3\n",
    "# display = Display(visible=0, size=(300, 200))\n",
    "# display.start()\n",
    "# Load agent\n",
    "agent = Agent(action_size)\n",
    "agent.policy_net.load_state_dict(torch.load(\"save_model/breakout_dqn_7.pth.tar\", map_location=device))\n",
    "# agent.load_policy_net(\"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials/save_model/breakout_dqn_latest_2.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state, _ = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "frame = 0\n",
    "while (not done):\n",
    "    # show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "    step += 1\n",
    "    frame += 1\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255., device)\n",
    "    state = next_state\n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1)\n",
    "    r = reward\n",
    "    # Store the transition in memory\n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "# show_video()\n",
    "# display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
