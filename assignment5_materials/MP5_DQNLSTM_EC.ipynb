{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DQN LSTM Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "COLAB = False\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "if COLAB:\n",
    "    work_dir = \"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\"\n",
    "else:\n",
    "    work_dir = \"/home/changl25/private/UIUC-CS444-DL-for-cv/assignment5_materials\"\n",
    "\n",
    "os.chdir(work_dir)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next state) tuples rather than just one (state, action, reward, next state) tuple so it can work with LSTMs. Use the previous tuples to generate the current hidden and context vector for LSTM. \n",
    "Esentially, when you get a sample from replay buffer during training, start with the first tuple and generate hidden and context vector from this and pass it onto the next tuple. Do so consequitively till you reach the last tuple, where you will make Q value predictions.\n",
    "Training loop remains nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_lstm import Agent_LSTM\n",
    "agent = Agent_LSTM(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 1.0   memory length: 168   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 1   score: 4.0   memory length: 446   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 2   score: 0.0   memory length: 569   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 1.0   memory length: 738   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 4   score: 4.0   memory length: 1034   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 5   score: 2.0   memory length: 1218   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 6   score: 2.0   memory length: 1433   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 7   score: 1.0   memory length: 1584   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 8   score: 1.0   memory length: 1736   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.7777777777777777\n",
      "episode: 9   score: 2.0   memory length: 1933   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 10   score: 2.0   memory length: 2130   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8181818181818181\n",
      "episode: 11   score: 2.0   memory length: 2327   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 12   score: 0.0   memory length: 2449   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6923076923076923\n",
      "episode: 13   score: 0.0   memory length: 2571   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 14   score: 1.0   memory length: 2739   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5333333333333334\n",
      "episode: 15   score: 0.0   memory length: 2862   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4375\n",
      "episode: 16   score: 3.0   memory length: 3127   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.5294117647058822\n",
      "episode: 17   score: 0.0   memory length: 3250   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 18   score: 3.0   memory length: 3516   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5263157894736843\n",
      "episode: 19   score: 1.0   memory length: 3686   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 20   score: 3.0   memory length: 3931   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 21   score: 2.0   memory length: 4128   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
      "episode: 22   score: 0.0   memory length: 4251   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5217391304347827\n",
      "episode: 23   score: 0.0   memory length: 4373   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4583333333333333\n",
      "episode: 24   score: 0.0   memory length: 4496   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 25   score: 1.0   memory length: 4665   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 26   score: 0.0   memory length: 4788   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 27   score: 2.0   memory length: 4985   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
      "episode: 28   score: 2.0   memory length: 5182   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3793103448275863\n",
      "episode: 29   score: 2.0   memory length: 5380   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 30   score: 0.0   memory length: 5503   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3548387096774193\n",
      "episode: 31   score: 0.0   memory length: 5626   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3125\n",
      "episode: 32   score: 2.0   memory length: 5842   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 33   score: 2.0   memory length: 6025   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
      "episode: 34   score: 0.0   memory length: 6148   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3142857142857143\n",
      "episode: 35   score: 2.0   memory length: 6346   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 36   score: 2.0   memory length: 6562   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.3513513513513513\n",
      "episode: 37   score: 3.0   memory length: 6829   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.394736842105263\n",
      "episode: 38   score: 1.0   memory length: 6998   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 39   score: 1.0   memory length: 7170   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 40   score: 0.0   memory length: 7292   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3414634146341464\n",
      "episode: 41   score: 2.0   memory length: 7510   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
      "episode: 42   score: 0.0   memory length: 7633   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3255813953488371\n",
      "episode: 43   score: 3.0   memory length: 7896   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 44   score: 0.0   memory length: 8019   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 45   score: 4.0   memory length: 8314   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.391304347826087\n",
      "episode: 46   score: 2.0   memory length: 8512   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4042553191489362\n",
      "episode: 47   score: 2.0   memory length: 8731   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 48   score: 2.0   memory length: 8949   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 49   score: 1.0   memory length: 9100   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 50   score: 1.0   memory length: 9272   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.411764705882353\n",
      "episode: 51   score: 0.0   memory length: 9395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 52   score: 0.0   memory length: 9517   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3584905660377358\n",
      "episode: 53   score: 3.0   memory length: 9762   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.3888888888888888\n",
      "episode: 54   score: 0.0   memory length: 9885   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 55   score: 1.0   memory length: 10036   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3571428571428572\n",
      "episode: 56   score: 4.0   memory length: 10312   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.4035087719298245\n",
      "episode: 57   score: 1.0   memory length: 10481   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.396551724137931\n",
      "episode: 58   score: 2.0   memory length: 10679   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4067796610169492\n",
      "episode: 59   score: 0.0   memory length: 10801   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3833333333333333\n",
      "episode: 60   score: 3.0   memory length: 11026   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.4098360655737705\n",
      "episode: 61   score: 2.0   memory length: 11224   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
      "episode: 62   score: 1.0   memory length: 11394   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4126984126984128\n",
      "episode: 63   score: 0.0   memory length: 11516   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.390625\n",
      "episode: 64   score: 2.0   memory length: 11713   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 65   score: 2.0   memory length: 11911   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4090909090909092\n",
      "episode: 66   score: 2.0   memory length: 12109   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4179104477611941\n",
      "episode: 67   score: 4.0   memory length: 12385   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.4558823529411764\n",
      "episode: 68   score: 0.0   memory length: 12508   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.434782608695652\n",
      "episode: 69   score: 0.0   memory length: 12630   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4142857142857144\n",
      "episode: 70   score: 2.0   memory length: 12827   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4225352112676057\n",
      "episode: 71   score: 1.0   memory length: 12997   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 72   score: 2.0   memory length: 13198   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.4246575342465753\n",
      "episode: 73   score: 1.0   memory length: 13367   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4189189189189189\n",
      "episode: 74   score: 0.0   memory length: 13490   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 75   score: 3.0   memory length: 13737   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
      "episode: 76   score: 0.0   memory length: 13860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4025974025974026\n",
      "episode: 77   score: 1.0   memory length: 14011   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3974358974358974\n",
      "episode: 78   score: 3.0   memory length: 14239   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.4177215189873418\n",
      "episode: 79   score: 2.0   memory length: 14438   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.425\n",
      "episode: 80   score: 1.0   memory length: 14589   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4197530864197532\n",
      "episode: 81   score: 3.0   memory length: 14854   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.4390243902439024\n",
      "episode: 82   score: 0.0   memory length: 14977   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4216867469879517\n",
      "episode: 83   score: 1.0   memory length: 15146   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 84   score: 2.0   memory length: 15362   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.423529411764706\n",
      "episode: 85   score: 4.0   memory length: 15677   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.4534883720930232\n",
      "episode: 86   score: 0.0   memory length: 15799   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4367816091954022\n",
      "episode: 87   score: 3.0   memory length: 16043   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 88   score: 0.0   memory length: 16166   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4382022471910112\n",
      "episode: 89   score: 0.0   memory length: 16288   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4222222222222223\n",
      "episode: 90   score: 3.0   memory length: 16533   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.4395604395604396\n",
      "episode: 91   score: 1.0   memory length: 16703   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.434782608695652\n",
      "episode: 92   score: 1.0   memory length: 16853   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4301075268817205\n",
      "episode: 93   score: 0.0   memory length: 16976   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4148936170212767\n",
      "episode: 94   score: 2.0   memory length: 17194   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
      "episode: 95   score: 1.0   memory length: 17344   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 96   score: 4.0   memory length: 17641   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.443298969072165\n",
      "episode: 97   score: 1.0   memory length: 17810   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4387755102040816\n",
      "episode: 98   score: 0.0   memory length: 17932   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4242424242424243\n",
      "episode: 99   score: 0.0   memory length: 18055   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 100   score: 1.0   memory length: 18226   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 101   score: 1.0   memory length: 18377   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 102   score: 1.0   memory length: 18527   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 103   score: 1.0   memory length: 18695   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 104   score: 4.0   memory length: 18991   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 105   score: 1.0   memory length: 19161   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 106   score: 0.0   memory length: 19284   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 107   score: 0.0   memory length: 19406   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 108   score: 1.0   memory length: 19575   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 109   score: 2.0   memory length: 19791   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 110   score: 0.0   memory length: 19914   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 111   score: 3.0   memory length: 20125   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 112   score: 4.0   memory length: 20442   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 113   score: 1.0   memory length: 20593   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 114   score: 2.0   memory length: 20790   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 115   score: 1.0   memory length: 20941   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 116   score: 2.0   memory length: 21139   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 117   score: 0.0   memory length: 21262   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 118   score: 1.0   memory length: 21430   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 119   score: 2.0   memory length: 21628   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 120   score: 9.0   memory length: 21992   epsilon: 1.0    steps: 364    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 121   score: 1.0   memory length: 22160   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 122   score: 1.0   memory length: 22310   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 123   score: 2.0   memory length: 22508   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 124   score: 1.0   memory length: 22677   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 125   score: 0.0   memory length: 22800   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 126   score: 0.0   memory length: 22923   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 127   score: 1.0   memory length: 23092   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 128   score: 0.0   memory length: 23215   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 129   score: 0.0   memory length: 23337   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 130   score: 0.0   memory length: 23460   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 131   score: 0.0   memory length: 23582   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 132   score: 2.0   memory length: 23780   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 133   score: 2.0   memory length: 23978   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 134   score: 5.0   memory length: 24279   epsilon: 1.0    steps: 301    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 135   score: 2.0   memory length: 24496   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 136   score: 2.0   memory length: 24694   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 137   score: 2.0   memory length: 24912   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 138   score: 3.0   memory length: 25159   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 139   score: 0.0   memory length: 25282   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 140   score: 6.0   memory length: 25663   epsilon: 1.0    steps: 381    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 141   score: 2.0   memory length: 25864   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 142   score: 6.0   memory length: 26242   epsilon: 1.0    steps: 378    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 143   score: 3.0   memory length: 26469   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 144   score: 1.0   memory length: 26638   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 145   score: 2.0   memory length: 26858   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 146   score: 0.0   memory length: 26980   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 147   score: 0.0   memory length: 27102   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 148   score: 0.0   memory length: 27225   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 149   score: 0.0   memory length: 27347   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 150   score: 2.0   memory length: 27544   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 151   score: 2.0   memory length: 27742   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 152   score: 0.0   memory length: 27865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 153   score: 6.0   memory length: 28231   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 154   score: 3.0   memory length: 28460   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 155   score: 3.0   memory length: 28690   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 156   score: 2.0   memory length: 28909   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 157   score: 2.0   memory length: 29106   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 158   score: 2.0   memory length: 29303   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 159   score: 0.0   memory length: 29426   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 160   score: 4.0   memory length: 29682   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 161   score: 1.0   memory length: 29851   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 162   score: 2.0   memory length: 30072   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 163   score: 3.0   memory length: 30298   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 164   score: 2.0   memory length: 30517   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 165   score: 2.0   memory length: 30715   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 166   score: 0.0   memory length: 30838   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 167   score: 2.0   memory length: 31036   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 168   score: 4.0   memory length: 31329   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 169   score: 0.0   memory length: 31452   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 170   score: 3.0   memory length: 31698   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 171   score: 0.0   memory length: 31821   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 172   score: 0.0   memory length: 31943   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 173   score: 2.0   memory length: 32141   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 174   score: 4.0   memory length: 32422   epsilon: 1.0    steps: 281    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 175   score: 0.0   memory length: 32545   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 176   score: 2.0   memory length: 32743   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 177   score: 3.0   memory length: 33007   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 178   score: 1.0   memory length: 33179   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 179   score: 2.0   memory length: 33396   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 180   score: 0.0   memory length: 33518   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 181   score: 0.0   memory length: 33640   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 182   score: 0.0   memory length: 33763   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 183   score: 0.0   memory length: 33886   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 184   score: 2.0   memory length: 34084   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 185   score: 2.0   memory length: 34303   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 186   score: 0.0   memory length: 34425   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 187   score: 0.0   memory length: 34547   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 188   score: 1.0   memory length: 34719   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 189   score: 3.0   memory length: 34944   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 190   score: 1.0   memory length: 35095   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 191   score: 1.0   memory length: 35246   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 192   score: 4.0   memory length: 35542   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 193   score: 4.0   memory length: 35818   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 194   score: 3.0   memory length: 36064   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 195   score: 1.0   memory length: 36215   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 196   score: 4.0   memory length: 36474   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 197   score: 3.0   memory length: 36721   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 198   score: 1.0   memory length: 36890   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 199   score: 0.0   memory length: 37012   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 200   score: 0.0   memory length: 37135   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 201   score: 3.0   memory length: 37361   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 202   score: 2.0   memory length: 37543   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 203   score: 2.0   memory length: 37741   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 204   score: 5.0   memory length: 38046   epsilon: 1.0    steps: 305    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 205   score: 3.0   memory length: 38292   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 206   score: 2.0   memory length: 38489   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 207   score: 4.0   memory length: 38785   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 208   score: 1.0   memory length: 38935   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 209   score: 2.0   memory length: 39133   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 210   score: 1.0   memory length: 39284   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 211   score: 0.0   memory length: 39406   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 212   score: 2.0   memory length: 39604   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 213   score: 2.0   memory length: 39802   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 214   score: 0.0   memory length: 39924   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 215   score: 1.0   memory length: 40093   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 216   score: 2.0   memory length: 40311   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 217   score: 1.0   memory length: 40483   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 218   score: 1.0   memory length: 40633   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 219   score: 1.0   memory length: 40801   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 220   score: 1.0   memory length: 40973   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 221   score: 2.0   memory length: 41170   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 222   score: 1.0   memory length: 41321   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 223   score: 3.0   memory length: 41570   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 224   score: 3.0   memory length: 41813   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 225   score: 0.0   memory length: 41936   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 226   score: 3.0   memory length: 42162   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 227   score: 4.0   memory length: 42454   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 228   score: 0.0   memory length: 42576   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 229   score: 0.0   memory length: 42699   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 230   score: 2.0   memory length: 42918   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 231   score: 0.0   memory length: 43041   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 232   score: 3.0   memory length: 43268   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 233   score: 1.0   memory length: 43437   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 234   score: 0.0   memory length: 43560   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 235   score: 1.0   memory length: 43729   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 236   score: 4.0   memory length: 44003   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 237   score: 0.0   memory length: 44126   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 238   score: 4.0   memory length: 44437   epsilon: 1.0    steps: 311    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 239   score: 1.0   memory length: 44605   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 240   score: 2.0   memory length: 44803   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 241   score: 0.0   memory length: 44926   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 242   score: 1.0   memory length: 45078   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 243   score: 1.0   memory length: 45246   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 244   score: 1.0   memory length: 45417   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 245   score: 0.0   memory length: 45540   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 246   score: 2.0   memory length: 45737   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 247   score: 1.0   memory length: 45905   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 248   score: 3.0   memory length: 46136   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 249   score: 1.0   memory length: 46286   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 250   score: 2.0   memory length: 46484   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 251   score: 7.0   memory length: 46893   epsilon: 1.0    steps: 409    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 252   score: 4.0   memory length: 47184   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 253   score: 2.0   memory length: 47402   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 254   score: 0.0   memory length: 47525   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 255   score: 3.0   memory length: 47789   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 256   score: 3.0   memory length: 48036   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 257   score: 1.0   memory length: 48207   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 258   score: 1.0   memory length: 48379   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 259   score: 1.0   memory length: 48549   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 260   score: 3.0   memory length: 48816   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 261   score: 3.0   memory length: 49063   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 262   score: 2.0   memory length: 49243   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 263   score: 0.0   memory length: 49366   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 264   score: 1.0   memory length: 49535   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 265   score: 3.0   memory length: 49761   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 266   score: 2.0   memory length: 49959   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 267   score: 0.0   memory length: 50082   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 268   score: 1.0   memory length: 50251   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 269   score: 0.0   memory length: 50374   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 270   score: 1.0   memory length: 50545   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 271   score: 1.0   memory length: 50716   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 272   score: 0.0   memory length: 50839   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 273   score: 2.0   memory length: 51057   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 274   score: 0.0   memory length: 51180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 275   score: 1.0   memory length: 51350   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 276   score: 1.0   memory length: 51519   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 277   score: 1.0   memory length: 51688   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 278   score: 0.0   memory length: 51811   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 279   score: 2.0   memory length: 52009   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 280   score: 0.0   memory length: 52132   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 281   score: 2.0   memory length: 52330   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 282   score: 2.0   memory length: 52548   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 283   score: 3.0   memory length: 52794   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 284   score: 1.0   memory length: 52945   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 285   score: 0.0   memory length: 53067   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 286   score: 4.0   memory length: 53381   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 287   score: 3.0   memory length: 53607   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 288   score: 2.0   memory length: 53827   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 289   score: 2.0   memory length: 54047   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 290   score: 1.0   memory length: 54198   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 291   score: 2.0   memory length: 54396   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 292   score: 3.0   memory length: 54666   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 293   score: 3.0   memory length: 54932   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 294   score: 2.0   memory length: 55149   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 295   score: 0.0   memory length: 55272   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 296   score: 0.0   memory length: 55395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 297   score: 1.0   memory length: 55564   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 298   score: 0.0   memory length: 55687   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 299   score: 1.0   memory length: 55838   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 300   score: 2.0   memory length: 56057   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 301   score: 1.0   memory length: 56207   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 302   score: 3.0   memory length: 56454   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 303   score: 3.0   memory length: 56697   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 304   score: 2.0   memory length: 56913   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 305   score: 4.0   memory length: 57188   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 306   score: 0.0   memory length: 57310   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 307   score: 2.0   memory length: 57508   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 308   score: 1.0   memory length: 57659   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 309   score: 2.0   memory length: 57876   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 310   score: 0.0   memory length: 57998   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 311   score: 1.0   memory length: 58170   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 312   score: 2.0   memory length: 58385   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 313   score: 0.0   memory length: 58508   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 314   score: 0.0   memory length: 58631   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 315   score: 2.0   memory length: 58829   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 316   score: 0.0   memory length: 58952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 317   score: 2.0   memory length: 59150   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 318   score: 1.0   memory length: 59302   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 319   score: 3.0   memory length: 59567   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 320   score: 1.0   memory length: 59718   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 321   score: 4.0   memory length: 59984   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 322   score: 0.0   memory length: 60107   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 323   score: 1.0   memory length: 60258   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 324   score: 3.0   memory length: 60501   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 325   score: 1.0   memory length: 60673   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 326   score: 2.0   memory length: 60871   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 327   score: 0.0   memory length: 60994   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 328   score: 2.0   memory length: 61174   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 329   score: 3.0   memory length: 61401   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 330   score: 2.0   memory length: 61620   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 331   score: 0.0   memory length: 61743   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 332   score: 1.0   memory length: 61895   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 333   score: 0.0   memory length: 62018   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 334   score: 5.0   memory length: 62362   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 335   score: 4.0   memory length: 62653   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 336   score: 0.0   memory length: 62775   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 337   score: 0.0   memory length: 62897   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 338   score: 3.0   memory length: 63146   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 339   score: 0.0   memory length: 63268   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 340   score: 2.0   memory length: 63486   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 341   score: 2.0   memory length: 63686   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 342   score: 1.0   memory length: 63854   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 343   score: 2.0   memory length: 64052   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 344   score: 2.0   memory length: 64252   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 345   score: 2.0   memory length: 64453   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 346   score: 1.0   memory length: 64621   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 347   score: 2.0   memory length: 64820   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 348   score: 0.0   memory length: 64943   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 349   score: 3.0   memory length: 65193   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 350   score: 1.0   memory length: 65364   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 351   score: 2.0   memory length: 65562   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 352   score: 1.0   memory length: 65731   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 353   score: 3.0   memory length: 65997   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 354   score: 3.0   memory length: 66243   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 355   score: 1.0   memory length: 66394   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 356   score: 1.0   memory length: 66564   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 357   score: 1.0   memory length: 66734   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 358   score: 3.0   memory length: 66959   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 359   score: 4.0   memory length: 67247   epsilon: 1.0    steps: 288    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 360   score: 2.0   memory length: 67426   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 361   score: 0.0   memory length: 67549   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 362   score: 2.0   memory length: 67746   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 363   score: 0.0   memory length: 67868   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 364   score: 1.0   memory length: 68038   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 365   score: 4.0   memory length: 68356   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 366   score: 4.0   memory length: 68652   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 367   score: 1.0   memory length: 68820   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 368   score: 2.0   memory length: 69017   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 369   score: 3.0   memory length: 69246   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 370   score: 2.0   memory length: 69443   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 371   score: 1.0   memory length: 69613   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 372   score: 2.0   memory length: 69811   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 373   score: 1.0   memory length: 69982   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 374   score: 2.0   memory length: 70181   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 375   score: 0.0   memory length: 70304   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 376   score: 6.0   memory length: 70660   epsilon: 1.0    steps: 356    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 377   score: 1.0   memory length: 70832   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 378   score: 1.0   memory length: 71000   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 379   score: 3.0   memory length: 71233   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 380   score: 1.0   memory length: 71402   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 381   score: 2.0   memory length: 71600   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 382   score: 2.0   memory length: 71818   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 383   score: 2.0   memory length: 72015   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 384   score: 2.0   memory length: 72213   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 385   score: 1.0   memory length: 72382   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 386   score: 1.0   memory length: 72533   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 387   score: 4.0   memory length: 72829   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 388   score: 0.0   memory length: 72952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 389   score: 0.0   memory length: 73075   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 390   score: 1.0   memory length: 73225   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 391   score: 2.0   memory length: 73445   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 392   score: 2.0   memory length: 73642   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 393   score: 1.0   memory length: 73810   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 394   score: 1.0   memory length: 73961   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 395   score: 3.0   memory length: 74209   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 396   score: 3.0   memory length: 74454   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 397   score: 3.0   memory length: 74723   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 398   score: 0.0   memory length: 74846   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 399   score: 0.0   memory length: 74969   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 400   score: 2.0   memory length: 75167   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 401   score: 2.0   memory length: 75365   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 402   score: 2.0   memory length: 75583   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 403   score: 1.0   memory length: 75734   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 404   score: 0.0   memory length: 75857   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 405   score: 1.0   memory length: 76025   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 406   score: 0.0   memory length: 76147   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 407   score: 0.0   memory length: 76270   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 408   score: 0.0   memory length: 76393   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 409   score: 1.0   memory length: 76561   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 410   score: 2.0   memory length: 76758   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 411   score: 3.0   memory length: 76985   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 412   score: 1.0   memory length: 77153   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 413   score: 1.0   memory length: 77304   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 414   score: 1.0   memory length: 77455   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 415   score: 0.0   memory length: 77577   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 416   score: 3.0   memory length: 77824   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 417   score: 0.0   memory length: 77947   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 418   score: 1.0   memory length: 78119   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 419   score: 0.0   memory length: 78241   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 420   score: 1.0   memory length: 78392   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 421   score: 1.0   memory length: 78564   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 422   score: 2.0   memory length: 78782   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 423   score: 1.0   memory length: 78951   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 424   score: 2.0   memory length: 79152   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 425   score: 2.0   memory length: 79353   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 426   score: 3.0   memory length: 79597   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 427   score: 1.0   memory length: 79766   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 428   score: 1.0   memory length: 79917   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 429   score: 3.0   memory length: 80163   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 430   score: 4.0   memory length: 80459   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 431   score: 2.0   memory length: 80677   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 432   score: 0.0   memory length: 80799   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 433   score: 1.0   memory length: 80950   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 434   score: 1.0   memory length: 81120   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 435   score: 0.0   memory length: 81243   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 436   score: 1.0   memory length: 81411   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 437   score: 1.0   memory length: 81579   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 438   score: 1.0   memory length: 81748   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 439   score: 2.0   memory length: 81946   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 440   score: 2.0   memory length: 82166   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 441   score: 0.0   memory length: 82289   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 442   score: 3.0   memory length: 82516   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 443   score: 7.0   memory length: 82836   epsilon: 1.0    steps: 320    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 444   score: 3.0   memory length: 83084   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 445   score: 2.0   memory length: 83282   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 446   score: 1.0   memory length: 83451   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 447   score: 0.0   memory length: 83573   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 448   score: 2.0   memory length: 83791   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 449   score: 2.0   memory length: 84009   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 450   score: 0.0   memory length: 84131   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 451   score: 2.0   memory length: 84350   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 452   score: 2.0   memory length: 84568   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 453   score: 0.0   memory length: 84691   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 454   score: 0.0   memory length: 84814   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 455   score: 0.0   memory length: 84937   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 456   score: 2.0   memory length: 85140   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 457   score: 0.0   memory length: 85263   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 458   score: 1.0   memory length: 85431   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 459   score: 1.0   memory length: 85600   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 460   score: 2.0   memory length: 85797   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 461   score: 4.0   memory length: 86114   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 462   score: 1.0   memory length: 86264   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 463   score: 2.0   memory length: 86461   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 464   score: 2.0   memory length: 86661   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 465   score: 0.0   memory length: 86784   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 466   score: 3.0   memory length: 87012   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 467   score: 0.0   memory length: 87134   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 468   score: 2.0   memory length: 87314   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 469   score: 1.0   memory length: 87464   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 470   score: 3.0   memory length: 87674   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 471   score: 0.0   memory length: 87797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 472   score: 0.0   memory length: 87920   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 473   score: 2.0   memory length: 88118   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 474   score: 2.0   memory length: 88316   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 475   score: 3.0   memory length: 88546   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 476   score: 1.0   memory length: 88697   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 477   score: 3.0   memory length: 88944   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 478   score: 3.0   memory length: 89188   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 479   score: 0.0   memory length: 89311   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 480   score: 1.0   memory length: 89483   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 481   score: 2.0   memory length: 89700   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 482   score: 2.0   memory length: 89898   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 483   score: 1.0   memory length: 90050   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 484   score: 3.0   memory length: 90275   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 485   score: 1.0   memory length: 90444   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 486   score: 0.0   memory length: 90567   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 487   score: 3.0   memory length: 90815   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 488   score: 1.0   memory length: 90985   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 489   score: 0.0   memory length: 91107   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 490   score: 2.0   memory length: 91324   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 491   score: 2.0   memory length: 91526   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 492   score: 2.0   memory length: 91723   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 493   score: 6.0   memory length: 92117   epsilon: 1.0    steps: 394    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 494   score: 2.0   memory length: 92315   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 495   score: 1.0   memory length: 92466   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 496   score: 3.0   memory length: 92731   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 497   score: 1.0   memory length: 92881   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 498   score: 0.0   memory length: 93004   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 499   score: 0.0   memory length: 93127   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 500   score: 0.0   memory length: 93250   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 501   score: 2.0   memory length: 93434   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 502   score: 2.0   memory length: 93631   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 503   score: 0.0   memory length: 93753   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 504   score: 0.0   memory length: 93875   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 505   score: 1.0   memory length: 94026   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 506   score: 2.0   memory length: 94224   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 507   score: 3.0   memory length: 94454   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 508   score: 2.0   memory length: 94673   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 509   score: 5.0   memory length: 94979   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 510   score: 0.0   memory length: 95101   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 511   score: 2.0   memory length: 95299   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 512   score: 0.0   memory length: 95422   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 513   score: 1.0   memory length: 95573   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 514   score: 0.0   memory length: 95696   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 515   score: 0.0   memory length: 95819   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 516   score: 3.0   memory length: 96063   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 517   score: 1.0   memory length: 96214   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 518   score: 2.0   memory length: 96432   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 519   score: 2.0   memory length: 96630   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 520   score: 0.0   memory length: 96752   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 521   score: 0.0   memory length: 96875   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 522   score: 0.0   memory length: 96998   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 523   score: 5.0   memory length: 97307   epsilon: 1.0    steps: 309    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 524   score: 1.0   memory length: 97476   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 525   score: 0.0   memory length: 97598   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 526   score: 1.0   memory length: 97749   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 527   score: 0.0   memory length: 97872   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 528   score: 2.0   memory length: 98089   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 529   score: 3.0   memory length: 98318   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 530   score: 1.0   memory length: 98486   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 531   score: 4.0   memory length: 98783   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 532   score: 1.0   memory length: 98934   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 533   score: 4.0   memory length: 99232   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 534   score: 1.0   memory length: 99401   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 535   score: 1.0   memory length: 99570   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 536   score: 0.0   memory length: 99692   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 537   score: 1.0   memory length: 99860   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 538   score: 1.0   memory length: 100029   epsilon: 0.9999406000000013    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 539   score: 0.0   memory length: 100152   epsilon: 0.9996970600000066    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 540   score: 0.0   memory length: 100274   epsilon: 0.9994555000000118    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 541   score: 0.0   memory length: 100397   epsilon: 0.9992119600000171    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 542   score: 1.0   memory length: 100569   epsilon: 0.9988714000000245    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 543   score: 2.0   memory length: 100767   epsilon: 0.998479360000033    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 544   score: 3.0   memory length: 101032   epsilon: 0.9979546600000444    steps: 265    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 545   score: 2.0   memory length: 101229   epsilon: 0.9975646000000529    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 546   score: 1.0   memory length: 101398   epsilon: 0.9972299800000601    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 547   score: 0.0   memory length: 101521   epsilon: 0.9969864400000654    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 548   score: 1.0   memory length: 101691   epsilon: 0.9966498400000727    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 549   score: 0.0   memory length: 101813   epsilon: 0.996408280000078    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 550   score: 1.0   memory length: 101964   epsilon: 0.9961093000000845    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 551   score: 3.0   memory length: 102229   epsilon: 0.9955846000000959    steps: 265    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 552   score: 3.0   memory length: 102475   epsilon: 0.9950975200001064    steps: 246    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 553   score: 0.0   memory length: 102598   epsilon: 0.9948539800001117    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 554   score: 1.0   memory length: 102767   epsilon: 0.994519360000119    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 555   score: 0.0   memory length: 102890   epsilon: 0.9942758200001243    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 556   score: 0.0   memory length: 103013   epsilon: 0.9940322800001296    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 557   score: 0.0   memory length: 103136   epsilon: 0.9937887400001348    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 558   score: 2.0   memory length: 103353   epsilon: 0.9933590800001442    steps: 217    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 559   score: 2.0   memory length: 103552   epsilon: 0.9929650600001527    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 560   score: 0.0   memory length: 103675   epsilon: 0.992721520000158    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 561   score: 2.0   memory length: 103894   epsilon: 0.9922879000001674    steps: 219    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 562   score: 1.0   memory length: 104062   epsilon: 0.9919552600001746    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 563   score: 5.0   memory length: 104371   epsilon: 0.9913434400001879    steps: 309    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 564   score: 0.0   memory length: 104494   epsilon: 0.9910999000001932    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 565   score: 3.0   memory length: 104740   epsilon: 0.9906128200002038    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 566   score: 1.0   memory length: 104891   epsilon: 0.9903138400002103    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 567   score: 1.0   memory length: 105059   epsilon: 0.9899812000002175    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 568   score: 1.0   memory length: 105228   epsilon: 0.9896465800002248    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 569   score: 3.0   memory length: 105474   epsilon: 0.9891595000002353    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 570   score: 3.0   memory length: 105722   epsilon: 0.988668460000246    steps: 248    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 571   score: 2.0   memory length: 105920   epsilon: 0.9882764200002545    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 572   score: 2.0   memory length: 106118   epsilon: 0.987884380000263    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 573   score: 2.0   memory length: 106333   epsilon: 0.9874586800002723    steps: 215    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 574   score: 2.0   memory length: 106531   epsilon: 0.9870666400002808    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 575   score: 1.0   memory length: 106699   epsilon: 0.986734000000288    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 576   score: 0.0   memory length: 106822   epsilon: 0.9864904600002933    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 577   score: 7.0   memory length: 107272   epsilon: 0.9855994600003126    steps: 450    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 578   score: 2.0   memory length: 107470   epsilon: 0.9852074200003211    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 579   score: 0.0   memory length: 107592   epsilon: 0.9849658600003264    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 580   score: 0.0   memory length: 107715   epsilon: 0.9847223200003317    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 581   score: 1.0   memory length: 107866   epsilon: 0.9844233400003382    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 582   score: 2.0   memory length: 108064   epsilon: 0.9840313000003467    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 583   score: 0.0   memory length: 108187   epsilon: 0.983787760000352    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 584   score: 1.0   memory length: 108356   epsilon: 0.9834531400003592    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 585   score: 1.0   memory length: 108507   epsilon: 0.9831541600003657    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 586   score: 0.0   memory length: 108630   epsilon: 0.982910620000371    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 587   score: 3.0   memory length: 108845   epsilon: 0.9824849200003802    steps: 215    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 588   score: 1.0   memory length: 109013   epsilon: 0.9821522800003875    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 589   score: 2.0   memory length: 109232   epsilon: 0.9817186600003969    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 590   score: 1.0   memory length: 109383   epsilon: 0.9814196800004034    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 591   score: 0.0   memory length: 109506   epsilon: 0.9811761400004086    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 592   score: 2.0   memory length: 109704   epsilon: 0.9807841000004172    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 593   score: 4.0   memory length: 109976   epsilon: 0.9802455400004288    steps: 272    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 594   score: 0.0   memory length: 110099   epsilon: 0.9800020000004341    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 595   score: 0.0   memory length: 110222   epsilon: 0.9797584600004394    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 596   score: 2.0   memory length: 110403   epsilon: 0.9794000800004472    steps: 181    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 597   score: 3.0   memory length: 110629   epsilon: 0.9789526000004569    steps: 226    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 598   score: 0.0   memory length: 110752   epsilon: 0.9787090600004622    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 599   score: 0.0   memory length: 110874   epsilon: 0.9784675000004674    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 600   score: 1.0   memory length: 111045   epsilon: 0.9781289200004748    steps: 171    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 601   score: 1.0   memory length: 111197   epsilon: 0.9778279600004813    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 602   score: 4.0   memory length: 111493   epsilon: 0.9772418800004941    steps: 296    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 603   score: 2.0   memory length: 111711   epsilon: 0.9768102400005034    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 604   score: 2.0   memory length: 111909   epsilon: 0.9764182000005119    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 605   score: 1.0   memory length: 112078   epsilon: 0.9760835800005192    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 606   score: 0.0   memory length: 112201   epsilon: 0.9758400400005245    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 607   score: 2.0   memory length: 112419   epsilon: 0.9754084000005339    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 608   score: 1.0   memory length: 112569   epsilon: 0.9751114000005403    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 609   score: 3.0   memory length: 112815   epsilon: 0.9746243200005509    steps: 246    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 610   score: 0.0   memory length: 112937   epsilon: 0.9743827600005561    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 611   score: 2.0   memory length: 113134   epsilon: 0.9739927000005646    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 612   score: 1.0   memory length: 113286   epsilon: 0.9736917400005711    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 613   score: 0.0   memory length: 113409   epsilon: 0.9734482000005764    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 614   score: 0.0   memory length: 113531   epsilon: 0.9732066400005817    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 615   score: 0.0   memory length: 113654   epsilon: 0.972963100000587    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 616   score: 4.0   memory length: 113919   epsilon: 0.9724384000005983    steps: 265    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 617   score: 0.0   memory length: 114042   epsilon: 0.9721948600006036    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 618   score: 2.0   memory length: 114240   epsilon: 0.9718028200006121    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 619   score: 0.0   memory length: 114362   epsilon: 0.9715612600006174    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 620   score: 3.0   memory length: 114625   epsilon: 0.9710405200006287    steps: 263    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 621   score: 0.0   memory length: 114748   epsilon: 0.970796980000634    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 622   score: 1.0   memory length: 114916   epsilon: 0.9704643400006412    steps: 168    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 623   score: 0.0   memory length: 115039   epsilon: 0.9702208000006465    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 624   score: 4.0   memory length: 115317   epsilon: 0.9696703600006584    steps: 278    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 625   score: 2.0   memory length: 115516   epsilon: 0.969276340000667    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 626   score: 1.0   memory length: 115686   epsilon: 0.9689397400006743    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 627   score: 1.0   memory length: 115837   epsilon: 0.9686407600006808    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 628   score: 1.0   memory length: 116008   epsilon: 0.9683021800006881    steps: 171    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 629   score: 3.0   memory length: 116254   epsilon: 0.9678151000006987    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 630   score: 4.0   memory length: 116516   epsilon: 0.96729634000071    steps: 262    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 631   score: 3.0   memory length: 116760   epsilon: 0.9668132200007205    steps: 244    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 632   score: 2.0   memory length: 116958   epsilon: 0.966421180000729    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 633   score: 0.0   memory length: 117081   epsilon: 0.9661776400007343    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 634   score: 0.0   memory length: 117204   epsilon: 0.9659341000007395    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 635   score: 1.0   memory length: 117374   epsilon: 0.9655975000007468    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 636   score: 1.0   memory length: 117525   epsilon: 0.9652985200007533    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 637   score: 0.0   memory length: 117648   epsilon: 0.9650549800007586    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 638   score: 1.0   memory length: 117818   epsilon: 0.9647183800007659    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 639   score: 1.0   memory length: 117990   epsilon: 0.9643778200007733    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 640   score: 2.0   memory length: 118190   epsilon: 0.9639818200007819    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 641   score: 1.0   memory length: 118341   epsilon: 0.9636828400007884    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 642   score: 0.0   memory length: 118464   epsilon: 0.9634393000007937    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 643   score: 0.0   memory length: 118587   epsilon: 0.963195760000799    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 644   score: 0.0   memory length: 118709   epsilon: 0.9629542000008042    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 645   score: 1.0   memory length: 118859   epsilon: 0.9626572000008107    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 646   score: 0.0   memory length: 118981   epsilon: 0.9624156400008159    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 647   score: 2.0   memory length: 119199   epsilon: 0.9619840000008253    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 648   score: 0.0   memory length: 119322   epsilon: 0.9617404600008306    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 649   score: 1.0   memory length: 119493   epsilon: 0.9614018800008379    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 650   score: 2.0   memory length: 119693   epsilon: 0.9610058800008465    steps: 200    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 651   score: 0.0   memory length: 119816   epsilon: 0.9607623400008518    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 652   score: 1.0   memory length: 119968   epsilon: 0.9604613800008583    steps: 152    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 653   score: 1.0   memory length: 120136   epsilon: 0.9601287400008656    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 654   score: 5.0   memory length: 120479   epsilon: 0.9594496000008803    steps: 343    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 655   score: 0.0   memory length: 120602   epsilon: 0.9592060600008856    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 656   score: 10.0   memory length: 121019   epsilon: 0.9583804000009035    steps: 417    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 657   score: 2.0   memory length: 121217   epsilon: 0.957988360000912    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 658   score: 3.0   memory length: 121460   epsilon: 0.9575072200009225    steps: 243    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 659   score: 2.0   memory length: 121675   epsilon: 0.9570815200009317    steps: 215    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 660   score: 4.0   memory length: 121950   epsilon: 0.9565370200009435    steps: 275    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 661   score: 3.0   memory length: 122198   epsilon: 0.9560459800009542    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 662   score: 0.0   memory length: 122320   epsilon: 0.9558044200009594    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 663   score: 1.0   memory length: 122471   epsilon: 0.9555054400009659    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 664   score: 3.0   memory length: 122717   epsilon: 0.9550183600009765    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 665   score: 0.0   memory length: 122840   epsilon: 0.9547748200009818    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 666   score: 2.0   memory length: 123056   epsilon: 0.9543471400009911    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 667   score: 2.0   memory length: 123272   epsilon: 0.9539194600010004    steps: 216    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 668   score: 0.0   memory length: 123395   epsilon: 0.9536759200010057    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 669   score: 1.0   memory length: 123545   epsilon: 0.9533789200010121    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 670   score: 1.0   memory length: 123715   epsilon: 0.9530423200010194    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 671   score: 1.0   memory length: 123865   epsilon: 0.9527453200010259    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 672   score: 3.0   memory length: 124091   epsilon: 0.9522978400010356    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 673   score: 2.0   memory length: 124275   epsilon: 0.9519335200010435    steps: 184    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 674   score: 1.0   memory length: 124443   epsilon: 0.9516008800010507    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 675   score: 0.0   memory length: 124565   epsilon: 0.9513593200010559    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 676   score: 6.0   memory length: 124933   epsilon: 0.9506306800010718    steps: 368    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 677   score: 2.0   memory length: 125149   epsilon: 0.950203000001081    steps: 216    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 678   score: 1.0   memory length: 125300   epsilon: 0.9499040200010875    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 679   score: 1.0   memory length: 125450   epsilon: 0.949607020001094    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 680   score: 3.0   memory length: 125718   epsilon: 0.9490763800011055    steps: 268    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 681   score: 1.0   memory length: 125869   epsilon: 0.948777400001112    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 682   score: 0.0   memory length: 125991   epsilon: 0.9485358400011172    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 683   score: 0.0   memory length: 126113   epsilon: 0.9482942800011225    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 684   score: 0.0   memory length: 126236   epsilon: 0.9480507400011278    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 685   score: 2.0   memory length: 126418   epsilon: 0.9476903800011356    steps: 182    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 686   score: 2.0   memory length: 126616   epsilon: 0.9472983400011441    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 687   score: 1.0   memory length: 126766   epsilon: 0.9470013400011505    steps: 150    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 688   score: 3.0   memory length: 126994   epsilon: 0.9465499000011603    steps: 228    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 689   score: 4.0   memory length: 127307   epsilon: 0.9459301600011738    steps: 313    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 690   score: 2.0   memory length: 127506   epsilon: 0.9455361400011824    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 691   score: 1.0   memory length: 127657   epsilon: 0.9452371600011888    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 692   score: 1.0   memory length: 127826   epsilon: 0.9449025400011961    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 693   score: 0.0   memory length: 127949   epsilon: 0.9446590000012014    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 694   score: 2.0   memory length: 128129   epsilon: 0.9443026000012091    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 695   score: 2.0   memory length: 128326   epsilon: 0.9439125400012176    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 696   score: 0.0   memory length: 128449   epsilon: 0.9436690000012229    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 697   score: 0.0   memory length: 128572   epsilon: 0.9434254600012282    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 698   score: 2.0   memory length: 128787   epsilon: 0.9429997600012374    steps: 215    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 699   score: 2.0   memory length: 128985   epsilon: 0.9426077200012459    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 700   score: 0.0   memory length: 129108   epsilon: 0.9423641800012512    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 701   score: 2.0   memory length: 129306   epsilon: 0.9419721400012597    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 702   score: 2.0   memory length: 129522   epsilon: 0.941544460001269    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 703   score: 2.0   memory length: 129719   epsilon: 0.9411544000012775    steps: 197    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 704   score: 2.0   memory length: 129934   epsilon: 0.9407287000012867    steps: 215    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 705   score: 2.0   memory length: 130151   epsilon: 0.940299040001296    steps: 217    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 706   score: 0.0   memory length: 130274   epsilon: 0.9400555000013013    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 707   score: 1.0   memory length: 130424   epsilon: 0.9397585000013078    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 708   score: 3.0   memory length: 130652   epsilon: 0.9393070600013176    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 709   score: 2.0   memory length: 130870   epsilon: 0.938875420001327    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 710   score: 3.0   memory length: 131116   epsilon: 0.9383883400013375    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 711   score: 0.0   memory length: 131239   epsilon: 0.9381448000013428    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 712   score: 0.0   memory length: 131361   epsilon: 0.9379032400013481    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 713   score: 3.0   memory length: 131607   epsilon: 0.9374161600013586    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 714   score: 1.0   memory length: 131758   epsilon: 0.9371171800013651    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 715   score: 0.0   memory length: 131881   epsilon: 0.9368736400013704    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 716   score: 0.0   memory length: 132004   epsilon: 0.9366301000013757    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 717   score: 0.0   memory length: 132127   epsilon: 0.936386560001381    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 718   score: 4.0   memory length: 132384   epsilon: 0.935877700001392    steps: 257    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 719   score: 3.0   memory length: 132651   epsilon: 0.9353490400014035    steps: 267    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 720   score: 1.0   memory length: 132820   epsilon: 0.9350144200014108    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 721   score: 1.0   memory length: 132970   epsilon: 0.9347174200014172    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 722   score: 3.0   memory length: 133233   epsilon: 0.9341966800014285    steps: 263    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 723   score: 1.0   memory length: 133402   epsilon: 0.9338620600014358    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 724   score: 1.0   memory length: 133570   epsilon: 0.933529420001443    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 725   score: 1.0   memory length: 133721   epsilon: 0.9332304400014495    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 726   score: 6.0   memory length: 134096   epsilon: 0.9324879400014656    steps: 375    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 727   score: 1.0   memory length: 134248   epsilon: 0.9321869800014722    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 728   score: 8.0   memory length: 134677   epsilon: 0.9313375600014906    steps: 429    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 729   score: 0.0   memory length: 134800   epsilon: 0.9310940200014959    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 730   score: 2.0   memory length: 134998   epsilon: 0.9307019800015044    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 731   score: 3.0   memory length: 135242   epsilon: 0.9302188600015149    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 732   score: 3.0   memory length: 135505   epsilon: 0.9296981200015262    steps: 263    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 733   score: 0.0   memory length: 135627   epsilon: 0.9294565600015314    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 734   score: 2.0   memory length: 135845   epsilon: 0.9290249200015408    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 735   score: 1.0   memory length: 136013   epsilon: 0.928692280001548    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 736   score: 1.0   memory length: 136183   epsilon: 0.9283556800015553    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 737   score: 0.0   memory length: 136305   epsilon: 0.9281141200015606    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 738   score: 2.0   memory length: 136503   epsilon: 0.9277220800015691    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 739   score: 1.0   memory length: 136671   epsilon: 0.9273894400015763    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 740   score: 3.0   memory length: 136937   epsilon: 0.9268627600015877    steps: 266    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 741   score: 0.0   memory length: 137059   epsilon: 0.926621200001593    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 742   score: 3.0   memory length: 137323   epsilon: 0.9260984800016043    steps: 264    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 743   score: 0.0   memory length: 137446   epsilon: 0.9258549400016096    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 744   score: 0.0   memory length: 137568   epsilon: 0.9256133800016149    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 745   score: 4.0   memory length: 137844   epsilon: 0.9250669000016267    steps: 276    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 746   score: 3.0   memory length: 138078   epsilon: 0.9246035800016368    steps: 234    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 747   score: 0.0   memory length: 138201   epsilon: 0.9243600400016421    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 748   score: 0.0   memory length: 138324   epsilon: 0.9241165000016474    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 749   score: 1.0   memory length: 138492   epsilon: 0.9237838600016546    steps: 168    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 750   score: 0.0   memory length: 138615   epsilon: 0.9235403200016599    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 751   score: 0.0   memory length: 138738   epsilon: 0.9232967800016652    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 752   score: 0.0   memory length: 138860   epsilon: 0.9230552200016704    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 753   score: 3.0   memory length: 139086   epsilon: 0.9226077400016801    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 754   score: 1.0   memory length: 139238   epsilon: 0.9223067800016866    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 755   score: 1.0   memory length: 139389   epsilon: 0.9220078000016931    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 756   score: 4.0   memory length: 139687   epsilon: 0.9214177600017059    steps: 298    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 757   score: 1.0   memory length: 139838   epsilon: 0.9211187800017124    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 758   score: 5.0   memory length: 140162   epsilon: 0.9204772600017264    steps: 324    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 759   score: 0.0   memory length: 140285   epsilon: 0.9202337200017316    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 760   score: 1.0   memory length: 140456   epsilon: 0.919895140001739    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 761   score: 0.0   memory length: 140578   epsilon: 0.9196535800017442    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 762   score: 1.0   memory length: 140729   epsilon: 0.9193546000017507    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 763   score: 1.0   memory length: 140901   epsilon: 0.9190140400017581    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 764   score: 0.0   memory length: 141024   epsilon: 0.9187705000017634    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 765   score: 1.0   memory length: 141192   epsilon: 0.9184378600017706    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 766   score: 3.0   memory length: 141438   epsilon: 0.9179507800017812    steps: 246    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 767   score: 2.0   memory length: 141637   epsilon: 0.9175567600017898    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 768   score: 1.0   memory length: 141790   epsilon: 0.9172538200017963    steps: 153    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 769   score: 1.0   memory length: 141940   epsilon: 0.9169568200018028    steps: 150    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 770   score: 0.0   memory length: 142063   epsilon: 0.9167132800018081    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 771   score: 3.0   memory length: 142289   epsilon: 0.9162658000018178    steps: 226    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 772   score: 4.0   memory length: 142600   epsilon: 0.9156500200018312    steps: 311    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 773   score: 3.0   memory length: 142845   epsilon: 0.9151649200018417    steps: 245    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 774   score: 1.0   memory length: 142996   epsilon: 0.9148659400018482    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 775   score: 2.0   memory length: 143214   epsilon: 0.9144343000018575    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 776   score: 5.0   memory length: 143521   epsilon: 0.9138264400018707    steps: 307    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 777   score: 0.0   memory length: 143644   epsilon: 0.913582900001876    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 778   score: 2.0   memory length: 143865   epsilon: 0.9131453200018855    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 779   score: 4.0   memory length: 144139   epsilon: 0.9126028000018973    steps: 274    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 780   score: 0.0   memory length: 144262   epsilon: 0.9123592600019026    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 781   score: 0.0   memory length: 144385   epsilon: 0.9121157200019079    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 782   score: 1.0   memory length: 144536   epsilon: 0.9118167400019144    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 783   score: 3.0   memory length: 144782   epsilon: 0.911329660001925    steps: 246    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 784   score: 4.0   memory length: 145096   epsilon: 0.9107079400019384    steps: 314    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 785   score: 2.0   memory length: 145293   epsilon: 0.9103178800019469    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 786   score: 0.0   memory length: 145416   epsilon: 0.9100743400019522    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 787   score: 3.0   memory length: 145646   epsilon: 0.9096189400019621    steps: 230    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 788   score: 1.0   memory length: 145797   epsilon: 0.9093199600019686    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 789   score: 3.0   memory length: 146008   epsilon: 0.9089021800019776    steps: 211    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 790   score: 0.0   memory length: 146131   epsilon: 0.9086586400019829    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 791   score: 0.0   memory length: 146254   epsilon: 0.9084151000019882    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 792   score: 1.0   memory length: 146405   epsilon: 0.9081161200019947    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 793   score: 3.0   memory length: 146651   epsilon: 0.9076290400020053    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 794   score: 1.0   memory length: 146820   epsilon: 0.9072944200020125    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 795   score: 0.0   memory length: 146943   epsilon: 0.9070508800020178    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 796   score: 3.0   memory length: 147208   epsilon: 0.9065261800020292    steps: 265    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 797   score: 0.0   memory length: 147330   epsilon: 0.9062846200020345    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 798   score: 1.0   memory length: 147502   epsilon: 0.9059440600020419    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 799   score: 0.0   memory length: 147625   epsilon: 0.9057005200020471    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 800   score: 3.0   memory length: 147873   epsilon: 0.9052094800020578    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 801   score: 2.0   memory length: 148092   epsilon: 0.9047758600020672    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 802   score: 1.0   memory length: 148261   epsilon: 0.9044412400020745    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 803   score: 0.0   memory length: 148383   epsilon: 0.9041996800020797    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 804   score: 2.0   memory length: 148581   epsilon: 0.9038076400020882    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 805   score: 0.0   memory length: 148704   epsilon: 0.9035641000020935    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 806   score: 2.0   memory length: 148901   epsilon: 0.903174040002102    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 807   score: 4.0   memory length: 149170   epsilon: 0.9026414200021136    steps: 269    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 808   score: 1.0   memory length: 149338   epsilon: 0.9023087800021208    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 809   score: 0.0   memory length: 149461   epsilon: 0.9020652400021261    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 810   score: 0.0   memory length: 149584   epsilon: 0.9018217000021314    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 811   score: 2.0   memory length: 149784   epsilon: 0.90142570000214    steps: 200    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 812   score: 1.0   memory length: 149952   epsilon: 0.9010930600021472    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 813   score: 3.0   memory length: 150177   epsilon: 0.9006475600021568    steps: 225    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 814   score: 0.0   memory length: 150299   epsilon: 0.9004060000021621    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 815   score: 2.0   memory length: 150496   epsilon: 0.9000159400021706    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 816   score: 4.0   memory length: 150774   epsilon: 0.8994655000021825    steps: 278    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 817   score: 0.0   memory length: 150897   epsilon: 0.8992219600021878    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 818   score: 0.0   memory length: 151019   epsilon: 0.898980400002193    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 819   score: 1.0   memory length: 151170   epsilon: 0.8986814200021995    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 820   score: 3.0   memory length: 151396   epsilon: 0.8982339400022092    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 821   score: 3.0   memory length: 151640   epsilon: 0.8977508200022197    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 822   score: 3.0   memory length: 151909   epsilon: 0.8972182000022313    steps: 269    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 823   score: 2.0   memory length: 152128   epsilon: 0.8967845800022407    steps: 219    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 824   score: 1.0   memory length: 152279   epsilon: 0.8964856000022472    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 825   score: 1.0   memory length: 152429   epsilon: 0.8961886000022536    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 826   score: 2.0   memory length: 152626   epsilon: 0.8957985400022621    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 827   score: 2.0   memory length: 152842   epsilon: 0.8953708600022714    steps: 216    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 828   score: 1.0   memory length: 152992   epsilon: 0.8950738600022778    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 829   score: 2.0   memory length: 153191   epsilon: 0.8946798400022864    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 830   score: 1.0   memory length: 153342   epsilon: 0.8943808600022929    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 831   score: 8.0   memory length: 153754   epsilon: 0.8935651000023106    steps: 412    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 832   score: 0.0   memory length: 153876   epsilon: 0.8933235400023158    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 833   score: 2.0   memory length: 154092   epsilon: 0.8928958600023251    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 834   score: 1.0   memory length: 154242   epsilon: 0.8925988600023316    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 835   score: 0.0   memory length: 154365   epsilon: 0.8923553200023369    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 836   score: 0.0   memory length: 154488   epsilon: 0.8921117800023421    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 837   score: 0.0   memory length: 154610   epsilon: 0.8918702200023474    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 838   score: 1.0   memory length: 154761   epsilon: 0.8915712400023539    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 839   score: 2.0   memory length: 154961   epsilon: 0.8911752400023625    steps: 200    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 840   score: 1.0   memory length: 155112   epsilon: 0.890876260002369    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 841   score: 0.0   memory length: 155234   epsilon: 0.8906347000023742    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 842   score: 5.0   memory length: 155560   epsilon: 0.8899892200023882    steps: 326    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 843   score: 2.0   memory length: 155757   epsilon: 0.8895991600023967    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 844   score: 2.0   memory length: 155954   epsilon: 0.8892091000024052    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 845   score: 1.0   memory length: 156126   epsilon: 0.8888685400024126    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 846   score: 1.0   memory length: 156277   epsilon: 0.888569560002419    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 847   score: 4.0   memory length: 156591   epsilon: 0.8879478400024325    steps: 314    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 848   score: 1.0   memory length: 156741   epsilon: 0.887650840002439    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 849   score: 2.0   memory length: 156938   epsilon: 0.8872607800024475    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 850   score: 0.0   memory length: 157060   epsilon: 0.8870192200024527    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 851   score: 2.0   memory length: 157259   epsilon: 0.8866252000024613    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 852   score: 2.0   memory length: 157456   epsilon: 0.8862351400024697    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 853   score: 3.0   memory length: 157722   epsilon: 0.8857084600024812    steps: 266    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 854   score: 2.0   memory length: 157920   epsilon: 0.8853164200024897    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 855   score: 2.0   memory length: 158118   epsilon: 0.8849243800024982    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 856   score: 2.0   memory length: 158316   epsilon: 0.8845323400025067    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 857   score: 0.0   memory length: 158439   epsilon: 0.884288800002512    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 858   score: 1.0   memory length: 158607   epsilon: 0.8839561600025192    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 859   score: 2.0   memory length: 158825   epsilon: 0.8835245200025286    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 860   score: 3.0   memory length: 159052   epsilon: 0.8830750600025383    steps: 227    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 861   score: 2.0   memory length: 159250   epsilon: 0.8826830200025468    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 862   score: 2.0   memory length: 159448   epsilon: 0.8822909800025553    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 863   score: 1.0   memory length: 159617   epsilon: 0.8819563600025626    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 864   score: 1.0   memory length: 159767   epsilon: 0.8816593600025691    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 865   score: 3.0   memory length: 160010   epsilon: 0.8811782200025795    steps: 243    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 866   score: 2.0   memory length: 160207   epsilon: 0.880788160002588    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 867   score: 2.0   memory length: 160405   epsilon: 0.8803961200025965    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 868   score: 3.0   memory length: 160653   epsilon: 0.8799050800026071    steps: 248    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 869   score: 1.0   memory length: 160825   epsilon: 0.8795645200026145    steps: 172    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 870   score: 0.0   memory length: 160948   epsilon: 0.8793209800026198    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 871   score: 0.0   memory length: 161071   epsilon: 0.8790774400026251    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 872   score: 3.0   memory length: 161319   epsilon: 0.8785864000026358    steps: 248    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 873   score: 1.0   memory length: 161469   epsilon: 0.8782894000026422    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 874   score: 0.0   memory length: 161591   epsilon: 0.8780478400026475    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 875   score: 1.0   memory length: 161742   epsilon: 0.877748860002654    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 876   score: 0.0   memory length: 161864   epsilon: 0.8775073000026592    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 877   score: 2.0   memory length: 162062   epsilon: 0.8771152600026677    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 878   score: 2.0   memory length: 162244   epsilon: 0.8767549000026755    steps: 182    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 879   score: 3.0   memory length: 162491   epsilon: 0.8762658400026861    steps: 247    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 880   score: 1.0   memory length: 162642   epsilon: 0.8759668600026926    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 881   score: 2.0   memory length: 162859   epsilon: 0.875537200002702    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 882   score: 1.0   memory length: 163009   epsilon: 0.8752402000027084    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 883   score: 3.0   memory length: 163252   epsilon: 0.8747590600027189    steps: 243    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 884   score: 2.0   memory length: 163449   epsilon: 0.8743690000027273    steps: 197    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 885   score: 1.0   memory length: 163618   epsilon: 0.8740343800027346    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 886   score: 2.0   memory length: 163816   epsilon: 0.8736423400027431    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 887   score: 1.0   memory length: 163986   epsilon: 0.8733057400027504    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 888   score: 1.0   memory length: 164137   epsilon: 0.8730067600027569    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 889   score: 2.0   memory length: 164334   epsilon: 0.8726167000027654    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 890   score: 0.0   memory length: 164457   epsilon: 0.8723731600027707    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 891   score: 1.0   memory length: 164607   epsilon: 0.8720761600027771    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 892   score: 0.0   memory length: 164730   epsilon: 0.8718326200027824    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 893   score: 1.0   memory length: 164901   epsilon: 0.8714940400027897    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 894   score: 0.0   memory length: 165024   epsilon: 0.871250500002795    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 895   score: 3.0   memory length: 165249   epsilon: 0.8708050000028047    steps: 225    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 896   score: 1.0   memory length: 165419   epsilon: 0.870468400002812    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 897   score: 2.0   memory length: 165616   epsilon: 0.8700783400028205    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 898   score: 2.0   memory length: 165815   epsilon: 0.869684320002829    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 899   score: 0.0   memory length: 165938   epsilon: 0.8694407800028343    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 900   score: 2.0   memory length: 166121   epsilon: 0.8690784400028422    steps: 183    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 901   score: 2.0   memory length: 166319   epsilon: 0.8686864000028507    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 902   score: 2.0   memory length: 166534   epsilon: 0.8682607000028599    steps: 215    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 903   score: 0.0   memory length: 166657   epsilon: 0.8680171600028652    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 904   score: 3.0   memory length: 166904   epsilon: 0.8675281000028758    steps: 247    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 905   score: 0.0   memory length: 167026   epsilon: 0.8672865400028811    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 906   score: 3.0   memory length: 167273   epsilon: 0.8667974800028917    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 907   score: 0.0   memory length: 167396   epsilon: 0.866553940002897    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 908   score: 2.0   memory length: 167594   epsilon: 0.8661619000029055    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 909   score: 0.0   memory length: 167716   epsilon: 0.8659203400029107    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 910   score: 4.0   memory length: 168009   epsilon: 0.8653402000029233    steps: 293    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 911   score: 5.0   memory length: 168335   epsilon: 0.8646947200029373    steps: 326    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 912   score: 3.0   memory length: 168600   epsilon: 0.8641700200029487    steps: 265    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 913   score: 0.0   memory length: 168723   epsilon: 0.863926480002954    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 914   score: 0.0   memory length: 168845   epsilon: 0.8636849200029593    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 915   score: 2.0   memory length: 169046   epsilon: 0.8632869400029679    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 916   score: 2.0   memory length: 169246   epsilon: 0.8628909400029765    steps: 200    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 917   score: 0.0   memory length: 169368   epsilon: 0.8626493800029817    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 918   score: 0.0   memory length: 169490   epsilon: 0.862407820002987    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 919   score: 1.0   memory length: 169658   epsilon: 0.8620751800029942    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 920   score: 0.0   memory length: 169780   epsilon: 0.8618336200029995    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 921   score: 1.0   memory length: 169950   epsilon: 0.8614970200030068    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 922   score: 2.0   memory length: 170148   epsilon: 0.8611049800030153    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 923   score: 3.0   memory length: 170413   epsilon: 0.8605802800030267    steps: 265    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 924   score: 1.0   memory length: 170585   epsilon: 0.8602397200030341    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 925   score: 1.0   memory length: 170754   epsilon: 0.8599051000030413    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 926   score: 2.0   memory length: 170951   epsilon: 0.8595150400030498    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 927   score: 3.0   memory length: 171215   epsilon: 0.8589923200030611    steps: 264    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 928   score: 0.0   memory length: 171337   epsilon: 0.8587507600030664    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 929   score: 1.0   memory length: 171505   epsilon: 0.8584181200030736    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 930   score: 1.0   memory length: 171673   epsilon: 0.8580854800030808    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 931   score: 2.0   memory length: 171854   epsilon: 0.8577271000030886    steps: 181    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 932   score: 1.0   memory length: 172004   epsilon: 0.857430100003095    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 933   score: 3.0   memory length: 172251   epsilon: 0.8569410400031057    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 934   score: 0.0   memory length: 172374   epsilon: 0.856697500003111    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 935   score: 1.0   memory length: 172544   epsilon: 0.8563609000031183    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 936   score: 2.0   memory length: 172762   epsilon: 0.8559292600031276    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 937   score: 2.0   memory length: 172959   epsilon: 0.8555392000031361    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 938   score: 2.0   memory length: 173157   epsilon: 0.8551471600031446    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 939   score: 0.0   memory length: 173279   epsilon: 0.8549056000031499    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 940   score: 0.0   memory length: 173402   epsilon: 0.8546620600031551    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 941   score: 0.0   memory length: 173525   epsilon: 0.8544185200031604    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 942   score: 0.0   memory length: 173647   epsilon: 0.8541769600031657    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 943   score: 0.0   memory length: 173769   epsilon: 0.8539354000031709    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 944   score: 6.0   memory length: 174141   epsilon: 0.8531988400031869    steps: 372    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 945   score: 0.0   memory length: 174263   epsilon: 0.8529572800031922    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 946   score: 1.0   memory length: 174432   epsilon: 0.8526226600031994    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 947   score: 0.0   memory length: 174554   epsilon: 0.8523811000032047    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 948   score: 0.0   memory length: 174677   epsilon: 0.85213756000321    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 949   score: 1.0   memory length: 174846   epsilon: 0.8518029400032172    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 950   score: 0.0   memory length: 174968   epsilon: 0.8515613800032225    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 951   score: 2.0   memory length: 175165   epsilon: 0.8511713200032309    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 952   score: 1.0   memory length: 175316   epsilon: 0.8508723400032374    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 953   score: 0.0   memory length: 175438   epsilon: 0.8506307800032427    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 954   score: 1.0   memory length: 175589   epsilon: 0.8503318000032491    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 955   score: 2.0   memory length: 175787   epsilon: 0.8499397600032577    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 956   score: 3.0   memory length: 176034   epsilon: 0.8494507000032683    steps: 247    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 957   score: 1.0   memory length: 176185   epsilon: 0.8491517200032748    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 958   score: 3.0   memory length: 176410   epsilon: 0.8487062200032844    steps: 225    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 959   score: 1.0   memory length: 176560   epsilon: 0.8484092200032909    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 960   score: 3.0   memory length: 176787   epsilon: 0.8479597600033006    steps: 227    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 961   score: 0.0   memory length: 176909   epsilon: 0.8477182000033059    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 962   score: 1.0   memory length: 177059   epsilon: 0.8474212000033123    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 963   score: 0.0   memory length: 177182   epsilon: 0.8471776600033176    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 964   score: 0.0   memory length: 177304   epsilon: 0.8469361000033229    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 965   score: 0.0   memory length: 177426   epsilon: 0.8466945400033281    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 966   score: 1.0   memory length: 177598   epsilon: 0.8463539800033355    steps: 172    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 967   score: 2.0   memory length: 177795   epsilon: 0.845963920003344    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 968   score: 1.0   memory length: 177964   epsilon: 0.8456293000033512    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 969   score: 2.0   memory length: 178162   epsilon: 0.8452372600033597    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 970   score: 1.0   memory length: 178330   epsilon: 0.844904620003367    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 971   score: 2.0   memory length: 178547   epsilon: 0.8444749600033763    steps: 217    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 972   score: 3.0   memory length: 178772   epsilon: 0.844029460003386    steps: 225    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 973   score: 3.0   memory length: 179019   epsilon: 0.8435404000033966    steps: 247    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 974   score: 4.0   memory length: 179314   epsilon: 0.8429563000034093    steps: 295    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 975   score: 2.0   memory length: 179531   epsilon: 0.8425266400034186    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 976   score: 2.0   memory length: 179729   epsilon: 0.8421346000034271    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 977   score: 0.0   memory length: 179852   epsilon: 0.8418910600034324    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 978   score: 1.0   memory length: 180021   epsilon: 0.8415564400034397    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 979   score: 3.0   memory length: 180248   epsilon: 0.8411069800034494    steps: 227    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 980   score: 0.0   memory length: 180371   epsilon: 0.8408634400034547    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 981   score: 0.0   memory length: 180493   epsilon: 0.8406218800034599    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 982   score: 2.0   memory length: 180690   epsilon: 0.8402318200034684    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 983   score: 0.0   memory length: 180813   epsilon: 0.8399882800034737    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 984   score: 3.0   memory length: 181059   epsilon: 0.8395012000034843    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 985   score: 0.0   memory length: 181182   epsilon: 0.8392576600034896    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 986   score: 0.0   memory length: 181304   epsilon: 0.8390161000034948    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 987   score: 2.0   memory length: 181502   epsilon: 0.8386240600035033    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 988   score: 0.0   memory length: 181625   epsilon: 0.8383805200035086    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 989   score: 0.0   memory length: 181747   epsilon: 0.8381389600035138    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 990   score: 2.0   memory length: 181944   epsilon: 0.8377489000035223    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 991   score: 3.0   memory length: 182214   epsilon: 0.8372143000035339    steps: 270    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 992   score: 0.0   memory length: 182337   epsilon: 0.8369707600035392    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 993   score: 0.0   memory length: 182460   epsilon: 0.8367272200035445    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 994   score: 1.0   memory length: 182610   epsilon: 0.8364302200035509    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 995   score: 3.0   memory length: 182835   epsilon: 0.8359847200035606    steps: 225    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 996   score: 2.0   memory length: 183014   epsilon: 0.8356303000035683    steps: 179    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 997   score: 1.0   memory length: 183164   epsilon: 0.8353333000035748    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 998   score: 2.0   memory length: 183384   epsilon: 0.8348977000035842    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 999   score: 0.0   memory length: 183507   epsilon: 0.8346541600035895    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1000   score: 3.0   memory length: 183757   epsilon: 0.8341591600036002    steps: 250    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1001   score: 0.0   memory length: 183879   epsilon: 0.8339176000036055    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1002   score: 1.0   memory length: 184030   epsilon: 0.833618620003612    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1003   score: 0.0   memory length: 184153   epsilon: 0.8333750800036173    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1004   score: 2.0   memory length: 184351   epsilon: 0.8329830400036258    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1005   score: 0.0   memory length: 184473   epsilon: 0.832741480003631    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1006   score: 2.0   memory length: 184670   epsilon: 0.8323514200036395    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1007   score: 1.0   memory length: 184839   epsilon: 0.8320168000036468    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1008   score: 0.0   memory length: 184961   epsilon: 0.831775240003652    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1009   score: 3.0   memory length: 185209   epsilon: 0.8312842000036627    steps: 248    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1010   score: 2.0   memory length: 185407   epsilon: 0.8308921600036712    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1011   score: 1.0   memory length: 185576   epsilon: 0.8305575400036784    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1012   score: 0.0   memory length: 185698   epsilon: 0.8303159800036837    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1013   score: 3.0   memory length: 185941   epsilon: 0.8298348400036941    steps: 243    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1014   score: 0.0   memory length: 186064   epsilon: 0.8295913000036994    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1015   score: 2.0   memory length: 186282   epsilon: 0.8291596600037088    steps: 218    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1016   score: 2.0   memory length: 186479   epsilon: 0.8287696000037172    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1017   score: 1.0   memory length: 186647   epsilon: 0.8284369600037245    steps: 168    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1018   score: 0.0   memory length: 186769   epsilon: 0.8281954000037297    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1019   score: 2.0   memory length: 186991   epsilon: 0.8277558400037393    steps: 222    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1020   score: 0.0   memory length: 187114   epsilon: 0.8275123000037445    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1021   score: 2.0   memory length: 187311   epsilon: 0.827122240003753    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1022   score: 1.0   memory length: 187483   epsilon: 0.8267816800037604    steps: 172    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1023   score: 3.0   memory length: 187713   epsilon: 0.8263262800037703    steps: 230    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1024   score: 3.0   memory length: 187957   epsilon: 0.8258431600037808    steps: 244    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1025   score: 2.0   memory length: 188155   epsilon: 0.8254511200037893    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1026   score: 0.0   memory length: 188277   epsilon: 0.8252095600037945    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1027   score: 2.0   memory length: 188496   epsilon: 0.8247759400038039    steps: 219    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1028   score: 4.0   memory length: 188759   epsilon: 0.8242552000038152    steps: 263    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1029   score: 1.0   memory length: 188910   epsilon: 0.8239562200038217    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1030   score: 1.0   memory length: 189060   epsilon: 0.8236592200038282    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1031   score: 0.0   memory length: 189183   epsilon: 0.8234156800038335    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1032   score: 3.0   memory length: 189429   epsilon: 0.822928600003844    steps: 246    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1033   score: 3.0   memory length: 189698   epsilon: 0.8223959800038556    steps: 269    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1034   score: 2.0   memory length: 189896   epsilon: 0.8220039400038641    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1035   score: 0.0   memory length: 190019   epsilon: 0.8217604000038694    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1036   score: 2.0   memory length: 190217   epsilon: 0.8213683600038779    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1037   score: 0.0   memory length: 190339   epsilon: 0.8211268000038832    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1038   score: 1.0   memory length: 190508   epsilon: 0.8207921800038904    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1039   score: 3.0   memory length: 190758   epsilon: 0.8202971800039012    steps: 250    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1040   score: 1.0   memory length: 190926   epsilon: 0.8199645400039084    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1041   score: 2.0   memory length: 191147   epsilon: 0.8195269600039179    steps: 221    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1042   score: 1.0   memory length: 191315   epsilon: 0.8191943200039251    steps: 168    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1043   score: 6.0   memory length: 191688   epsilon: 0.8184557800039411    steps: 373    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1044   score: 1.0   memory length: 191859   epsilon: 0.8181172000039485    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1045   score: 2.0   memory length: 192056   epsilon: 0.817727140003957    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1046   score: 2.0   memory length: 192254   epsilon: 0.8173351000039655    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1047   score: 2.0   memory length: 192451   epsilon: 0.8169450400039739    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1048   score: 1.0   memory length: 192620   epsilon: 0.8166104200039812    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1049   score: 0.0   memory length: 192743   epsilon: 0.8163668800039865    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1050   score: 1.0   memory length: 192911   epsilon: 0.8160342400039937    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1051   score: 0.0   memory length: 193033   epsilon: 0.815792680003999    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1052   score: 2.0   memory length: 193231   epsilon: 0.8154006400040075    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1053   score: 0.0   memory length: 193354   epsilon: 0.8151571000040128    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1054   score: 1.0   memory length: 193505   epsilon: 0.8148581200040192    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1055   score: 0.0   memory length: 193628   epsilon: 0.8146145800040245    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1056   score: 0.0   memory length: 193751   epsilon: 0.8143710400040298    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1057   score: 3.0   memory length: 193995   epsilon: 0.8138879200040403    steps: 244    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1058   score: 3.0   memory length: 194242   epsilon: 0.8133988600040509    steps: 247    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1059   score: 0.0   memory length: 194364   epsilon: 0.8131573000040562    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1060   score: 1.0   memory length: 194515   epsilon: 0.8128583200040627    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1061   score: 1.0   memory length: 194684   epsilon: 0.8125237000040699    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1062   score: 2.0   memory length: 194902   epsilon: 0.8120920600040793    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1063   score: 0.0   memory length: 195024   epsilon: 0.8118505000040845    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1064   score: 2.0   memory length: 195242   epsilon: 0.8114188600040939    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1065   score: 1.0   memory length: 195392   epsilon: 0.8111218600041004    steps: 150    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1066   score: 0.0   memory length: 195515   epsilon: 0.8108783200041056    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1067   score: 0.0   memory length: 195638   epsilon: 0.8106347800041109    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1068   score: 3.0   memory length: 195902   epsilon: 0.8101120600041223    steps: 264    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1069   score: 2.0   memory length: 196119   epsilon: 0.8096824000041316    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1070   score: 0.0   memory length: 196241   epsilon: 0.8094408400041369    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1071   score: 2.0   memory length: 196457   epsilon: 0.8090131600041461    steps: 216    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1072   score: 0.0   memory length: 196580   epsilon: 0.8087696200041514    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1073   score: 0.0   memory length: 196702   epsilon: 0.8085280600041567    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1074   score: 1.0   memory length: 196873   epsilon: 0.808189480004164    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1075   score: 3.0   memory length: 197141   epsilon: 0.8076588400041755    steps: 268    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1076   score: 3.0   memory length: 197368   epsilon: 0.8072093800041853    steps: 227    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1077   score: 1.0   memory length: 197519   epsilon: 0.8069104000041918    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1078   score: 1.0   memory length: 197670   epsilon: 0.8066114200041983    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1079   score: 0.0   memory length: 197793   epsilon: 0.8063678800042036    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1080   score: 1.0   memory length: 197944   epsilon: 0.80606890000421    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1081   score: 3.0   memory length: 198209   epsilon: 0.8055442000042214    steps: 265    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1082   score: 5.0   memory length: 198550   epsilon: 0.8048690200042361    steps: 341    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1083   score: 2.0   memory length: 198750   epsilon: 0.8044730200042447    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1084   score: 1.0   memory length: 198900   epsilon: 0.8041760200042511    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1085   score: 1.0   memory length: 199051   epsilon: 0.8038770400042576    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1086   score: 2.0   memory length: 199249   epsilon: 0.8034850000042661    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1087   score: 0.0   memory length: 199371   epsilon: 0.8032434400042714    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1088   score: 0.0   memory length: 199493   epsilon: 0.8030018800042766    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1089   score: 4.0   memory length: 199778   epsilon: 0.8024375800042889    steps: 285    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1090   score: 0.0   memory length: 199901   epsilon: 0.8021940400042942    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1091   score: 0.0   memory length: 200024   epsilon: 0.8019505000042995    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1092   score: 1.0   memory length: 200193   epsilon: 0.8016158800043067    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1093   score: 2.0   memory length: 200392   epsilon: 0.8012218600043153    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1094   score: 2.0   memory length: 200608   epsilon: 0.8007941800043246    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1095   score: 2.0   memory length: 200823   epsilon: 0.8003684800043338    steps: 215    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1096   score: 1.0   memory length: 200993   epsilon: 0.8000318800043411    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1097   score: 2.0   memory length: 201191   epsilon: 0.7996398400043496    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1098   score: 0.0   memory length: 201314   epsilon: 0.7993963000043549    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1099   score: 1.0   memory length: 201465   epsilon: 0.7990973200043614    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1100   score: 1.0   memory length: 201616   epsilon: 0.7987983400043679    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1101   score: 0.0   memory length: 201738   epsilon: 0.7985567800043731    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1102   score: 3.0   memory length: 201985   epsilon: 0.7980677200043838    steps: 247    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1103   score: 0.0   memory length: 202108   epsilon: 0.797824180004389    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1104   score: 2.0   memory length: 202308   epsilon: 0.7974281800043976    steps: 200    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1105   score: 2.0   memory length: 202527   epsilon: 0.796994560004407    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1106   score: 1.0   memory length: 202698   epsilon: 0.7966559800044144    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1107   score: 1.0   memory length: 202868   epsilon: 0.7963193800044217    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1108   score: 1.0   memory length: 203019   epsilon: 0.7960204000044282    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1109   score: 0.0   memory length: 203141   epsilon: 0.7957788400044334    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1110   score: 0.0   memory length: 203264   epsilon: 0.7955353000044387    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1111   score: 3.0   memory length: 203491   epsilon: 0.7950858400044485    steps: 227    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1112   score: 0.0   memory length: 203614   epsilon: 0.7948423000044538    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1113   score: 1.0   memory length: 203783   epsilon: 0.794507680004461    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1114   score: 1.0   memory length: 203954   epsilon: 0.7941691000044684    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1115   score: 1.0   memory length: 204125   epsilon: 0.7938305200044757    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1116   score: 2.0   memory length: 204322   epsilon: 0.7934404600044842    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1117   score: 2.0   memory length: 204540   epsilon: 0.7930088200044936    steps: 218    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1118   score: 2.0   memory length: 204738   epsilon: 0.7926167800045021    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1119   score: 0.0   memory length: 204860   epsilon: 0.7923752200045073    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1120   score: 2.0   memory length: 205078   epsilon: 0.7919435800045167    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1121   score: 0.0   memory length: 205200   epsilon: 0.791702020004522    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1122   score: 1.0   memory length: 205371   epsilon: 0.7913634400045293    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1123   score: 0.0   memory length: 205494   epsilon: 0.7911199000045346    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1124   score: 1.0   memory length: 205644   epsilon: 0.790822900004541    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1125   score: 3.0   memory length: 205891   epsilon: 0.7903338400045516    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1126   score: 3.0   memory length: 206135   epsilon: 0.7898507200045621    steps: 244    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1127   score: 0.0   memory length: 206257   epsilon: 0.7896091600045674    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1128   score: 0.0   memory length: 206380   epsilon: 0.7893656200045727    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1129   score: 1.0   memory length: 206531   epsilon: 0.7890666400045792    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1130   score: 1.0   memory length: 206699   epsilon: 0.7887340000045864    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1131   score: 0.0   memory length: 206822   epsilon: 0.7884904600045917    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1132   score: 0.0   memory length: 206944   epsilon: 0.7882489000045969    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1133   score: 0.0   memory length: 207066   epsilon: 0.7880073400046022    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 1134   score: 2.0   memory length: 207284   epsilon: 0.7875757000046115    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 1135   score: 3.0   memory length: 207509   epsilon: 0.7871302000046212    steps: 225    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1136   score: 0.0   memory length: 207632   epsilon: 0.7868866600046265    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1137   score: 2.0   memory length: 207829   epsilon: 0.786496600004635    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1138   score: 3.0   memory length: 208055   epsilon: 0.7860491200046447    steps: 226    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1139   score: 2.0   memory length: 208253   epsilon: 0.7856570800046532    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1140   score: 3.0   memory length: 208497   epsilon: 0.7851739600046637    steps: 244    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1141   score: 1.0   memory length: 208667   epsilon: 0.784837360004671    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1142   score: 0.0   memory length: 208790   epsilon: 0.7845938200046763    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1143   score: 0.0   memory length: 208912   epsilon: 0.7843522600046815    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1144   score: 2.0   memory length: 209127   epsilon: 0.7839265600046907    steps: 215    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1145   score: 2.0   memory length: 209324   epsilon: 0.7835365000046992    steps: 197    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1146   score: 1.0   memory length: 209494   epsilon: 0.7831999000047065    steps: 170    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1147   score: 2.0   memory length: 209691   epsilon: 0.782809840004715    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1148   score: 3.0   memory length: 209940   epsilon: 0.7823168200047257    steps: 249    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 1149   score: 1.0   memory length: 210108   epsilon: 0.7819841800047329    steps: 168    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1150   score: 2.0   memory length: 210305   epsilon: 0.7815941200047414    steps: 197    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1151   score: 2.0   memory length: 210503   epsilon: 0.7812020800047499    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1152   score: 0.0   memory length: 210625   epsilon: 0.7809605200047551    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1153   score: 0.0   memory length: 210747   epsilon: 0.7807189600047604    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1154   score: 6.0   memory length: 211140   epsilon: 0.7799408200047773    steps: 393    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1155   score: 1.0   memory length: 211291   epsilon: 0.7796418400047838    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1156   score: 2.0   memory length: 211492   epsilon: 0.7792438600047924    steps: 201    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1157   score: 0.0   memory length: 211614   epsilon: 0.7790023000047976    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1158   score: 0.0   memory length: 211737   epsilon: 0.7787587600048029    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1159   score: 2.0   memory length: 211934   epsilon: 0.7783687000048114    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1160   score: 2.0   memory length: 212113   epsilon: 0.7780142800048191    steps: 179    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1161   score: 0.0   memory length: 212235   epsilon: 0.7777727200048243    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1162   score: 1.0   memory length: 212385   epsilon: 0.7774757200048308    steps: 150    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1163   score: 0.0   memory length: 212507   epsilon: 0.777234160004836    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1164   score: 3.0   memory length: 212736   epsilon: 0.7767807400048459    steps: 229    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1165   score: 3.0   memory length: 212962   epsilon: 0.7763332600048556    steps: 226    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1166   score: 0.0   memory length: 213084   epsilon: 0.7760917000048608    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1167   score: 4.0   memory length: 213378   epsilon: 0.7755095800048735    steps: 294    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1168   score: 2.0   memory length: 213593   epsilon: 0.7750838800048827    steps: 215    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1169   score: 0.0   memory length: 213716   epsilon: 0.774840340004888    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1170   score: 3.0   memory length: 213945   epsilon: 0.7743869200048978    steps: 229    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1171   score: 0.0   memory length: 214068   epsilon: 0.7741433800049031    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1172   score: 2.0   memory length: 214287   epsilon: 0.7737097600049125    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1173   score: 1.0   memory length: 214437   epsilon: 0.773412760004919    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1174   score: 4.0   memory length: 214732   epsilon: 0.7728286600049317    steps: 295    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1175   score: 1.0   memory length: 214883   epsilon: 0.7725296800049382    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1176   score: 5.0   memory length: 215227   epsilon: 0.7718485600049529    steps: 344    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1177   score: 0.0   memory length: 215350   epsilon: 0.7716050200049582    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1178   score: 2.0   memory length: 215568   epsilon: 0.7711733800049676    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1179   score: 2.0   memory length: 215765   epsilon: 0.7707833200049761    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1180   score: 3.0   memory length: 216031   epsilon: 0.7702566400049875    steps: 266    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1181   score: 0.0   memory length: 216153   epsilon: 0.7700150800049927    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1182   score: 6.0   memory length: 216537   epsilon: 0.7692547600050093    steps: 384    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1183   score: 0.0   memory length: 216660   epsilon: 0.7690112200050145    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1184   score: 0.0   memory length: 216783   epsilon: 0.7687676800050198    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1185   score: 0.0   memory length: 216905   epsilon: 0.7685261200050251    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1186   score: 4.0   memory length: 217199   epsilon: 0.7679440000050377    steps: 294    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1187   score: 1.0   memory length: 217368   epsilon: 0.767609380005045    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1188   score: 1.0   memory length: 217518   epsilon: 0.7673123800050514    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1189   score: 1.0   memory length: 217688   epsilon: 0.7669757800050587    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1190   score: 2.0   memory length: 217888   epsilon: 0.7665797800050673    steps: 200    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1191   score: 3.0   memory length: 218155   epsilon: 0.7660511200050788    steps: 267    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1192   score: 0.0   memory length: 218278   epsilon: 0.7658075800050841    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1193   score: 0.0   memory length: 218401   epsilon: 0.7655640400050894    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1194   score: 4.0   memory length: 218674   epsilon: 0.7650235000051011    steps: 273    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1195   score: 1.0   memory length: 218842   epsilon: 0.7646908600051083    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1196   score: 0.0   memory length: 218965   epsilon: 0.7644473200051136    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1197   score: 1.0   memory length: 219116   epsilon: 0.7641483400051201    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1198   score: 3.0   memory length: 219381   epsilon: 0.7636236400051315    steps: 265    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1199   score: 1.0   memory length: 219532   epsilon: 0.763324660005138    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1200   score: 2.0   memory length: 219753   epsilon: 0.7628870800051475    steps: 221    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1201   score: 3.0   memory length: 220019   epsilon: 0.7623604000051589    steps: 266    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1202   score: 1.0   memory length: 220170   epsilon: 0.7620614200051654    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1203   score: 0.0   memory length: 220293   epsilon: 0.7618178800051707    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1204   score: 2.0   memory length: 220511   epsilon: 0.7613862400051801    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1205   score: 1.0   memory length: 220679   epsilon: 0.7610536000051873    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1206   score: 3.0   memory length: 220907   epsilon: 0.7606021600051971    steps: 228    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1207   score: 1.0   memory length: 221058   epsilon: 0.7603031800052036    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1208   score: 4.0   memory length: 221314   epsilon: 0.7597963000052146    steps: 256    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1209   score: 1.0   memory length: 221464   epsilon: 0.759499300005221    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1210   score: 0.0   memory length: 221587   epsilon: 0.7592557600052263    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1211   score: 2.0   memory length: 221784   epsilon: 0.7588657000052348    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1212   score: 0.0   memory length: 221906   epsilon: 0.75862414000524    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1213   score: 1.0   memory length: 222074   epsilon: 0.7582915000052473    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1214   score: 0.0   memory length: 222197   epsilon: 0.7580479600052525    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1215   score: 3.0   memory length: 222462   epsilon: 0.7575232600052639    steps: 265    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1216   score: 0.0   memory length: 222584   epsilon: 0.7572817000052692    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1217   score: 5.0   memory length: 222912   epsilon: 0.7566322600052833    steps: 328    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1218   score: 3.0   memory length: 223158   epsilon: 0.7561451800052938    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1219   score: 2.0   memory length: 223356   epsilon: 0.7557531400053024    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1220   score: 1.0   memory length: 223524   epsilon: 0.7554205000053096    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1221   score: 2.0   memory length: 223722   epsilon: 0.7550284600053181    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1222   score: 1.0   memory length: 223872   epsilon: 0.7547314600053245    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1223   score: 1.0   memory length: 224041   epsilon: 0.7543968400053318    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1224   score: 1.0   memory length: 224191   epsilon: 0.7540998400053383    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1225   score: 2.0   memory length: 224388   epsilon: 0.7537097800053467    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1226   score: 2.0   memory length: 224606   epsilon: 0.7532781400053561    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1227   score: 1.0   memory length: 224774   epsilon: 0.7529455000053633    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1228   score: 2.0   memory length: 224995   epsilon: 0.7525079200053728    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1229   score: 0.0   memory length: 225118   epsilon: 0.7522643800053781    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1230   score: 0.0   memory length: 225241   epsilon: 0.7520208400053834    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1231   score: 1.0   memory length: 225411   epsilon: 0.7516842400053907    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1232   score: 2.0   memory length: 225609   epsilon: 0.7512922000053992    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1233   score: 2.0   memory length: 225807   epsilon: 0.7509001600054077    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1234   score: 1.0   memory length: 225958   epsilon: 0.7506011800054142    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1235   score: 2.0   memory length: 226139   epsilon: 0.750242800005422    steps: 181    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1236   score: 0.0   memory length: 226262   epsilon: 0.7499992600054273    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1237   score: 2.0   memory length: 226463   epsilon: 0.7496012800054359    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1238   score: 4.0   memory length: 226757   epsilon: 0.7490191600054485    steps: 294    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1239   score: 1.0   memory length: 226908   epsilon: 0.748720180005455    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1240   score: 1.0   memory length: 227076   epsilon: 0.7483875400054623    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1241   score: 1.0   memory length: 227245   epsilon: 0.7480529200054695    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1242   score: 3.0   memory length: 227491   epsilon: 0.7475658400054801    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1243   score: 0.0   memory length: 227614   epsilon: 0.7473223000054854    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1244   score: 3.0   memory length: 227882   epsilon: 0.7467916600054969    steps: 268    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1245   score: 0.0   memory length: 228005   epsilon: 0.7465481200055022    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1246   score: 3.0   memory length: 228249   epsilon: 0.7460650000055127    steps: 244    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1247   score: 3.0   memory length: 228498   epsilon: 0.7455719800055234    steps: 249    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1248   score: 1.0   memory length: 228668   epsilon: 0.7452353800055307    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1249   score: 0.0   memory length: 228791   epsilon: 0.744991840005536    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1250   score: 2.0   memory length: 228990   epsilon: 0.7445978200055445    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1251   score: 1.0   memory length: 229141   epsilon: 0.744298840005551    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1252   score: 2.0   memory length: 229339   epsilon: 0.7439068000055595    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1253   score: 3.0   memory length: 229565   epsilon: 0.7434593200055692    steps: 226    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1254   score: 0.0   memory length: 229687   epsilon: 0.7432177600055745    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1255   score: 2.0   memory length: 229908   epsilon: 0.742780180005584    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1256   score: 2.0   memory length: 230127   epsilon: 0.7423465600055934    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1257   score: 2.0   memory length: 230325   epsilon: 0.7419545200056019    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1258   score: 2.0   memory length: 230543   epsilon: 0.7415228800056113    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1259   score: 2.0   memory length: 230740   epsilon: 0.7411328200056198    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1260   score: 2.0   memory length: 230937   epsilon: 0.7407427600056282    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1261   score: 1.0   memory length: 231105   epsilon: 0.7404101200056354    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1262   score: 2.0   memory length: 231322   epsilon: 0.7399804600056448    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1263   score: 0.0   memory length: 231444   epsilon: 0.73973890000565    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1264   score: 2.0   memory length: 231641   epsilon: 0.7393488400056585    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1265   score: 3.0   memory length: 231888   epsilon: 0.7388597800056691    steps: 247    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1266   score: 1.0   memory length: 232056   epsilon: 0.7385271400056763    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1267   score: 3.0   memory length: 232284   epsilon: 0.7380757000056861    steps: 228    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1268   score: 0.0   memory length: 232406   epsilon: 0.7378341400056914    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1269   score: 1.0   memory length: 232576   epsilon: 0.7374975400056987    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1270   score: 0.0   memory length: 232699   epsilon: 0.737254000005704    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1271   score: 3.0   memory length: 232924   epsilon: 0.7368085000057136    steps: 225    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1272   score: 1.0   memory length: 233093   epsilon: 0.7364738800057209    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1273   score: 0.0   memory length: 233216   epsilon: 0.7362303400057262    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1274   score: 3.0   memory length: 233462   epsilon: 0.7357432600057368    steps: 246    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1275   score: 2.0   memory length: 233660   epsilon: 0.7353512200057453    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1276   score: 1.0   memory length: 233810   epsilon: 0.7350542200057517    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1277   score: 1.0   memory length: 233979   epsilon: 0.734719600005759    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1278   score: 2.0   memory length: 234176   epsilon: 0.7343295400057674    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1279   score: 1.0   memory length: 234345   epsilon: 0.7339949200057747    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1280   score: 1.0   memory length: 234517   epsilon: 0.7336543600057821    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1281   score: 2.0   memory length: 234733   epsilon: 0.7332266800057914    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1282   score: 0.0   memory length: 234856   epsilon: 0.7329831400057967    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1283   score: 4.0   memory length: 235131   epsilon: 0.7324386400058085    steps: 275    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1284   score: 3.0   memory length: 235395   epsilon: 0.7319159200058198    steps: 264    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1285   score: 2.0   memory length: 235596   epsilon: 0.7315179400058285    steps: 201    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1286   score: 5.0   memory length: 235898   epsilon: 0.7309199800058415    steps: 302    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1287   score: 0.0   memory length: 236021   epsilon: 0.7306764400058468    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1288   score: 3.0   memory length: 236292   epsilon: 0.7301398600058584    steps: 271    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1289   score: 2.0   memory length: 236510   epsilon: 0.7297082200058678    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1290   score: 0.0   memory length: 236632   epsilon: 0.729466660005873    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1291   score: 2.0   memory length: 236832   epsilon: 0.7290706600058816    steps: 200    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1292   score: 0.0   memory length: 236955   epsilon: 0.7288271200058869    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1293   score: 2.0   memory length: 237172   epsilon: 0.7283974600058962    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1294   score: 0.0   memory length: 237295   epsilon: 0.7281539200059015    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1295   score: 0.0   memory length: 237418   epsilon: 0.7279103800059068    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1296   score: 2.0   memory length: 237637   epsilon: 0.7274767600059162    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1297   score: 1.0   memory length: 237805   epsilon: 0.7271441200059234    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1298   score: 5.0   memory length: 238168   epsilon: 0.726425380005939    steps: 363    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1299   score: 0.0   memory length: 238290   epsilon: 0.7261838200059443    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1300   score: 0.0   memory length: 238412   epsilon: 0.7259422600059495    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1301   score: 0.0   memory length: 238535   epsilon: 0.7256987200059548    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1302   score: 0.0   memory length: 238657   epsilon: 0.7254571600059601    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1303   score: 3.0   memory length: 238882   epsilon: 0.7250116600059697    steps: 225    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1304   score: 0.0   memory length: 239004   epsilon: 0.724770100005975    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1305   score: 2.0   memory length: 239202   epsilon: 0.7243780600059835    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1306   score: 0.0   memory length: 239324   epsilon: 0.7241365000059887    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1307   score: 0.0   memory length: 239446   epsilon: 0.723894940005994    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1308   score: 0.0   memory length: 239569   epsilon: 0.7236514000059993    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1309   score: 0.0   memory length: 239691   epsilon: 0.7234098400060045    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1310   score: 0.0   memory length: 239813   epsilon: 0.7231682800060097    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1311   score: 1.0   memory length: 239984   epsilon: 0.7228297000060171    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1312   score: 3.0   memory length: 240210   epsilon: 0.7223822200060268    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1313   score: 0.0   memory length: 240333   epsilon: 0.7221386800060321    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1314   score: 1.0   memory length: 240501   epsilon: 0.7218060400060393    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1315   score: 1.0   memory length: 240652   epsilon: 0.7215070600060458    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1316   score: 0.0   memory length: 240775   epsilon: 0.7212635200060511    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1317   score: 0.0   memory length: 240898   epsilon: 0.7210199800060564    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1318   score: 0.0   memory length: 241021   epsilon: 0.7207764400060617    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1319   score: 2.0   memory length: 241219   epsilon: 0.7203844000060702    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1320   score: 0.0   memory length: 241341   epsilon: 0.7201428400060754    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1321   score: 1.0   memory length: 241512   epsilon: 0.7198042600060828    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1322   score: 0.0   memory length: 241635   epsilon: 0.7195607200060881    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1323   score: 4.0   memory length: 241912   epsilon: 0.7190122600061    steps: 277    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1324   score: 2.0   memory length: 242112   epsilon: 0.7186162600061086    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1325   score: 2.0   memory length: 242309   epsilon: 0.718226200006117    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1326   score: 2.0   memory length: 242527   epsilon: 0.7177945600061264    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1327   score: 0.0   memory length: 242650   epsilon: 0.7175510200061317    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1328   score: 1.0   memory length: 242819   epsilon: 0.717216400006139    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1329   score: 3.0   memory length: 243067   epsilon: 0.7167253600061496    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1330   score: 0.0   memory length: 243189   epsilon: 0.7164838000061549    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1331   score: 0.0   memory length: 243311   epsilon: 0.7162422400061601    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1332   score: 0.0   memory length: 243433   epsilon: 0.7160006800061653    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1333   score: 3.0   memory length: 243679   epsilon: 0.7155136000061759    steps: 246    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1334   score: 1.0   memory length: 243830   epsilon: 0.7152146200061824    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1335   score: 0.0   memory length: 243953   epsilon: 0.7149710800061877    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1336   score: 2.0   memory length: 244170   epsilon: 0.714541420006197    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1337   score: 0.0   memory length: 244293   epsilon: 0.7142978800062023    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1338   score: 2.0   memory length: 244490   epsilon: 0.7139078200062108    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1339   score: 3.0   memory length: 244715   epsilon: 0.7134623200062205    steps: 225    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1340   score: 3.0   memory length: 244948   epsilon: 0.7130009800062305    steps: 233    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1341   score: 0.0   memory length: 245070   epsilon: 0.7127594200062357    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1342   score: 2.0   memory length: 245267   epsilon: 0.7123693600062442    steps: 197    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1343   score: 0.0   memory length: 245389   epsilon: 0.7121278000062494    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1344   score: 0.0   memory length: 245511   epsilon: 0.7118862400062547    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1345   score: 2.0   memory length: 245708   epsilon: 0.7114961800062631    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1346   score: 1.0   memory length: 245860   epsilon: 0.7111952200062697    steps: 152    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1347   score: 3.0   memory length: 246086   epsilon: 0.7107477400062794    steps: 226    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1348   score: 1.0   memory length: 246236   epsilon: 0.7104507400062858    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1349   score: 2.0   memory length: 246434   epsilon: 0.7100587000062943    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1350   score: 1.0   memory length: 246603   epsilon: 0.7097240800063016    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1351   score: 1.0   memory length: 246753   epsilon: 0.709427080006308    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1352   score: 1.0   memory length: 246904   epsilon: 0.7091281000063145    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1353   score: 2.0   memory length: 247122   epsilon: 0.7086964600063239    steps: 218    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1354   score: 2.0   memory length: 247339   epsilon: 0.7082668000063332    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1355   score: 1.0   memory length: 247489   epsilon: 0.7079698000063397    steps: 150    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1356   score: 1.0   memory length: 247639   epsilon: 0.7076728000063461    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1357   score: 1.0   memory length: 247789   epsilon: 0.7073758000063526    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1358   score: 0.0   memory length: 247911   epsilon: 0.7071342400063578    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1359   score: 0.0   memory length: 248034   epsilon: 0.7068907000063631    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1360   score: 0.0   memory length: 248156   epsilon: 0.7066491400063684    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 1361   score: 2.0   memory length: 248372   epsilon: 0.7062214600063776    steps: 216    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1362   score: 2.0   memory length: 248571   epsilon: 0.7058274400063862    steps: 199    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1363   score: 2.0   memory length: 248792   epsilon: 0.7053898600063957    steps: 221    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1364   score: 0.0   memory length: 248914   epsilon: 0.7051483000064009    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1365   score: 3.0   memory length: 249142   epsilon: 0.7046968600064107    steps: 228    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1366   score: 1.0   memory length: 249292   epsilon: 0.7043998600064172    steps: 150    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1367   score: 0.0   memory length: 249414   epsilon: 0.7041583000064224    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1368   score: 0.0   memory length: 249537   epsilon: 0.7039147600064277    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1369   score: 0.0   memory length: 249660   epsilon: 0.703671220006433    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1370   score: 2.0   memory length: 249877   epsilon: 0.7032415600064423    steps: 217    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1371   score: 1.0   memory length: 250046   epsilon: 0.7029069400064496    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1372   score: 2.0   memory length: 250243   epsilon: 0.7025168800064581    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1373   score: 2.0   memory length: 250440   epsilon: 0.7021268200064665    steps: 197    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 1374   score: 0.0   memory length: 250563   epsilon: 0.7018832800064718    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1375   score: 0.0   memory length: 250686   epsilon: 0.7016397400064771    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1376   score: 0.0   memory length: 250809   epsilon: 0.7013962000064824    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1377   score: 1.0   memory length: 250960   epsilon: 0.7010972200064889    steps: 151    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1378   score: 3.0   memory length: 251209   epsilon: 0.7006042000064996    steps: 249    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1379   score: 0.0   memory length: 251331   epsilon: 0.7003626400065048    steps: 122    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1380   score: 2.0   memory length: 251549   epsilon: 0.6999310000065142    steps: 218    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1381   score: 2.0   memory length: 251746   epsilon: 0.6995409400065227    steps: 197    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1382   score: 2.0   memory length: 251964   epsilon: 0.699109300006532    steps: 218    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1383   score: 3.0   memory length: 252211   epsilon: 0.6986202400065427    steps: 247    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1384   score: 2.0   memory length: 252409   epsilon: 0.6982282000065512    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1385   score: 2.0   memory length: 252612   epsilon: 0.6978262600065599    steps: 203    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1386   score: 2.0   memory length: 252809   epsilon: 0.6974362000065684    steps: 197    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1387   score: 0.0   memory length: 252931   epsilon: 0.6971946400065736    steps: 122    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1388   score: 2.0   memory length: 253150   epsilon: 0.696761020006583    steps: 219    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 1389   score: 2.0   memory length: 253369   epsilon: 0.6963274000065924    steps: 219    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 1390   score: 1.0   memory length: 253520   epsilon: 0.6960284200065989    steps: 151    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1391   score: 2.0   memory length: 253720   epsilon: 0.6956324200066075    steps: 200    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1392   score: 4.0   memory length: 253994   epsilon: 0.6950899000066193    steps: 274    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1393   score: 10.0   memory length: 254442   epsilon: 0.6942028600066386    steps: 448    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1394   score: 1.0   memory length: 254611   epsilon: 0.6938682400066458    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1395   score: 2.0   memory length: 254808   epsilon: 0.6934781800066543    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1396   score: 2.0   memory length: 255025   epsilon: 0.6930485200066636    steps: 217    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1397   score: 3.0   memory length: 255235   epsilon: 0.6926327200066726    steps: 210    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1398   score: 3.0   memory length: 255465   epsilon: 0.6921773200066825    steps: 230    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1399   score: 0.0   memory length: 255588   epsilon: 0.6919337800066878    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1400   score: 0.0   memory length: 255710   epsilon: 0.6916922200066931    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1401   score: 0.0   memory length: 255832   epsilon: 0.6914506600066983    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1402   score: 2.0   memory length: 256029   epsilon: 0.6910606000067068    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1403   score: 3.0   memory length: 256257   epsilon: 0.6906091600067166    steps: 228    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1404   score: 0.0   memory length: 256380   epsilon: 0.6903656200067219    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1405   score: 1.0   memory length: 256552   epsilon: 0.6900250600067293    steps: 172    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1406   score: 2.0   memory length: 256770   epsilon: 0.6895934200067386    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1407   score: 0.0   memory length: 256893   epsilon: 0.6893498800067439    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1408   score: 4.0   memory length: 257170   epsilon: 0.6888014200067558    steps: 277    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1409   score: 2.0   memory length: 257368   epsilon: 0.6884093800067643    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1410   score: 3.0   memory length: 257616   epsilon: 0.687918340006775    steps: 248    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1411   score: 0.0   memory length: 257738   epsilon: 0.6876767800067802    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1412   score: 5.0   memory length: 258043   epsilon: 0.6870728800067933    steps: 305    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1413   score: 0.0   memory length: 258166   epsilon: 0.6868293400067986    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1414   score: 2.0   memory length: 258364   epsilon: 0.6864373000068071    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1415   score: 0.0   memory length: 258487   epsilon: 0.6861937600068124    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1416   score: 1.0   memory length: 258655   epsilon: 0.6858611200068196    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1417   score: 1.0   memory length: 258807   epsilon: 0.6855601600068262    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1418   score: 0.0   memory length: 258930   epsilon: 0.6853166200068315    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1419   score: 2.0   memory length: 259127   epsilon: 0.6849265600068399    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1420   score: 4.0   memory length: 259425   epsilon: 0.6843365200068527    steps: 298    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1421   score: 0.0   memory length: 259548   epsilon: 0.684092980006858    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1422   score: 0.0   memory length: 259671   epsilon: 0.6838494400068633    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1423   score: 2.0   memory length: 259868   epsilon: 0.6834593800068718    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1424   score: 0.0   memory length: 259990   epsilon: 0.683217820006877    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1425   score: 2.0   memory length: 260188   epsilon: 0.6828257800068855    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1426   score: 4.0   memory length: 260468   epsilon: 0.6822713800068976    steps: 280    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1427   score: 1.0   memory length: 260638   epsilon: 0.6819347800069049    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1428   score: 3.0   memory length: 260881   epsilon: 0.6814536400069153    steps: 243    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1429   score: 4.0   memory length: 261197   epsilon: 0.6808279600069289    steps: 316    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1430   score: 1.0   memory length: 261347   epsilon: 0.6805309600069354    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1431   score: 1.0   memory length: 261515   epsilon: 0.6801983200069426    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1432   score: 2.0   memory length: 261715   epsilon: 0.6798023200069512    steps: 200    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1433   score: 2.0   memory length: 261894   epsilon: 0.6794479000069589    steps: 179    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1434   score: 2.0   memory length: 262092   epsilon: 0.6790558600069674    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1435   score: 4.0   memory length: 262351   epsilon: 0.6785430400069785    steps: 259    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1436   score: 1.0   memory length: 262501   epsilon: 0.678246040006985    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1437   score: 2.0   memory length: 262699   epsilon: 0.6778540000069935    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1438   score: 0.0   memory length: 262822   epsilon: 0.6776104600069988    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1439   score: 2.0   memory length: 263038   epsilon: 0.677182780007008    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1440   score: 3.0   memory length: 263284   epsilon: 0.6766957000070186    steps: 246    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1441   score: 3.0   memory length: 263509   epsilon: 0.6762502000070283    steps: 225    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1442   score: 2.0   memory length: 263691   epsilon: 0.6758898400070361    steps: 182    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1443   score: 1.0   memory length: 263844   epsilon: 0.6755869000070427    steps: 153    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1444   score: 0.0   memory length: 263967   epsilon: 0.675343360007048    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1445   score: 2.0   memory length: 264184   epsilon: 0.6749137000070573    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1446   score: 1.0   memory length: 264355   epsilon: 0.6745751200070647    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1447   score: 0.0   memory length: 264478   epsilon: 0.6743315800070699    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1448   score: 1.0   memory length: 264649   epsilon: 0.6739930000070773    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1449   score: 1.0   memory length: 264799   epsilon: 0.6736960000070837    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1450   score: 1.0   memory length: 264949   epsilon: 0.6733990000070902    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1451   score: 0.0   memory length: 265072   epsilon: 0.6731554600070955    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1452   score: 5.0   memory length: 265398   epsilon: 0.6725099800071095    steps: 326    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1453   score: 0.0   memory length: 265521   epsilon: 0.6722664400071148    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1454   score: 1.0   memory length: 265671   epsilon: 0.6719694400071212    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1455   score: 0.0   memory length: 265794   epsilon: 0.6717259000071265    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1456   score: 2.0   memory length: 266013   epsilon: 0.6712922800071359    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1457   score: 3.0   memory length: 266261   epsilon: 0.6708012400071466    steps: 248    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1458   score: 3.0   memory length: 266507   epsilon: 0.6703141600071572    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1459   score: 1.0   memory length: 266657   epsilon: 0.6700171600071636    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1460   score: 2.0   memory length: 266874   epsilon: 0.6695875000071729    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1461   score: 2.0   memory length: 267091   epsilon: 0.6691578400071823    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1462   score: 0.0   memory length: 267213   epsilon: 0.6689162800071875    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1463   score: 2.0   memory length: 267414   epsilon: 0.6685183000071961    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1464   score: 2.0   memory length: 267614   epsilon: 0.6681223000072047    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1465   score: 1.0   memory length: 267764   epsilon: 0.6678253000072112    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1466   score: 0.0   memory length: 267886   epsilon: 0.6675837400072164    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1467   score: 1.0   memory length: 268056   epsilon: 0.6672471400072237    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1468   score: 0.0   memory length: 268179   epsilon: 0.667003600007229    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1469   score: 0.0   memory length: 268301   epsilon: 0.6667620400072343    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1470   score: 1.0   memory length: 268469   epsilon: 0.6664294000072415    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1471   score: 0.0   memory length: 268592   epsilon: 0.6661858600072468    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1472   score: 1.0   memory length: 268743   epsilon: 0.6658868800072533    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1473   score: 0.0   memory length: 268866   epsilon: 0.6656433400072586    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1474   score: 1.0   memory length: 269034   epsilon: 0.6653107000072658    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1475   score: 2.0   memory length: 269231   epsilon: 0.6649206400072742    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1476   score: 2.0   memory length: 269428   epsilon: 0.6645305800072827    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1477   score: 2.0   memory length: 269625   epsilon: 0.6641405200072912    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1478   score: 1.0   memory length: 269795   epsilon: 0.6638039200072985    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1479   score: 2.0   memory length: 270013   epsilon: 0.6633722800073079    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1480   score: 1.0   memory length: 270164   epsilon: 0.6630733000073143    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1481   score: 2.0   memory length: 270362   epsilon: 0.6626812600073229    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1482   score: 1.0   memory length: 270532   epsilon: 0.6623446600073302    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1483   score: 3.0   memory length: 270781   epsilon: 0.6618516400073409    steps: 249    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1484   score: 1.0   memory length: 270950   epsilon: 0.6615170200073481    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1485   score: 3.0   memory length: 271179   epsilon: 0.661063600007358    steps: 229    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1486   score: 4.0   memory length: 271437   epsilon: 0.6605527600073691    steps: 258    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1487   score: 4.0   memory length: 271713   epsilon: 0.6600062800073809    steps: 276    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1488   score: 3.0   memory length: 271939   epsilon: 0.6595588000073906    steps: 226    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1489   score: 1.0   memory length: 272107   epsilon: 0.6592261600073979    steps: 168    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1490   score: 0.0   memory length: 272229   epsilon: 0.6589846000074031    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1491   score: 0.0   memory length: 272351   epsilon: 0.6587430400074084    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1492   score: 2.0   memory length: 272551   epsilon: 0.658347040007417    steps: 200    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1493   score: 1.0   memory length: 272702   epsilon: 0.6580480600074234    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1494   score: 1.0   memory length: 272870   epsilon: 0.6577154200074307    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1495   score: 0.0   memory length: 272993   epsilon: 0.657471880007436    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1496   score: 4.0   memory length: 273267   epsilon: 0.6569293600074477    steps: 274    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1497   score: 3.0   memory length: 273532   epsilon: 0.6564046600074591    steps: 265    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1498   score: 3.0   memory length: 273760   epsilon: 0.6559532200074689    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1499   score: 1.0   memory length: 273929   epsilon: 0.6556186000074762    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1500   score: 0.0   memory length: 274051   epsilon: 0.6553770400074814    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1501   score: 2.0   memory length: 274249   epsilon: 0.6549850000074899    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1502   score: 1.0   memory length: 274399   epsilon: 0.6546880000074964    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1503   score: 0.0   memory length: 274521   epsilon: 0.6544464400075016    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1504   score: 2.0   memory length: 274718   epsilon: 0.6540563800075101    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1505   score: 5.0   memory length: 275063   epsilon: 0.6533732800075249    steps: 345    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1506   score: 4.0   memory length: 275340   epsilon: 0.6528248200075368    steps: 277    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1507   score: 2.0   memory length: 275539   epsilon: 0.6524308000075454    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1508   score: 5.0   memory length: 275878   epsilon: 0.65175958000756    steps: 339    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1509   score: 0.0   memory length: 276001   epsilon: 0.6515160400075652    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1510   score: 2.0   memory length: 276198   epsilon: 0.6511259800075737    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1511   score: 3.0   memory length: 276427   epsilon: 0.6506725600075836    steps: 229    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1512   score: 2.0   memory length: 276627   epsilon: 0.6502765600075922    steps: 200    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1513   score: 0.0   memory length: 276749   epsilon: 0.6500350000075974    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1514   score: 0.0   memory length: 276871   epsilon: 0.6497934400076026    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1515   score: 2.0   memory length: 277086   epsilon: 0.6493677400076119    steps: 215    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1516   score: 1.0   memory length: 277236   epsilon: 0.6490707400076183    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1517   score: 1.0   memory length: 277386   epsilon: 0.6487737400076248    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1518   score: 0.0   memory length: 277508   epsilon: 0.64853218000763    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1519   score: 3.0   memory length: 277755   epsilon: 0.6480431200076406    steps: 247    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1520   score: 1.0   memory length: 277923   epsilon: 0.6477104800076479    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1521   score: 3.0   memory length: 278169   epsilon: 0.6472234000076584    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1522   score: 0.0   memory length: 278291   epsilon: 0.6469818400076637    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1523   score: 0.0   memory length: 278414   epsilon: 0.646738300007669    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1524   score: 1.0   memory length: 278564   epsilon: 0.6464413000076754    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1525   score: 2.0   memory length: 278761   epsilon: 0.6460512400076839    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1526   score: 3.0   memory length: 278987   epsilon: 0.6456037600076936    steps: 226    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1527   score: 1.0   memory length: 279155   epsilon: 0.6452711200077008    steps: 168    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1528   score: 2.0   memory length: 279373   epsilon: 0.6448394800077102    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1529   score: 1.0   memory length: 279541   epsilon: 0.6445068400077174    steps: 168    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1530   score: 0.0   memory length: 279663   epsilon: 0.6442652800077227    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1531   score: 0.0   memory length: 279785   epsilon: 0.6440237200077279    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1532   score: 0.0   memory length: 279907   epsilon: 0.6437821600077331    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1533   score: 2.0   memory length: 280105   epsilon: 0.6433901200077417    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1534   score: 2.0   memory length: 280321   epsilon: 0.6429624400077509    steps: 216    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1535   score: 0.0   memory length: 280443   epsilon: 0.6427208800077562    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1536   score: 0.0   memory length: 280566   epsilon: 0.6424773400077615    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1537   score: 2.0   memory length: 280763   epsilon: 0.6420872800077699    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1538   score: 0.0   memory length: 280886   epsilon: 0.6418437400077752    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1539   score: 0.0   memory length: 281009   epsilon: 0.6416002000077805    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1540   score: 1.0   memory length: 281178   epsilon: 0.6412655800077878    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1541   score: 3.0   memory length: 281424   epsilon: 0.6407785000077983    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1542   score: 0.0   memory length: 281546   epsilon: 0.6405369400078036    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1543   score: 1.0   memory length: 281718   epsilon: 0.640196380007811    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1544   score: 1.0   memory length: 281887   epsilon: 0.6398617600078182    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1545   score: 3.0   memory length: 282112   epsilon: 0.6394162600078279    steps: 225    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1546   score: 2.0   memory length: 282309   epsilon: 0.6390262000078364    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1547   score: 2.0   memory length: 282507   epsilon: 0.6386341600078449    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1548   score: 2.0   memory length: 282692   epsilon: 0.6382678600078528    steps: 185    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1549   score: 3.0   memory length: 282960   epsilon: 0.6377372200078644    steps: 268    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1550   score: 0.0   memory length: 283083   epsilon: 0.6374936800078697    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1551   score: 2.0   memory length: 283301   epsilon: 0.637062040007879    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1552   score: 2.0   memory length: 283499   epsilon: 0.6366700000078875    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1553   score: 0.0   memory length: 283622   epsilon: 0.6364264600078928    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1554   score: 1.0   memory length: 283773   epsilon: 0.6361274800078993    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1555   score: 0.0   memory length: 283895   epsilon: 0.6358859200079046    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1556   score: 1.0   memory length: 284046   epsilon: 0.635586940007911    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1557   score: 0.0   memory length: 284169   epsilon: 0.6353434000079163    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1558   score: 4.0   memory length: 284485   epsilon: 0.6347177200079299    steps: 316    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1559   score: 0.0   memory length: 284607   epsilon: 0.6344761600079352    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1560   score: 0.0   memory length: 284730   epsilon: 0.6342326200079405    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1561   score: 3.0   memory length: 284977   epsilon: 0.6337435600079511    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1562   score: 0.0   memory length: 285100   epsilon: 0.6335000200079564    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1563   score: 0.0   memory length: 285223   epsilon: 0.6332564800079616    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1564   score: 1.0   memory length: 285392   epsilon: 0.6329218600079689    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1565   score: 1.0   memory length: 285543   epsilon: 0.6326228800079754    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1566   score: 0.0   memory length: 285665   epsilon: 0.6323813200079806    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1567   score: 1.0   memory length: 285815   epsilon: 0.6320843200079871    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1568   score: 1.0   memory length: 285983   epsilon: 0.6317516800079943    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1569   score: 1.0   memory length: 286154   epsilon: 0.6314131000080017    steps: 171    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1570   score: 0.0   memory length: 286276   epsilon: 0.6311715400080069    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1571   score: 2.0   memory length: 286495   epsilon: 0.6307379200080163    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1572   score: 0.0   memory length: 286618   epsilon: 0.6304943800080216    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1573   score: 0.0   memory length: 286740   epsilon: 0.6302528200080268    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1574   score: 3.0   memory length: 286967   epsilon: 0.6298033600080366    steps: 227    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1575   score: 0.0   memory length: 287090   epsilon: 0.6295598200080419    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1576   score: 0.0   memory length: 287213   epsilon: 0.6293162800080472    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1577   score: 0.0   memory length: 287336   epsilon: 0.6290727400080525    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1578   score: 0.0   memory length: 287459   epsilon: 0.6288292000080578    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1579   score: 0.0   memory length: 287582   epsilon: 0.628585660008063    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1580   score: 1.0   memory length: 287752   epsilon: 0.6282490600080703    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1581   score: 3.0   memory length: 287982   epsilon: 0.6277936600080802    steps: 230    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1582   score: 0.0   memory length: 288105   epsilon: 0.6275501200080855    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1583   score: 2.0   memory length: 288287   epsilon: 0.6271897600080933    steps: 182    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1584   score: 1.0   memory length: 288438   epsilon: 0.6268907800080998    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1585   score: 2.0   memory length: 288635   epsilon: 0.6265007200081083    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1586   score: 2.0   memory length: 288832   epsilon: 0.6261106600081168    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1587   score: 2.0   memory length: 289029   epsilon: 0.6257206000081252    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1588   score: 1.0   memory length: 289179   epsilon: 0.6254236000081317    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1589   score: 1.0   memory length: 289347   epsilon: 0.6250909600081389    steps: 168    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1590   score: 3.0   memory length: 289594   epsilon: 0.6246019000081495    steps: 247    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1591   score: 1.0   memory length: 289762   epsilon: 0.6242692600081567    steps: 168    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1592   score: 0.0   memory length: 289885   epsilon: 0.624025720008162    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1593   score: 2.0   memory length: 290083   epsilon: 0.6236336800081705    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1594   score: 3.0   memory length: 290311   epsilon: 0.6231822400081803    steps: 228    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1595   score: 4.0   memory length: 290598   epsilon: 0.6226139800081927    steps: 287    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1596   score: 2.0   memory length: 290796   epsilon: 0.6222219400082012    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1597   score: 2.0   memory length: 290995   epsilon: 0.6218279200082097    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1598   score: 2.0   memory length: 291192   epsilon: 0.6214378600082182    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1599   score: 3.0   memory length: 291421   epsilon: 0.6209844400082281    steps: 229    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1600   score: 1.0   memory length: 291571   epsilon: 0.6206874400082345    steps: 150    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1601   score: 4.0   memory length: 291885   epsilon: 0.620065720008248    steps: 314    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1602   score: 3.0   memory length: 292111   epsilon: 0.6196182400082577    steps: 226    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1603   score: 2.0   memory length: 292332   epsilon: 0.6191806600082672    steps: 221    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1604   score: 1.0   memory length: 292482   epsilon: 0.6188836600082737    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1605   score: 0.0   memory length: 292605   epsilon: 0.618640120008279    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1606   score: 1.0   memory length: 292775   epsilon: 0.6183035200082863    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1607   score: 3.0   memory length: 293002   epsilon: 0.617854060008296    steps: 227    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1608   score: 0.0   memory length: 293124   epsilon: 0.6176125000083013    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1609   score: 1.0   memory length: 293292   epsilon: 0.6172798600083085    steps: 168    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1610   score: 6.0   memory length: 293628   epsilon: 0.6166145800083229    steps: 336    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1611   score: 0.0   memory length: 293750   epsilon: 0.6163730200083282    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1612   score: 1.0   memory length: 293900   epsilon: 0.6160760200083346    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1613   score: 2.0   memory length: 294117   epsilon: 0.6156463600083439    steps: 217    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1614   score: 4.0   memory length: 294411   epsilon: 0.6150642400083566    steps: 294    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1615   score: 0.0   memory length: 294534   epsilon: 0.6148207000083619    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1616   score: 0.0   memory length: 294656   epsilon: 0.6145791400083671    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1617   score: 1.0   memory length: 294825   epsilon: 0.6142445200083744    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1618   score: 2.0   memory length: 295022   epsilon: 0.6138544600083828    steps: 197    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1619   score: 1.0   memory length: 295172   epsilon: 0.6135574600083893    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1620   score: 0.0   memory length: 295294   epsilon: 0.6133159000083945    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1621   score: 2.0   memory length: 295491   epsilon: 0.612925840008403    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1622   score: 0.0   memory length: 295613   epsilon: 0.6126842800084082    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1623   score: 0.0   memory length: 295736   epsilon: 0.6124407400084135    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1624   score: 2.0   memory length: 295934   epsilon: 0.612048700008422    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1625   score: 0.0   memory length: 296057   epsilon: 0.6118051600084273    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1626   score: 3.0   memory length: 296303   epsilon: 0.6113180800084379    steps: 246    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1627   score: 3.0   memory length: 296550   epsilon: 0.6108290200084485    steps: 247    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1628   score: 3.0   memory length: 296817   epsilon: 0.61030036000846    steps: 267    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1629   score: 1.0   memory length: 296985   epsilon: 0.6099677200084672    steps: 168    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1630   score: 0.0   memory length: 297107   epsilon: 0.6097261600084725    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1631   score: 0.0   memory length: 297229   epsilon: 0.6094846000084777    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1632   score: 1.0   memory length: 297397   epsilon: 0.6091519600084849    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1633   score: 2.0   memory length: 297596   epsilon: 0.6087579400084935    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1634   score: 4.0   memory length: 297885   epsilon: 0.6081857200085059    steps: 289    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1635   score: 0.0   memory length: 298007   epsilon: 0.6079441600085111    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1636   score: 1.0   memory length: 298157   epsilon: 0.6076471600085176    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1637   score: 0.0   memory length: 298280   epsilon: 0.6074036200085229    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1638   score: 0.0   memory length: 298403   epsilon: 0.6071600800085282    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1639   score: 1.0   memory length: 298571   epsilon: 0.6068274400085354    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1640   score: 1.0   memory length: 298742   epsilon: 0.6064888600085427    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1641   score: 1.0   memory length: 298910   epsilon: 0.60615622000855    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1642   score: 1.0   memory length: 299060   epsilon: 0.6058592200085564    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1643   score: 3.0   memory length: 299289   epsilon: 0.6054058000085663    steps: 229    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1644   score: 3.0   memory length: 299533   epsilon: 0.6049226800085767    steps: 244    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1645   score: 2.0   memory length: 299750   epsilon: 0.6044930200085861    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1646   score: 1.0   memory length: 299900   epsilon: 0.6041960200085925    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1647   score: 1.0   memory length: 300069   epsilon: 0.6038614000085998    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1648   score: 1.0   memory length: 300241   epsilon: 0.6035208400086072    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1649   score: 0.0   memory length: 300363   epsilon: 0.6032792800086124    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1650   score: 0.0   memory length: 300485   epsilon: 0.6030377200086177    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1651   score: 0.0   memory length: 300608   epsilon: 0.602794180008623    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1652   score: 0.0   memory length: 300731   epsilon: 0.6025506400086282    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1653   score: 1.0   memory length: 300883   epsilon: 0.6022496800086348    steps: 152    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1654   score: 0.0   memory length: 301005   epsilon: 0.60200812000864    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1655   score: 3.0   memory length: 301236   epsilon: 0.6015507400086499    steps: 231    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1656   score: 0.0   memory length: 301359   epsilon: 0.6013072000086552    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1657   score: 6.0   memory length: 301713   epsilon: 0.6006062800086704    steps: 354    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1658   score: 4.0   memory length: 301980   epsilon: 0.6000776200086819    steps: 267    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1659   score: 1.0   memory length: 302131   epsilon: 0.5997786400086884    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1660   score: 2.0   memory length: 302348   epsilon: 0.5993489800086977    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1661   score: 2.0   memory length: 302545   epsilon: 0.5989589200087062    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1662   score: 0.0   memory length: 302667   epsilon: 0.5987173600087115    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1663   score: 1.0   memory length: 302836   epsilon: 0.5983827400087187    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1664   score: 0.0   memory length: 302959   epsilon: 0.598139200008724    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1665   score: 1.0   memory length: 303109   epsilon: 0.5978422000087305    steps: 150    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1666   score: 2.0   memory length: 303291   epsilon: 0.5974818400087383    steps: 182    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1667   score: 2.0   memory length: 303508   epsilon: 0.5970521800087476    steps: 217    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1668   score: 2.0   memory length: 303724   epsilon: 0.5966245000087569    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1669   score: 0.0   memory length: 303846   epsilon: 0.5963829400087621    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1670   score: 0.0   memory length: 303968   epsilon: 0.5961413800087674    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1671   score: 0.0   memory length: 304090   epsilon: 0.5958998200087726    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1672   score: 0.0   memory length: 304212   epsilon: 0.5956582600087779    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1673   score: 1.0   memory length: 304381   epsilon: 0.5953236400087851    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1674   score: 2.0   memory length: 304578   epsilon: 0.5949335800087936    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1675   score: 2.0   memory length: 304758   epsilon: 0.5945771800088013    steps: 180    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1676   score: 0.0   memory length: 304880   epsilon: 0.5943356200088066    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1677   score: 0.0   memory length: 305003   epsilon: 0.5940920800088119    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1678   score: 2.0   memory length: 305202   epsilon: 0.5936980600088204    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1679   score: 1.0   memory length: 305372   epsilon: 0.5933614600088277    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1680   score: 1.0   memory length: 305522   epsilon: 0.5930644600088342    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1681   score: 4.0   memory length: 305796   epsilon: 0.592521940008846    steps: 274    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1682   score: 0.0   memory length: 305918   epsilon: 0.5922803800088512    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1683   score: 2.0   memory length: 306115   epsilon: 0.5918903200088597    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1684   score: 1.0   memory length: 306283   epsilon: 0.5915576800088669    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1685   score: 2.0   memory length: 306500   epsilon: 0.5911280200088762    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1686   score: 2.0   memory length: 306699   epsilon: 0.5907340000088848    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1687   score: 2.0   memory length: 306899   epsilon: 0.5903380000088934    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1688   score: 0.0   memory length: 307021   epsilon: 0.5900964400088986    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1689   score: 2.0   memory length: 307218   epsilon: 0.5897063800089071    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1690   score: 3.0   memory length: 307490   epsilon: 0.5891678200089188    steps: 272    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1691   score: 3.0   memory length: 307736   epsilon: 0.5886807400089293    steps: 246    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1692   score: 0.0   memory length: 307859   epsilon: 0.5884372000089346    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1693   score: 0.0   memory length: 307981   epsilon: 0.5881956400089399    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1694   score: 2.0   memory length: 308178   epsilon: 0.5878055800089483    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1695   score: 0.0   memory length: 308300   epsilon: 0.5875640200089536    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1696   score: 0.0   memory length: 308422   epsilon: 0.5873224600089588    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1697   score: 0.0   memory length: 308544   epsilon: 0.5870809000089641    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1698   score: 3.0   memory length: 308772   epsilon: 0.5866294600089739    steps: 228    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1699   score: 1.0   memory length: 308942   epsilon: 0.5862928600089812    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1700   score: 2.0   memory length: 309139   epsilon: 0.5859028000089896    steps: 197    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1701   score: 0.0   memory length: 309261   epsilon: 0.5856612400089949    steps: 122    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1702   score: 4.0   memory length: 309526   epsilon: 0.5851365400090063    steps: 265    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1703   score: 1.0   memory length: 309677   epsilon: 0.5848375600090128    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1704   score: 2.0   memory length: 309875   epsilon: 0.5844455200090213    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1705   score: 2.0   memory length: 310093   epsilon: 0.5840138800090307    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1706   score: 0.0   memory length: 310215   epsilon: 0.5837723200090359    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1707   score: 2.0   memory length: 310412   epsilon: 0.5833822600090444    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1708   score: 1.0   memory length: 310582   epsilon: 0.5830456600090517    steps: 170    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1709   score: 3.0   memory length: 310792   epsilon: 0.5826298600090607    steps: 210    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1710   score: 2.0   memory length: 310990   epsilon: 0.5822378200090692    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1711   score: 4.0   memory length: 311278   epsilon: 0.5816675800090816    steps: 288    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1712   score: 2.0   memory length: 311498   epsilon: 0.581231980009091    steps: 220    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1713   score: 2.0   memory length: 311714   epsilon: 0.5808043000091003    steps: 216    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1714   score: 1.0   memory length: 311882   epsilon: 0.5804716600091075    steps: 168    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1715   score: 1.0   memory length: 312032   epsilon: 0.580174660009114    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1716   score: 3.0   memory length: 312280   epsilon: 0.5796836200091247    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1717   score: 2.0   memory length: 312497   epsilon: 0.579253960009134    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1718   score: 3.0   memory length: 312722   epsilon: 0.5788084600091437    steps: 225    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1719   score: 1.0   memory length: 312891   epsilon: 0.5784738400091509    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1720   score: 1.0   memory length: 313041   epsilon: 0.5781768400091574    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1721   score: 0.0   memory length: 313163   epsilon: 0.5779352800091626    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1722   score: 1.0   memory length: 313331   epsilon: 0.5776026400091698    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1723   score: 2.0   memory length: 313529   epsilon: 0.5772106000091783    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1724   score: 3.0   memory length: 313795   epsilon: 0.5766839200091898    steps: 266    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1725   score: 0.0   memory length: 313917   epsilon: 0.576442360009195    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1726   score: 1.0   memory length: 314067   epsilon: 0.5761453600092015    steps: 150    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1727   score: 0.0   memory length: 314189   epsilon: 0.5759038000092067    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1728   score: 3.0   memory length: 314415   epsilon: 0.5754563200092164    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1729   score: 2.0   memory length: 314597   epsilon: 0.5750959600092242    steps: 182    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1730   score: 0.0   memory length: 314719   epsilon: 0.5748544000092295    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1731   score: 1.0   memory length: 314889   epsilon: 0.5745178000092368    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1732   score: 2.0   memory length: 315087   epsilon: 0.5741257600092453    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1733   score: 1.0   memory length: 315255   epsilon: 0.5737931200092525    steps: 168    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1734   score: 0.0   memory length: 315377   epsilon: 0.5735515600092578    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1735   score: 0.0   memory length: 315499   epsilon: 0.573310000009263    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1736   score: 1.0   memory length: 315668   epsilon: 0.5729753800092703    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1737   score: 2.0   memory length: 315865   epsilon: 0.5725853200092788    steps: 197    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1738   score: 1.0   memory length: 316035   epsilon: 0.5722487200092861    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1739   score: 1.0   memory length: 316185   epsilon: 0.5719517200092925    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1740   score: 3.0   memory length: 316455   epsilon: 0.5714171200093041    steps: 270    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1741   score: 2.0   memory length: 316674   epsilon: 0.5709835000093135    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1742   score: 2.0   memory length: 316871   epsilon: 0.570593440009322    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1743   score: 0.0   memory length: 316994   epsilon: 0.5703499000093273    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1744   score: 3.0   memory length: 317241   epsilon: 0.5698608400093379    steps: 247    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1745   score: 1.0   memory length: 317391   epsilon: 0.5695638400093443    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1746   score: 3.0   memory length: 317639   epsilon: 0.569072800009355    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1747   score: 0.0   memory length: 317761   epsilon: 0.5688312400093603    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1748   score: 2.0   memory length: 317959   epsilon: 0.5684392000093688    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1749   score: 2.0   memory length: 318157   epsilon: 0.5680471600093773    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1750   score: 2.0   memory length: 318354   epsilon: 0.5676571000093857    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1751   score: 4.0   memory length: 318610   epsilon: 0.5671502200093967    steps: 256    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1752   score: 2.0   memory length: 318807   epsilon: 0.5667601600094052    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1753   score: 2.0   memory length: 319005   epsilon: 0.5663681200094137    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1754   score: 2.0   memory length: 319225   epsilon: 0.5659325200094232    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1755   score: 0.0   memory length: 319347   epsilon: 0.5656909600094284    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1756   score: 0.0   memory length: 319470   epsilon: 0.5654474200094337    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1757   score: 2.0   memory length: 319672   epsilon: 0.5650474600094424    steps: 202    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1758   score: 1.0   memory length: 319823   epsilon: 0.5647484800094489    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1759   score: 2.0   memory length: 320021   epsilon: 0.5643564400094574    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1760   score: 1.0   memory length: 320171   epsilon: 0.5640594400094638    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1761   score: 0.0   memory length: 320293   epsilon: 0.5638178800094691    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1762   score: 3.0   memory length: 320519   epsilon: 0.5633704000094788    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1763   score: 2.0   memory length: 320737   epsilon: 0.5629387600094882    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1764   score: 0.0   memory length: 320859   epsilon: 0.5626972000094934    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1765   score: 0.0   memory length: 320982   epsilon: 0.5624536600094987    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1766   score: 0.0   memory length: 321105   epsilon: 0.562210120009504    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1767   score: 0.0   memory length: 321227   epsilon: 0.5619685600095092    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1768   score: 2.0   memory length: 321425   epsilon: 0.5615765200095177    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1769   score: 1.0   memory length: 321575   epsilon: 0.5612795200095242    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1770   score: 3.0   memory length: 321803   epsilon: 0.560828080009534    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1771   score: 1.0   memory length: 321953   epsilon: 0.5605310800095404    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1772   score: 3.0   memory length: 322197   epsilon: 0.5600479600095509    steps: 244    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1773   score: 0.0   memory length: 322319   epsilon: 0.5598064000095562    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1774   score: 1.0   memory length: 322487   epsilon: 0.5594737600095634    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1775   score: 2.0   memory length: 322684   epsilon: 0.5590837000095719    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1776   score: 0.0   memory length: 322806   epsilon: 0.5588421400095771    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1777   score: 2.0   memory length: 323003   epsilon: 0.5584520800095856    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1778   score: 0.0   memory length: 323125   epsilon: 0.5582105200095908    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1779   score: 0.0   memory length: 323247   epsilon: 0.5579689600095961    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1780   score: 2.0   memory length: 323444   epsilon: 0.5575789000096045    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1781   score: 1.0   memory length: 323616   epsilon: 0.5572383400096119    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1782   score: 1.0   memory length: 323784   epsilon: 0.5569057000096191    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1783   score: 0.0   memory length: 323906   epsilon: 0.5566641400096244    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1784   score: 3.0   memory length: 324134   epsilon: 0.5562127000096342    steps: 228    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1785   score: 2.0   memory length: 324331   epsilon: 0.5558226400096427    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1786   score: 2.0   memory length: 324529   epsilon: 0.5554306000096512    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1787   score: 0.0   memory length: 324652   epsilon: 0.5551870600096565    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1788   score: 1.0   memory length: 324821   epsilon: 0.5548524400096637    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1789   score: 1.0   memory length: 324990   epsilon: 0.554517820009671    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1790   score: 0.0   memory length: 325112   epsilon: 0.5542762600096762    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1791   score: 2.0   memory length: 325310   epsilon: 0.5538842200096847    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1792   score: 0.0   memory length: 325433   epsilon: 0.55364068000969    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1793   score: 3.0   memory length: 325658   epsilon: 0.5531951800096997    steps: 225    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1794   score: 2.0   memory length: 325876   epsilon: 0.5527635400097091    steps: 218    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1795   score: 0.0   memory length: 325999   epsilon: 0.5525200000097144    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1796   score: 0.0   memory length: 326121   epsilon: 0.5522784400097196    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1797   score: 0.0   memory length: 326243   epsilon: 0.5520368800097248    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1798   score: 3.0   memory length: 326488   epsilon: 0.5515517800097354    steps: 245    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1799   score: 1.0   memory length: 326657   epsilon: 0.5512171600097426    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1800   score: 2.0   memory length: 326854   epsilon: 0.5508271000097511    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1801   score: 2.0   memory length: 327072   epsilon: 0.5503954600097605    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1802   score: 0.0   memory length: 327195   epsilon: 0.5501519200097658    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1803   score: 1.0   memory length: 327345   epsilon: 0.5498549200097722    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1804   score: 1.0   memory length: 327496   epsilon: 0.5495559400097787    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1805   score: 0.0   memory length: 327618   epsilon: 0.5493143800097839    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1806   score: 2.0   memory length: 327816   epsilon: 0.5489223400097925    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1807   score: 2.0   memory length: 328014   epsilon: 0.548530300009801    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1808   score: 2.0   memory length: 328215   epsilon: 0.5481323200098096    steps: 201    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1809   score: 11.0   memory length: 328675   epsilon: 0.5472215200098294    steps: 460    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1810   score: 1.0   memory length: 328825   epsilon: 0.5469245200098358    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1811   score: 0.0   memory length: 328947   epsilon: 0.5466829600098411    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1812   score: 2.0   memory length: 329144   epsilon: 0.5462929000098495    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1813   score: 2.0   memory length: 329341   epsilon: 0.545902840009858    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1814   score: 0.0   memory length: 329464   epsilon: 0.5456593000098633    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1815   score: 0.0   memory length: 329587   epsilon: 0.5454157600098686    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1816   score: 3.0   memory length: 329857   epsilon: 0.5448811600098802    steps: 270    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1817   score: 1.0   memory length: 330026   epsilon: 0.5445465400098874    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1818   score: 0.0   memory length: 330148   epsilon: 0.5443049800098927    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1819   score: 0.0   memory length: 330270   epsilon: 0.5440634200098979    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1820   score: 0.0   memory length: 330393   epsilon: 0.5438198800099032    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1821   score: 1.0   memory length: 330561   epsilon: 0.5434872400099104    steps: 168    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1822   score: 1.0   memory length: 330712   epsilon: 0.5431882600099169    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1823   score: 1.0   memory length: 330880   epsilon: 0.5428556200099242    steps: 168    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1824   score: 0.0   memory length: 331003   epsilon: 0.5426120800099294    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1825   score: 3.0   memory length: 331246   epsilon: 0.5421309400099399    steps: 243    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1826   score: 0.0   memory length: 331369   epsilon: 0.5418874000099452    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1827   score: 1.0   memory length: 331538   epsilon: 0.5415527800099524    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1828   score: 1.0   memory length: 331709   epsilon: 0.5412142000099598    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1829   score: 1.0   memory length: 331880   epsilon: 0.5408756200099671    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 1830   score: 2.0   memory length: 332077   epsilon: 0.5404855600099756    steps: 197    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1831   score: 1.0   memory length: 332228   epsilon: 0.5401865800099821    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1832   score: 1.0   memory length: 332379   epsilon: 0.5398876000099886    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1833   score: 2.0   memory length: 332597   epsilon: 0.539455960009998    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1834   score: 0.0   memory length: 332720   epsilon: 0.5392124200100032    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 1835   score: 2.0   memory length: 332918   epsilon: 0.5388203800100118    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1836   score: 3.0   memory length: 333182   epsilon: 0.5382976600100231    steps: 264    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1837   score: 1.0   memory length: 333351   epsilon: 0.5379630400100304    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1838   score: 5.0   memory length: 333635   epsilon: 0.5374007200100426    steps: 284    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1839   score: 1.0   memory length: 333804   epsilon: 0.5370661000100498    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1840   score: 3.0   memory length: 334050   epsilon: 0.5365790200100604    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1841   score: 0.0   memory length: 334173   epsilon: 0.5363354800100657    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1842   score: 2.0   memory length: 334372   epsilon: 0.5359414600100743    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1843   score: 2.0   memory length: 334590   epsilon: 0.5355098200100836    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 1844   score: 0.0   memory length: 334712   epsilon: 0.5352682600100889    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1845   score: 0.0   memory length: 334834   epsilon: 0.5350267000100941    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1846   score: 1.0   memory length: 334984   epsilon: 0.5347297000101006    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1847   score: 1.0   memory length: 335153   epsilon: 0.5343950800101078    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 1848   score: 1.0   memory length: 335303   epsilon: 0.5340980800101143    steps: 150    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 1849   score: 0.0   memory length: 335425   epsilon: 0.5338565200101195    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 1850   score: 0.0   memory length: 335547   epsilon: 0.5336149600101248    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 1851   score: 1.0   memory length: 335697   epsilon: 0.5333179600101312    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1852   score: 1.0   memory length: 335847   epsilon: 0.5330209600101377    steps: 150    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1853   score: 0.0   memory length: 335969   epsilon: 0.5327794000101429    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1854   score: 0.0   memory length: 336092   epsilon: 0.5325358600101482    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1855   score: 2.0   memory length: 336292   epsilon: 0.5321398600101568    steps: 200    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1856   score: 0.0   memory length: 336414   epsilon: 0.531898300010162    steps: 122    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1857   score: 0.0   memory length: 336536   epsilon: 0.5316567400101673    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1858   score: 3.0   memory length: 336781   epsilon: 0.5311716400101778    steps: 245    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1859   score: 2.0   memory length: 336979   epsilon: 0.5307796000101863    steps: 198    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1860   score: 0.0   memory length: 337101   epsilon: 0.5305380400101916    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1861   score: 1.0   memory length: 337270   epsilon: 0.5302034200101988    steps: 169    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1862   score: 1.0   memory length: 337420   epsilon: 0.5299064200102053    steps: 150    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1863   score: 0.0   memory length: 337543   epsilon: 0.5296628800102106    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1864   score: 2.0   memory length: 337760   epsilon: 0.5292332200102199    steps: 217    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1865   score: 2.0   memory length: 337958   epsilon: 0.5288411800102284    steps: 198    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1866   score: 2.0   memory length: 338155   epsilon: 0.5284511200102369    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1867   score: 1.0   memory length: 338323   epsilon: 0.5281184800102441    steps: 168    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1868   score: 4.0   memory length: 338581   epsilon: 0.5276076400102552    steps: 258    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 1869   score: 0.0   memory length: 338703   epsilon: 0.5273660800102604    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 1870   score: 2.0   memory length: 338900   epsilon: 0.5269760200102689    steps: 197    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 1871   score: 0.0   memory length: 339022   epsilon: 0.5267344600102741    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 1872   score: 0.0   memory length: 339144   epsilon: 0.5264929000102794    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1873   score: 0.0   memory length: 339266   epsilon: 0.5262513400102846    steps: 122    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1874   score: 1.0   memory length: 339416   epsilon: 0.5259543400102911    steps: 150    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1875   score: 0.0   memory length: 339538   epsilon: 0.5257127800102963    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1876   score: 0.0   memory length: 339660   epsilon: 0.5254712200103016    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1877   score: 1.0   memory length: 339810   epsilon: 0.525174220010308    steps: 150    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1878   score: 1.0   memory length: 339960   epsilon: 0.5248772200103144    steps: 150    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1879   score: 1.0   memory length: 340110   epsilon: 0.5245802200103209    steps: 150    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1880   score: 0.0   memory length: 340233   epsilon: 0.5243366800103262    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1881   score: 0.0   memory length: 340356   epsilon: 0.5240931400103315    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1882   score: 0.0   memory length: 340478   epsilon: 0.5238515800103367    steps: 122    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 1883   score: 4.0   memory length: 340736   epsilon: 0.5233407400103478    steps: 258    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1884   score: 1.0   memory length: 340886   epsilon: 0.5230437400103543    steps: 150    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1885   score: 2.0   memory length: 341084   epsilon: 0.5226517000103628    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1886   score: 2.0   memory length: 341302   epsilon: 0.5222200600103721    steps: 218    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1887   score: 2.0   memory length: 341499   epsilon: 0.5218300000103806    steps: 197    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1888   score: 0.0   memory length: 341622   epsilon: 0.5215864600103859    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1889   score: 0.0   memory length: 341744   epsilon: 0.5213449000103911    steps: 122    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1890   score: 4.0   memory length: 342037   epsilon: 0.5207647600104037    steps: 293    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 1891   score: 0.0   memory length: 342159   epsilon: 0.520523200010409    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 1892   score: 1.0   memory length: 342310   epsilon: 0.5202242200104155    steps: 151    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 1893   score: 1.0   memory length: 342478   epsilon: 0.5198915800104227    steps: 168    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1894   score: 0.0   memory length: 342601   epsilon: 0.519648040010428    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1895   score: 1.0   memory length: 342751   epsilon: 0.5193510400104344    steps: 150    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 1896   score: 1.0   memory length: 342923   epsilon: 0.5190104800104418    steps: 172    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1897   score: 0.0   memory length: 343046   epsilon: 0.5187669400104471    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 1898   score: 1.0   memory length: 343214   epsilon: 0.5184343000104543    steps: 168    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 1899   score: 0.0   memory length: 343336   epsilon: 0.5181927400104596    steps: 122    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 1900   score: 0.0   memory length: 343459   epsilon: 0.5179492000104648    steps: 123    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 1901   score: 0.0   memory length: 343581   epsilon: 0.5177076400104701    steps: 122    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 1902   score: 3.0   memory length: 343827   epsilon: 0.5172205600104807    steps: 246    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1903   score: 1.0   memory length: 343997   epsilon: 0.516883960010488    steps: 170    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 1904   score: 0.0   memory length: 344120   epsilon: 0.5166404200104933    steps: 123    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 1905   score: 2.0   memory length: 344319   epsilon: 0.5162464000105018    steps: 199    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 1906   score: 2.0   memory length: 344535   epsilon: 0.5158187200105111    steps: 216    lr: 0.0001     evaluation reward: 1.17\n",
      "episode: 1907   score: 0.0   memory length: 344657   epsilon: 0.5155771600105163    steps: 122    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 1908   score: 1.0   memory length: 344807   epsilon: 0.5152801600105228    steps: 150    lr: 0.0001     evaluation reward: 1.14\n",
      "episode: 1909   score: 0.0   memory length: 344929   epsilon: 0.515038600010528    steps: 122    lr: 0.0001     evaluation reward: 1.03\n",
      "episode: 1910   score: 0.0   memory length: 345051   epsilon: 0.5147970400105333    steps: 122    lr: 0.0001     evaluation reward: 1.02\n",
      "episode: 1911   score: 3.0   memory length: 345276   epsilon: 0.514351540010543    steps: 225    lr: 0.0001     evaluation reward: 1.05\n",
      "episode: 1912   score: 3.0   memory length: 345502   epsilon: 0.5139040600105527    steps: 226    lr: 0.0001     evaluation reward: 1.06\n",
      "episode: 1913   score: 0.0   memory length: 345624   epsilon: 0.5136625000105579    steps: 122    lr: 0.0001     evaluation reward: 1.04\n",
      "episode: 1914   score: 1.0   memory length: 345794   epsilon: 0.5133259000105652    steps: 170    lr: 0.0001     evaluation reward: 1.05\n",
      "episode: 1915   score: 0.0   memory length: 345917   epsilon: 0.5130823600105705    steps: 123    lr: 0.0001     evaluation reward: 1.05\n",
      "episode: 1916   score: 1.0   memory length: 346086   epsilon: 0.5127477400105778    steps: 169    lr: 0.0001     evaluation reward: 1.03\n",
      "episode: 1917   score: 2.0   memory length: 346283   epsilon: 0.5123576800105862    steps: 197    lr: 0.0001     evaluation reward: 1.04\n",
      "episode: 1918   score: 2.0   memory length: 346498   epsilon: 0.5119319800105955    steps: 215    lr: 0.0001     evaluation reward: 1.06\n",
      "episode: 1919   score: 3.0   memory length: 346744   epsilon: 0.511444900010606    steps: 246    lr: 0.0001     evaluation reward: 1.09\n",
      "episode: 1920   score: 2.0   memory length: 346941   epsilon: 0.5110548400106145    steps: 197    lr: 0.0001     evaluation reward: 1.11\n",
      "episode: 1921   score: 0.0   memory length: 347063   epsilon: 0.5108132800106198    steps: 122    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 1922   score: 2.0   memory length: 347260   epsilon: 0.5104232200106282    steps: 197    lr: 0.0001     evaluation reward: 1.11\n",
      "episode: 1923   score: 0.0   memory length: 347382   epsilon: 0.5101816600106335    steps: 122    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 1924   score: 0.0   memory length: 347505   epsilon: 0.5099381200106388    steps: 123    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 1925   score: 1.0   memory length: 347673   epsilon: 0.509605480010646    steps: 168    lr: 0.0001     evaluation reward: 1.08\n",
      "episode: 1926   score: 0.0   memory length: 347796   epsilon: 0.5093619400106513    steps: 123    lr: 0.0001     evaluation reward: 1.08\n",
      "episode: 1927   score: 0.0   memory length: 347918   epsilon: 0.5091203800106565    steps: 122    lr: 0.0001     evaluation reward: 1.07\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m Jupyter  \"http://172.22.224.169:8888/\""
     ]
    }
   ],
   "source": [
    "HISTORY_SIZE = 1\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    hidden = None\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n",
    "        state = next_state\n",
    "        next_state, reward, done, truncated, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[1, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "        score += reward\n",
    "        history[:1, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_lstm_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
